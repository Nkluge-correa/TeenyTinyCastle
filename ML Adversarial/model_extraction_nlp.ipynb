{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Cloning_ Language Models with Data Augmentation via Textattack\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n",
    "\n",
    "**_Adversarial machine learning_ is the study of the attacks on [machine learning](https://en.wikipedia.org/wiki/Machine_learning \"Machine learning\") algorithms and the defenses against such attacks. Recent surveys expose the fact that practitioners report a dire need for better protecting machine learning systems in real-world applications.**\n",
    "\n",
    "**In this notebook we will be exploring a type of attack called model extraction (_cloning_). But _what is model extraction?_ A model extraction attack is an attack to violate intellectual property and privacy in which an adversary steals trained models in a cloud using only their predictions.**\n",
    "\n",
    "![extraction](https://vitalab.github.io/article/images/stealml/fig1.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_The Unprotected Model ..._**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "\n",
    "model_api = keras.models.load_model('models\\model_api.h5')\n",
    "\n",
    "with open('models\\model_api.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "def api_call(string):\n",
    "    api_response = model_api.predict(\n",
    "        keras.preprocessing.sequence.pad_sequences(\n",
    "            tokenizer.texts_to_sequences([string]),\n",
    "            maxlen=256,\n",
    "            truncating='post'\n",
    "        )\n",
    "    )\n",
    "    return api_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's assume that our victim has a model that we can call via an API. This particular model is a _sentiment classifier_ (a.k.a., a language model) that we would like to clone. As an attacker, _we do not have a large budget_ (i.e. we must limit the number of calls we make to the model/API), and _we do not have a database of hundreds of thousands of labeled examples_ (if we did, we probably wouldn't need to be cloning this model).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 13s 13s/step\n",
      "\n",
      "REQUEST\n",
      "**************************************************\n",
      "\n",
      "\"this is a great example of NLP\" \n",
      "\n",
      "API RESPONSE\n",
      "**************************************************\n",
      "\n",
      "Negative Sentiment 😔 11%\n",
      "Neutral Sentiment 😐 11%\n",
      "Positive Sentiment 😊 78%\n"
     ]
    }
   ],
   "source": [
    "# 'this model is kind of normal'\n",
    "# 'it is an ok model'\n",
    "# 'nothing special about this model'\n",
    "# 'this is a great example of NLP'\n",
    "# 'is nice to see philosophers doing machine learning'\n",
    "# 'this model is great, one of the best models ever done by a human'\n",
    "# 'i hard to say something about a model so simple'\n",
    "# 'you call this NLP, please, my nana can do it better in pascal'\n",
    "# 'this model is garbage, i wont my money back'\n",
    "\n",
    "request = 'this is a great example of NLP'\n",
    "\n",
    "api_response = api_call(request)\n",
    "\n",
    "print(f'\\nREQUEST\\n{\"*\" * 50}\\n\\n\"{request}\" \\n\\nAPI RESPONSE\\n{\"*\" * 50}\\n')\n",
    "print(f'Negative Sentiment 😔 {round(api_response[0][0] * 100)}%')\n",
    "print(f'Neutral Sentiment 😐 {round(api_response[0][1] * 100)}%')\n",
    "print(f'Positive Sentiment 😊 {round(api_response[0][2] * 100)}%')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model looks good, and we want to clone it.**\n",
    "\n",
    "**_How should an attacker proceed?_**\n",
    "\n",
    "**To start with, if we don't want to write all our initial samples by hand, we need some data. Via [web scrapping](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Explainability/NLP%20Interpreter%20(en)/scrape_en.ipynb) or through public data repositories (e.g., [Kaggle](https://www.kaggle.com/)) we were able to assemble an initial database containing 3000 unlabeled samples (_not enough to train a good sentiment classifier_).**\n",
    "\n",
    "**This is our `proto_dataset.csv`.**\n",
    "\n",
    "**This is a _black-box attack_, which means that we have no access to the _parameters/gradient/architecture_ of the model (to us it is just something that produces outputs after receiving inputs). However, we can use these outputs to classify our `proto_dataset`. Thus, information about the target model will (indirectly) be passed to our samples. We are basically stealing the predictive power of this model, to later try to replicate.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data\\proto_dataset.csv')\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if type(df['clean_text'][i]) != str:\n",
    "        df = df.drop([i])\n",
    "\n",
    "df = df.reset_index().drop('index', axis=1)\n",
    "df['proba'] = df.clean_text.apply(api_call)\n",
    "df['class'] = df.proba.apply(np.argmax)\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The process of classifying our `proto_dataset` will vary according to the constraints imposed by our victim API (e.g., _cost per call, the limit of calls per minute, etc._). In the end, we now have a (_small_) dataset labeled by the target model. And if this model is indeed good (_why else would we want to clone it_), our samples have been accurately classified.**\n",
    "\n",
    "**Now we need to \"multiply our data\". We are assuming that the attacker does not have a large initial database, and it is not feasible to classify 30000 samples using the API of the target model (either by price or other restrictions).**\n",
    "\n",
    "**_[Data augmentation](https://en.wikipedia.org/wiki/Data_augmentation)_ are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. _This is exactly what we need._ And one library that does this for us (specifically in NLP) is [TextAttack](https://textattack.readthedocs.io/en/latest/index.html).**\n",
    "\n",
    "**[TextAttack](https://github.com/QData/TextAttack) is a Python framework for adversarial attacks, adversarial training, and data augmentation in NLP.**\n",
    "\n",
    "**The part of TextAttack that interests us right now is its _data augmentation part_. Below we list some of the many ready-made augmentation classes from this library:**\n",
    "\n",
    "> **Transforation tools $\\rightarrow$ text transformations implemented (e.g., _swaping words, like names and places_) used to create an `Augmenter` object.**\n",
    "\n",
    "- `CompositeTransformation`: use to combine multiple transformations.\n",
    "- `WordInsertionRandomSynonym`: inserts synonyms of words that are already in the sequence.\n",
    "- `WordInsertionMaskedLM`: generate potential insertion for a word using a masked language model.\n",
    "- `WordSwapHowNet`: transforms an input by replacing its words with synonyms in the stored synonyms bank generated by the OpenHowNet (needs a python version > 3.8.1).\n",
    "- `WordSwapEmbedding`: transforms an input by replacing its words with synonyms in the word embedding space.\n",
    "- `WordSwapHomoglyphSwap`: transforms an input by replacing its words with visually similar words using homoglyph swaps.\n",
    "- `WordSwapQWERTY`: common misspellings related to the QWERTY keyboard style.\n",
    "- `WordSwapContract`: transforms an input by performing contraction on recognized combinations.\n",
    "- `WordSwapChangeLocation`: changes a location described in text (e.g., Brazil -> Argentina).\n",
    "- `WordSwapChangeNumber`: changes a number mentioned in text (e.g., 7 -> 13).\n",
    "- `WordSwapChangeName`: changes a name mentioned in text (e.g., Alice -> Bob).\n",
    "- `WordSwapInflections`: transforms an input by replacing its words with their inflections.\n",
    "- `WordSwapMaskedLM` generates potential replacements for a word using a masked language model.\n",
    "- `WordSwapRandomCharacterDeletion`: transforms an input by deleting its characters (`random_one=True, skip_first_char=True, skip_last_char=True` works good!).\n",
    "- `WordSwapRandomCharacterInsertion`: transforms an input by inserting a random character (`random_one=True, skip_first_char=True, skip_last_char=True` works good!).\n",
    "- `WordSwapRandomCharacterSubstitution` transforms an input by replacing one character in a word with a random new character.\n",
    "\n",
    "> **Constraints $\\rightarrow$ constraints determine whether or not a given augmentation is valid, consequently enhancing the quality of the augmentations.**\n",
    "\n",
    "- `RepeatModification`: a constraint disallowing the modification of words that have already been modified.\n",
    "- `StopwordModification`: a constraint disallowing the modification of stopwords.\n",
    "\n",
    "> **Augmantation parameters $\\rightarrow$ control parameters of the augmenting object.**\n",
    "\n",
    "- `pct_words_to_swap`: percentage of words to swap per augmented example. The default is set to 0.1 (10%).\n",
    "- `transformations_per_example`: maximum number of augmentations per input. The default is set to 1 (one augmented sentence given one original input)\n",
    "\n",
    "> **Ready Recipes $\\rightarrow$ in addition to creating your own augmenter, you could also use pre-built augmentation recipes. These [recipes are implemented from published papers](https://textattack.readthedocs.io/en/latest/3recipes/augmenter_recipes.html) and are very convenient to use.**\n",
    "\n",
    "- `CheckListAugmenter`: augments words by using the transformation methods provided by **CheckList INV testing**, which combines **Name Replacement, Location Replacement, Number Alteration, and Contraction/Extension**.\n",
    "- `WordNetAugmenter`: another pre-made augmentation recipe (`high_yield=True, enable_advanced_metrics=True` works good!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modi constitute teli obc embody because his upbringing and mindset outlook can hold chowkidar had there been his kids they too would have become besides chowkidar get gatekeeper\n",
      "\n",
      "modi teli obc because his fosterage upbringing and mindset upbringing can embody chowkidar had there been his ingest kids they too also would rearing have become chowkidar gatekeeper\n",
      "\n",
      "modi teli obc because his upbringing and mentality mindset can personify chowkidar pot had consume there been his kids they too would embody have go become chowkidar gatekeeper\n",
      "\n",
      "modi teli obc because his upbringing child and suit mindset can chowkidar had there fostering been his doorkeeper kids they too would porter have nipper become chowkidar gatekeeper\n",
      "\n",
      "modi teli obc because his upbringing too and mindset can chowkidar breeding had excessively there been his kids they likewise too doorkeeper would have become outlook chowkidar gatekeeper\n",
      "\n",
      "modi teli obc because thither his upbringing and mindset can chowkidar there had live there been go his kids they comprise too would have become chowkidar depart gatekeeper\n",
      "\n",
      "modi teli obc likewise because excessively his upbringing and thither mindset can chowkidar had there been his suit kids they too thither would have become chowkidar sack gatekeeper\n",
      "\n",
      "modi thither teli obc because his upbringing ostiary and accept mindset can chowkidar had there been likewise his mentality kids they too would have too become chowkidar gatekeeper\n",
      "\n",
      "outlook modi teli obc because hold his upbringing and mindset can chowkidar had equal there besides been thither his kids they too would have become chowkidar birth gatekeeper\n",
      "\n",
      "shaver modi teli obc because suffer his upbringing and mindset youngster can chowkidar had there been take his outlook kids they too would ingest have become chowkidar gatekeeper\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import Augmenter\n",
    "from textattack.transformations import CompositeTransformation, WordInsertionRandomSynonym, WordSwapContract\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "\n",
    "transformation = CompositeTransformation(\n",
    "    [WordInsertionRandomSynonym(), WordSwapContract()])\n",
    "constraints = [RepeatModification(), StopwordModification()]\n",
    "\n",
    "aug = Augmenter(transformation=transformation,\n",
    "                constraints=constraints,\n",
    "                pct_words_to_swap=0.5,\n",
    "                transformations_per_example=10)\n",
    "\n",
    "request = df['clean_text'][1058]\n",
    "aug_request = aug.augment(request)\n",
    "for generated_data in aug_request:\n",
    "    print(generated_data + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For each labeled sample in our `proto_dataset`, we will generate $10$ augmented copies.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 samples augmented ...\n",
      "250 samples augmented ...\n",
      "500 samples augmented ...\n",
      "750 samples augmented ...\n",
      "1000 samples augmented ...\n",
      "1250 samples augmented ...\n",
      "1500 samples augmented ...\n",
      "1750 samples augmented ...\n",
      "2000 samples augmented ...\n",
      "2250 samples augmented ...\n",
      "2500 samples augmented ...\n",
      "2750 samples augmented ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>direct when year modi promised “minimum politi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>posit when modi promised “minimum release gove...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>when get modi nation promised “age minimum min...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when modi promised “class minimum government g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when modi promised “commonwealth minimum gover...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29263</th>\n",
       "      <td>kamre modi brand stiff remove marque first let...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29264</th>\n",
       "      <td>kamre modi stay brand remove corpse first miss...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29265</th>\n",
       "      <td>missive kamre modi brand remove offset first l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29266</th>\n",
       "      <td>take kamre modi brand remove first absent lett...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29267</th>\n",
       "      <td>take kamre stigmatize modi brand remove first ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29268 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clean_text  class\n",
       "0      direct when year modi promised “minimum politi...      0\n",
       "1      posit when modi promised “minimum release gove...      0\n",
       "2      when get modi nation promised “age minimum min...      0\n",
       "3      when modi promised “class minimum government g...      0\n",
       "4      when modi promised “commonwealth minimum gover...      0\n",
       "...                                                  ...    ...\n",
       "29263  kamre modi brand stiff remove marque first let...      0\n",
       "29264  kamre modi stay brand remove corpse first miss...      0\n",
       "29265  missive kamre modi brand remove offset first l...      0\n",
       "29266  take kamre modi brand remove first absent lett...      0\n",
       "29267  take kamre stigmatize modi brand remove first ...      0\n",
       "\n",
       "[29268 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = []\n",
    "generated_sentences = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if i % 250 == 0:\n",
    "        print(f'{i} samples augmented ...')\n",
    "    if i % len(df) == 0 and i != 0:\n",
    "        print(f'{i} samples augmented. Augmentation Complete.')\n",
    "    request = df['clean_text'][i]\n",
    "    label = df['class'][i]\n",
    "    aug_request = aug.augment(request)\n",
    "    for generated_data in aug_request:\n",
    "        generated_sentences.append(generated_data)\n",
    "        labels.append(label)\n",
    "\n",
    "data = {'clean_text': generated_sentences,\n",
    "        'class': labels}\n",
    "\n",
    "generated_data = pd.DataFrame(data)\n",
    "generated_data.to_csv('augmented_dataset.csv')\n",
    "generated_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We repeat this process twice, wherein the second time we increase the percentage of words to be changed in each sentence (`pct_words_to_swap=0.8`), and including the `WordSwapQWERTY` transformation, to simulate common typing errors. Eliminating duplicates, we arrive at a `dataset_final` with $59258$ samples. Any imbalance in the distribution of samples across classes is just a _mirror image of the biases of the original model_ (e.g., most of the samples classified in the `proto_dataset` have the label _\"negative sentiment\"_). The total time for creating this dataset was $5$ hours.**\n",
    "\n",
    "**Also, given the way that the API delivers model outputs (it gives us the _probability distribution_ of the victim's model `softmax` function), there is more information to be extracted. For example, we could [recover the model's logits from its probability predictions to approximate gradients](https://arxiv.org/abs/2011.14779). However, in this notebook/toy-example, we will limit ourselves to the vanilla version of this attack.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>State when sodi Dromised “start minimur goveJn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>behave wBen need modo promised “whC minimum as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tabernacle non when modi promised “minimum gov...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>when mMdi promiseO “minimum gBvernment maximum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>when mbdi promised “minimum government maximum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59253</th>\n",
       "      <td>59253</td>\n",
       "      <td>kamre modi brand stiff remove marque first let...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59254</th>\n",
       "      <td>59254</td>\n",
       "      <td>kamre modi stay brand remove corpse first miss...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59255</th>\n",
       "      <td>59255</td>\n",
       "      <td>missive kamre modi brand remove offset first l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59256</th>\n",
       "      <td>59256</td>\n",
       "      <td>take kamre modi brand remove first absent lett...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59257</th>\n",
       "      <td>59257</td>\n",
       "      <td>take kamre stigmatize modi brand remove first ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59258 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                         clean_text  class\n",
       "0               0  State when sodi Dromised “start minimur goveJn...      0\n",
       "1               1  behave wBen need modo promised “whC minimum as...      0\n",
       "2               2  tabernacle non when modi promised “minimum gov...      0\n",
       "3               3  when mMdi promiseO “minimum gBvernment maximum...      0\n",
       "4               4  when mbdi promised “minimum government maximum...      0\n",
       "...           ...                                                ...    ...\n",
       "59253       59253  kamre modi brand stiff remove marque first let...      0\n",
       "59254       59254  kamre modi stay brand remove corpse first miss...      0\n",
       "59255       59255  missive kamre modi brand remove offset first l...      0\n",
       "59256       59256  take kamre modi brand remove first absent lett...      0\n",
       "59257       59257  take kamre stigmatize modi brand remove first ...      0\n",
       "\n",
       "[59258 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('data\\\\final_dataset.csv')\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can train our surrogate model the _old fashion_.**\n",
    "\n",
    "- **Load & Split the `dataset`**;\n",
    "- **Build & Save the `tokenizer`**;\n",
    "- **Train the `surrogate_model`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "926/926 [==============================] - 58s 60ms/step - loss: 0.4759 - accuracy: 0.8131\n",
      "Epoch 2/10\n",
      "926/926 [==============================] - 56s 60ms/step - loss: 0.1612 - accuracy: 0.9418\n",
      "Epoch 3/10\n",
      "926/926 [==============================] - 58s 63ms/step - loss: 0.0963 - accuracy: 0.9657\n",
      "Epoch 4/10\n",
      "926/926 [==============================] - 62s 67ms/step - loss: 0.0668 - accuracy: 0.9777\n",
      "Epoch 5/10\n",
      "926/926 [==============================] - 69s 75ms/step - loss: 0.0533 - accuracy: 0.9819\n",
      "Epoch 6/10\n",
      "926/926 [==============================] - 69s 75ms/step - loss: 0.0383 - accuracy: 0.9875\n",
      "Epoch 7/10\n",
      "926/926 [==============================] - 65s 71ms/step - loss: 0.0392 - accuracy: 0.9867\n",
      "Epoch 8/10\n",
      "926/926 [==============================] - 74s 80ms/step - loss: 0.0311 - accuracy: 0.9897\n",
      "Epoch 9/10\n",
      "926/926 [==============================] - 68s 74ms/step - loss: 0.0286 - accuracy: 0.9903\n",
      "Epoch 10/10\n",
      "926/926 [==============================] - 66s 71ms/step - loss: 0.0235 - accuracy: 0.9923\n",
      "926/926 [==============================] - 32s 34ms/step - loss: 0.1975 - accuracy: 0.9471\n",
      "Final Loss: 0.2.\n",
      "Final Performance: 94.71 %.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_127_layer_call_fn, lstm_cell_127_layer_call_and_return_conditional_losses, lstm_cell_128_layer_call_fn, lstm_cell_128_layer_call_and_return_conditional_losses, lstm_cell_130_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: surrogate_model_3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: surrogate_model_3\\assets\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('data\\\\final_dataset.csv')\n",
    "\n",
    "x = list(df.clean_text)\n",
    "y = list(df['class'])\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.5, random_state=42)\n",
    "\n",
    "y_train = np.array(y_train).astype(float)\n",
    "y_test = np.array(y_test).astype(float)\n",
    "\n",
    "vocab_size = 3000\n",
    "embed_size = 50\n",
    "max_len = 280\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size,\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                      lower=True,\n",
    "                      split=\" \",\n",
    "                      oov_token=\"<OOV>\")\n",
    "\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "training_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "training_padded = pad_sequences(\n",
    "    training_sequences, maxlen=max_len, truncating='post')\n",
    "\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with io.open('surrogate_model.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\n",
    "x = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                              output_dim=embed_size,\n",
    "                              input_length=max_len)(inputs)\n",
    "x = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
    "outputs = tf.keras.layers.Dense(3, activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(training_padded,\n",
    "          y_train,\n",
    "          epochs=10,\n",
    "          verbose=1)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_len, truncating='post')\n",
    "\n",
    "test_loss_score, test_acc_score = model.evaluate(test_padded, y_test)\n",
    "\n",
    "print(f'Final Loss: {round(test_loss_score, 2)}.')\n",
    "print(f'Final Performance: {round(test_acc_score * 100, 2)} %.')\n",
    "model.save('models\\surrogate_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In real-life situations, we could not do a comparison test between the original model and our clone. But since this is just a toy example, we can! 🙃**\n",
    "\n",
    "**For this, we are using a test database not seen by both models.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 2s 27ms/step - loss: 0.6821 - accuracy: 0.7937\n",
      "\n",
      "Accuracy of the API MODEL: 79.37 %.\n",
      "\n",
      "46/46 [==============================] - 2s 28ms/step - loss: 1.8153 - accuracy: 0.6387\n",
      "\n",
      "Accuracy of the SURROGATE MODEL: 63.87 %.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataset = pd.read_csv('data\\compare_models_dataset.csv')\n",
    "test_dataset = test_dataset.dropna(how='any', axis=0)\n",
    "\n",
    "x = list(test_dataset.clean_text)\n",
    "y = np.array(list(test_dataset['class'])).astype(float)\n",
    "\n",
    "\n",
    "model_api = keras.models.load_model('models\\model_api.h5')\n",
    "\n",
    "with open('models\\model_api.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer_api = tokenizer_from_json(data)\n",
    "    word_index_api = tokenizer_api.word_index\n",
    "\n",
    "surrogate_model = keras.models.load_model('models\\surrogate_model.h5')\n",
    "\n",
    "with open('models\\surrogate_model.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer_surrogate = tokenizer_from_json(data)\n",
    "    word_index_surrogate = tokenizer_surrogate.word_index\n",
    "\n",
    "test_sequences_api = tokenizer_api.texts_to_sequences(x)\n",
    "test_padded_api = pad_sequences(\n",
    "    test_sequences_api, maxlen=256, truncating='post')\n",
    "\n",
    "_, test_acc_score = model_api.evaluate(test_padded_api, y)\n",
    "\n",
    "print(f'\\nAccuracy of the API MODEL: {round(test_acc_score * 100, 2)} %.\\n')\n",
    "\n",
    "test_sequences_surrogate = tokenizer_surrogate.texts_to_sequences(x)\n",
    "test_padded_surrogate = pad_sequences(\n",
    "    test_sequences_surrogate, maxlen=280, truncating='post')\n",
    "\n",
    "_, test_acc_score = surrogate_model.evaluate(test_padded_surrogate, y)\n",
    "\n",
    "print(\n",
    "    f'\\nAccuracy of the SURROGATE MODEL: {round(test_acc_score * 100, 2)} %.\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$15,4\\%$ less accurate than the original model in this benchmark, but still a valid model. Architecture changes and database augmentation can improve the performance of our `surrogate_model`. We now have our own language model for sentiment classification, and we have spent not even $10%$ of what was invested in creating the original model (_supposedly_).**\n",
    "\n",
    "**Now let's put our `surrogate_model` into production.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 872ms/step\n",
      "\n",
      "REQUEST\n",
      "**************************************************\n",
      "\n",
      "\"nothing special about this model\" \n",
      "\n",
      "SURROGATE API RESPONSE\n",
      "**************************************************\n",
      "\n",
      "Negative Sentiment 😔 1%\n",
      "Neutral Sentiment 😐 98%\n",
      "Positive Sentiment 😊 0%\n"
     ]
    }
   ],
   "source": [
    "def surrogate_api_call(string):\n",
    "    surrogate_api_response = surrogate_model.predict(\n",
    "        keras.preprocessing.sequence.pad_sequences(\n",
    "            tokenizer_surrogate.texts_to_sequences([string]),\n",
    "            maxlen=280,\n",
    "            truncating='post'\n",
    "        )\n",
    "    )\n",
    "    return surrogate_api_response\n",
    "\n",
    "# 'nothing special about this model'\n",
    "# 'this model is great, one of the best models ever done by a human'\n",
    "# 'this model is garbage, i wont my money back'\n",
    "\n",
    "\n",
    "request = 'nothing special about this model'\n",
    "\n",
    "surrogate_api_response = surrogate_api_call(request)\n",
    "\n",
    "print(\n",
    "    f'\\nREQUEST\\n{\"*\" * 50}\\n\\n\"{request}\" \\n\\nSURROGATE API RESPONSE\\n{\"*\" * 50}\\n')\n",
    "print(f'Negative Sentiment 😔 {round(surrogate_api_response[0][0] * 100)}%')\n",
    "print(f'Neutral Sentiment 😐 {round(surrogate_api_response[0][1] * 100)}%')\n",
    "print(f'Positive Sentiment 😊 {round(surrogate_api_response[0][2] * 100)}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model extraction attacks pose a treat to intellectual property and privacy. The availability of a model in the cloud, whether as a service or API, must be carefully architected by developers if they do not want to fall victim to this kind of attack. 🐱‍💻**\n",
    "\n",
    "**For more information on the subject, check the literature listed below:**\n",
    "\n",
    "- [A Framework for Understanding Model Extraction Attack and Defense](https://arxiv.org/abs/2206.11480);\n",
    "- [Increasing the Cost of Model Extraction with Calibrated Proof of Work](https://arxiv.org/abs/2201.09243);\n",
    "- [Data-Free Model Extraction](https://arxiv.org/abs/2011.14779);\n",
    "- [MEGEX: Data-Free Model Extraction Attack against Gradient-Based Explainable AI](https://arxiv.org/abs/2107.08909);\n",
    "- [DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories](https://arxiv.org/abs/2111.04625);\n",
    "- [Model Extraction and Defenses on Generative Adversarial Networks](https://arxiv.org/abs/2101.02069)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
