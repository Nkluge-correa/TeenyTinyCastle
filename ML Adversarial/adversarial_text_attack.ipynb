{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Adversarial _text_ with `TextAttack`\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n",
    "\n",
    "**Adversarial machine learning (`AML`) is a subfield of machine learning that focuses on developing algorithms and techniques that can withstand and respond to adversarial attacks.** \n",
    "\n",
    "**Adversarial attacks are a type of cyber attack where an attacker deliberately manipulates data inputs to ML models with the aim of causing them to produce incorrect outputs.** \n",
    "\n",
    "**`AML` aims to improve the robustness and security of ML models by identifying vulnerabilities and developing countermeasures to mitigate the impact of adversarial attacks. A range of techniques have been developed for `AML`, including `adversarial training` (_training models on adversarial examples_), and `defensive distillation` (_creating a distilled version of a model that is resistant to adversarial attacks_).**\n",
    "\n",
    "**`AML` is an active area of research, as ML models continue to be deployed in a wide range of applications where they may be vulnerable to attack.**\n",
    "\n",
    "**One of the alternatives for making models more resilient against adversarial attacks is `adversarial training`. In `adversarial training`, we generate adversarial examples and use them as samples (with their correct labels) for training (retraining) the original model, making it more robust.**\n",
    "\n",
    "**In this notebook, we will be exploring one of the functionalities of the `textattack` library.**\n",
    "\n",
    "> **_TextAttack is a Python framework for adversarial attacks, data augmentation, and model training in NLP_.**\n",
    "\n",
    "**We already work with the `text augmentation` on our [notebook](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/fa17764aa8800c388d0d298b750c686757e0861e/ML%20Adversarial/model_extraction_nlp.ipynb) about `model extraction attacks`. But in this notebook, we will develop and attack a language model trained on sentiment classification.**\n",
    "\n",
    "![sentiment-analisys](https://miro.medium.com/proxy/1*_JW1JaMpK_fVGld8pd1_JQ.gif)\n",
    "\n",
    "**In this notebook, similar to other tutorials from the [Teeny-Tiny Castle üè∞](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/fa17764aa8800c388d0d298b750c686757e0861e/ML%20Explainability/NLP%20Interpreter/model_maker.ipynb), we will create a `Bidirectional long-short term memory(bi-lstm)` for sentiment classification.**\n",
    "\n",
    "**We will be using a dataset that was put together by combining several datasets for sentiment classification available on [Kaggle](https://www.kaggle.com/):**\n",
    "\n",
    "- **The `IMDB 50K` [dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?select=IMDB+Dataset.csv): _0K movie reviews for natural language processing or Text analytics._**\n",
    "- **The `Twitter US Airline Sentiment` [dataset](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment): _originated from the [Crowdflower's Data for Everyone library](http://www.crowdflower.com/data-for-everyone)._**\n",
    "- **Our `google_play_apps_review` _dataset: built using the `google_play_scraper` in [this notebook](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/64d0693c28786ce42149411bec8b3b42520fc4df/ML%20Explainability/NLP%20Interpreter%20(en)/scrape(en).ipynb)._**\n",
    "- **The `EcoPreprocessed` [dataset](https://www.kaggle.com/datasets/pradeeshprabhakar/preprocessed-dataset-sentiment-analysis): _scrapped amazon product reviews_.**\n",
    "\n",
    "**The final result is the `sentiment_analysis_dataset.csv` available for download [here](https://drive.google.com/uc?export=download&id=1_ijhnVLHddM7Cm3R3vfqBB-svw6iNfpv). We also have a portuguese (PT-BR) version [here](https://drive.google.com/uc?export=download&id=1YCIzGqcdlHSy-GvghRp0U5USUhuOVEE3).**\n",
    "\n",
    "**This dataset already comes preprocessed, and the `cleaning` function we used is this:**\n",
    "\n",
    "```python\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    clean_text = input_data.lower().replace(\"<br />\", \" \")\n",
    "    clean_text = re.sub(r\"[-()\\\"#/@;:<>{}=~|.?,]\", ' ', clean_text)\n",
    "    clean_text = re.sub(' +', ' ', clean_text)\n",
    "    return unidecode(clean_text)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's love in the time of money is a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85084</th>\n",
       "      <td>yaaa cool use last weeks give good response</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85085</th>\n",
       "      <td>years daughter love alexa enjoy alexa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85086</th>\n",
       "      <td>yes popular but doesnt use except listen songs...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85087</th>\n",
       "      <td>yo alexa love</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85088</th>\n",
       "      <td>yo yo yo love go if want one smart speaker val...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85089 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  sentiment\n",
       "0      one of the other reviewers has mentioned that ...          1\n",
       "1      a wonderful little production the filming tech...          1\n",
       "2      i thought this was a wonderful way to spend ti...          1\n",
       "3      basically there's a family where a little boy ...          0\n",
       "4      petter mattei's love in the time of money is a...          1\n",
       "...                                                  ...        ...\n",
       "85084        yaaa cool use last weeks give good response          1\n",
       "85085              years daughter love alexa enjoy alexa          1\n",
       "85086  yes popular but doesnt use except listen songs...          1\n",
       "85087                                      yo alexa love          1\n",
       "85088  yo yo yo love go if want one smart speaker val...          1\n",
       "\n",
       "[85089 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    'https://drive.google.com/uc?export=download&id=1_ijhnVLHddM7Cm3R3vfqBB-svw6iNfpv', \n",
    "    'sentiment_analysis_dataset.csv'\n",
    ")\n",
    "\n",
    "df = pd.read_csv('sentiment_analysis_dataset.csv')\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following cells will train a `Bidirectional long-short term memory (bi-lstm)` for binary sentiment classification (Negative versus Positive). The training process may take a while, so if you want to skip this, you can load our `pre-trained senti-model` directly below the next cell.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.10.1\n",
      "Eager mode:  True\n",
      "GPU is available\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, None, 128)         640000    \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, None, 128)        98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 837,761\n",
      "Trainable params: 837,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Final Loss: 0.29.\n",
      "Final Performance: 88.23 %.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "\n",
    "vocab_size = 5000\n",
    "embed_size = 128\n",
    "sequence_length = 250\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size,\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                      lower=True,\n",
    "                      split=\" \",\n",
    "                      oov_token=\"<OOV>\")\n",
    "\n",
    "tokenizer.fit_on_texts(df.review)\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "\n",
    "with io.open('models/tokenizer_senti_model.json', 'w', encoding='utf-8') as fp:\n",
    "    fp.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "    fp.close()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.review, df.sentiment, test_size=0.2, random_state=42)\n",
    "\n",
    "x_train = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(x_train), \n",
    "    maxlen=sequence_length, \n",
    "    truncating='post')\n",
    "x_test = pad_sequences(\n",
    "    tokenizer.texts_to_sequences(x_test), \n",
    "    maxlen=sequence_length, \n",
    "    truncating='post')\n",
    "y_train = np.array(y_train).astype(float)\n",
    "y_test = np.array(y_test).astype(float)\n",
    "\n",
    "\n",
    "inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\n",
    "x = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                              output_dim=embed_size,\n",
    "                              input_length=sequence_length)(inputs)\n",
    "\n",
    "x = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True))(x)\n",
    "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n",
    "\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss=tf.losses.BinaryCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"models/senti_model.keras\",\n",
    "                                                save_best_only=True),\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                            patience=3,\n",
    "                                            verbose=1,\n",
    "                                            mode=\"auto\",\n",
    "                                            baseline=None,\n",
    "                                            restore_best_weights=True)]\n",
    "                                                                                        \n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          epochs=20,\n",
    "          validation_split=0.2,\n",
    "          callbacks=callbacks,\n",
    "          verbose=1)\n",
    "\n",
    "test_loss_score, test_acc_score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'Final Loss: {round(test_loss_score, 2)}.')\n",
    "print(f'Final Performance: {round(test_acc_score * 100, 2)} %.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you do not want to train the model, you can load the trained version in the cell below. But first, you need to download them (instructions in the `models` folder.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \"this explanation is really bad\"\n",
      "(Negative üòî 95% | Positive üòä 5%)\n",
      "\n",
      "Review: \"i did not like this tutorial 2/10\"\n",
      "(Negative üòî 81% | Positive üòä 19%)\n",
      "\n",
      "Review: \"this tutorial is garbage i wont my money back\"\n",
      "(Negative üòî 93% | Positive üòä 7%)\n",
      "\n",
      "Review: \"is nice to see philosophers doing machine learning\"\n",
      "(Negative üòî 3% | Positive üòä 97%)\n",
      "\n",
      "Review: \"this is a great and wonderful example of nlp\"\n",
      "(Negative üòî 0% | Positive üòä 100%)\n",
      "\n",
      "Review: \"this tutorial is great one of the best tutorials ever made\"\n",
      "(Negative üòî 0% | Positive üòä 100%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "\n",
    "model = keras.models.load_model('models/senti_model.keras')\n",
    "\n",
    "with open('models/tokenizer_senti_model.json') as fp:\n",
    "    data = json.load(fp)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "    fp.close()\n",
    "\n",
    "strings = [\n",
    "    'this explanation is really bad',\n",
    "    'i did not like this tutorial 2/10',\n",
    "    'this tutorial is garbage i wont my money back',\n",
    "    'is nice to see philosophers doing machine learning',\n",
    "    'this is a great and wonderful example of nlp',\n",
    "    'this tutorial is great one of the best tutorials ever made'\n",
    "]\n",
    "\n",
    "preds = model.predict(\n",
    "        keras.preprocessing.sequence.pad_sequences(\n",
    "                                                    tokenizer.texts_to_sequences(strings),\n",
    "                                                    maxlen=250,\n",
    "                                                    truncating='post'\n",
    "                                                ),\n",
    "    verbose=0)\n",
    "    \n",
    "for i, string in enumerate(strings):\n",
    "    print(f'Review: \"{string}\"\\n(Negative üòî {round((1 - preds[i][0]) * 100)}% | Positive üòä {round(preds[i][0] * 100)}%)\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model seems to be working fine! Now, let us change this.** üôÉ\n",
    "\n",
    "**Using the `textattack`, we can _wrap_ a model (like a Keras, TensorFlow, Scikitlearn, or AllenNLP model) using the `ModelWrapper` class. Then, using the `call` method, we can create a function that gives us the prediction scores for our model output.**\n",
    "\n",
    "**Creating this function/method will be a specific-task, given the natural output format of your model. Below, you can find out how to turn the output of a `sigmoid function` (the last layer of our `bi-lstm`) into a torch tensor that contains the probabilities for each of the sentiment classes (`0` for negative, `1` for positive).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.models.wrappers import ModelWrapper\n",
    "\n",
    "class ModelWrapper(ModelWrapper):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, text_input_list):\n",
    "        text_array = tokenizer.texts_to_sequences(text_input_list)\n",
    "        padded_text_array = keras.preprocessing.sequence.pad_sequences(\n",
    "                                                    text_array,\n",
    "                                                    maxlen=250,\n",
    "                                                    truncating='post'\n",
    "                                                )\n",
    "        preds = self.model.predict(padded_text_array, verbose=0)\n",
    "        logits = torch.tensor(preds)\n",
    "        logits = logits.squeeze(dim=-1)\n",
    "        final_preds = torch.stack((1-logits, logits), dim=1)\n",
    "        return final_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let us see the outputs of our `ModelWrapper`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9459, 0.0541],\n",
       "        [0.8122, 0.1878],\n",
       "        [0.9319, 0.0681],\n",
       "        [0.0289, 0.9711],\n",
       "        [0.0024, 0.9976],\n",
       "        [0.0015, 0.9985]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ModelWrapper(model)([\n",
    "    'this explanation is really bad',\n",
    "    'i did not like this tutorial 2/10',\n",
    "    'this tutorial is garbage i wont my money back',\n",
    "    'is nice to see philosophers doing machine learning',\n",
    "    'this is a great and wonderful example of nlp',\n",
    "    'this tutorial is great one of the best tutorials ever made'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exactly what we wanted, and the probabilities are in agreement with the input. Now we can just call an attack recipe from the `Attack Recipes` in`textattack`.**\n",
    "\n",
    "**However, we need something to attack. `Textattack` allows you to use `HuggingFace` Datasets for the attack. You can also use your own dataset for this.**\n",
    "\n",
    "**The `textattack.datasets.Dataset` method takes as input a list of tuples, e.g., `[('some text', label_1), ('some other text', label_2)]`. Below we transform the examples used above into a mini-dataset. `Textattack` will use these samples to create adversarial examples against our model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    ('this explanation is really bad', 0),\n",
    "    ('this tutorial is garbage i wont my money back', 0),\n",
    "    ('i did not like this tutorial 2/10', 0),\n",
    "    ('is nice to see philosophers doing machine learning', 1),\n",
    "    ('this is a great and wonderful example of nlp', 1),\n",
    "    ('this tutorial is great one of the best tutorials ever made', 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You could also transform a portion of your dataset into a list of tuples (`text, label`). You can transform any list of labeled text samples into a `textattack.datasets`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textattack\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('sentiment_analysis_dataset.csv')\n",
    "\n",
    "_, x_test, _, y_test = train_test_split(\n",
    "   list(df.review), list(df.sentiment), test_size=0.2, random_state=42)\n",
    "\n",
    "y_test = np.array(y_test).astype(float)\n",
    "\n",
    "data=[(x_test[i], int(y_test[i])) for i in range(len(x_test))]\n",
    "np.random.shuffle(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have a dataset. We can call one of the attack recipes from `textattack`. All available recipes correspond to attacks from the literature in Adversarial ML.**\n",
    "\n",
    "**Attack recipes allow you to create an `Attack` object where the goal function (determines both the conditions under which the attack is successful), transformation (the adversarial perturbations produced in the samples of the dataset), constraints (the limitations imposed on theses transformations), and search method are those specified in the origin paper.**\n",
    "\n",
    "**Here you can find a list of _fast_ attack recipes form `textattack`:**\n",
    "\n",
    "- **`PWWSRen2019`: in this attack, words are perturbed by a synonym-swap transformation based on a combination of their saliency score (e.g., _the importance of a linguistic feature_) and maximum word-swap effectiveness (proposed in \"[Generating Natural Langauge Adversarial Examples through Probability Weighted Word Saliency](https://aclanthology.org/P19-1103/)\").**\n",
    "- **`CheckList2020`: this attack focuses on several √ßangiage perturbations, like contractions, extensions, changing names, numbers, and locations (proposed in \"[Beyond Accuracy: Behavioral Testing of NLP models with CheckList](https://aclanthology.org/2020.acl-main.442/)\").**\n",
    "- **`DeepWordBugGao2018`: this attack performs simple character-level transformations (_changes certain letters of a word_) to the highest-ranked tokens (proposed in [Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers](https://arxiv.org/abs/1801.04354)).**\n",
    "- **`IGAWang2019`: this attack can be characterized as a synonym substitution-based attack that preserves the syntactic structure and semantic information of the original text (proposed in [Natural Language Adversarial Attacks and Defenses in Word Level](http://arxiv.org/abs/1909.06723)).**\n",
    "- **`InputReductionFeng2018`: this attack does not cause the model to misclassify a sample. However, it removes words with low saliency scores, creating nonsensical sentences that the model classifies with high confidence as the original predicted class (proposed in [Pathologies of Neural Models Make Interpretations Difficult](https://arxiv.org/abs/1804.07781)).**\n",
    "- **`Pruthi2019`: this attack focuses on a small number of character-level changes that simulate common typos, like _swapping neighboring characters, deleting characters, inserting characters,_ and _swapping characters for adjacent keys_ on a QWERTY keyboard (proposed in [Pruthi2019: Combating with Robust Word Recognition](https://arxiv.org/abs/1905.11268)).**\n",
    "- **`TextBuggerLi2018`: this is a general attack framework for generating adversarial texts (proposed in [TextBugger: Generating Adversarial Text Against Real-world Applications](https://arxiv.org/abs/1812.05271)).**\n",
    "\n",
    "**In the example below, we will use the `IGAWang2019` recipe.**\n",
    "\n",
    "**The `Attacker` class also accepts additional arguments (full list [here](https://textattack.readthedocs.io/en/latest/api/attacker.html#attackargs)). Below we are passing a `log_to_csv ` argument equal to the name of `.csv` file (all attacks will be saved in this file).**\n",
    "\n",
    "**For clarity purposes, all perturbed words are highlighted with [[ ]].**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'keras.engine.functional.Functional'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "textattack: Logging to CSV at path textattack_logs_IGAWang2019.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): ImprovedGeneticAlgorithm(\n",
      "    (pop_size):  60\n",
      "    (max_iters):  20\n",
      "    (temp):  0.3\n",
      "    (give_up_if_no_improvement):  False\n",
      "    (post_crossover_check):  False\n",
      "    (max_crossover_retries):  20\n",
      "    (max_replace_times_per_index):  5\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  WordSwapEmbedding(\n",
      "    (max_candidates):  50\n",
      "    (embedding):  WordEmbedding\n",
      "  )\n",
      "  (constraints): \n",
      "    (0): MaxWordsPerturbed(\n",
      "        (max_percent):  0.2\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): WordEmbeddingDistance(\n",
      "        (embedding):  WordEmbedding\n",
      "        (max_mse_dist):  0.5\n",
      "        (cased):  False\n",
      "        (include_unknown_words):  True\n",
      "        (compare_against_original):  False\n",
      "      )\n",
      "    (2): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  17%|‚ñà‚ñã        | 1/6 [00:00<00:02,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[0 (95%)]] --> [[1 (74%)]]\n",
      "\n",
      "this explanation is really [[bad]]\n",
      "\n",
      "this explanation is really [[adverse]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [00:01<00:02,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[0 (93%)]] --> [[1 (79%)]]\n",
      "\n",
      "this tutorial is [[garbage]] i wont my [[money]] back\n",
      "\n",
      "this tutorial is [[detritus]] i wont my [[financial]] back\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 1 / 0 / 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:02<00:02,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[0 (81%)]] --> [[[FAILED]]]\n",
      "\n",
      "i did not like this tutorial 2/10\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 1 / 0 / 4:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:03<00:01,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[1 (97%)]] --> [[0 (69%)]]\n",
      "\n",
      "is [[nice]] to see philosophers doing machine learning\n",
      "\n",
      "is [[handsome]] to see philosophers doing machine learning\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 1 / 0 / 5:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:04<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[1 (100%)]] --> [[0 (61%)]]\n",
      "\n",
      "this is a [[great]] and [[wonderful]] example of nlp\n",
      "\n",
      "this is a [[enormous]] and [[unbelievable]] example of nlp\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 1 / 0 / 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:06<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[1 (100%)]] --> [[0 (82%)]]\n",
      "\n",
      "this tutorial is [[great]] [[one]] of the [[best]] tutorials ever made\n",
      "\n",
      "this tutorial is [[giant]] [[eden]] of the [[higher]] tutorials ever made\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 5      |\n",
      "| Number of failed attacks:     | 1      |\n",
      "| Number of skipped attacks:    | 0      |\n",
      "| Original accuracy:            | 100.0% |\n",
      "| Accuracy under attack:        | 16.67% |\n",
      "| Attack success rate:          | 83.33% |\n",
      "| Average perturbed word %:     | 20.84% |\n",
      "| Average num. words per input: | 8.33   |\n",
      "| Avg num queries:              | 398.33 |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x182c7b2adf0>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x182a12eba00>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x182962be880>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x182c6fc74f0>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x182aa7a8be0>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x1829c6db520>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapper = ModelWrapper(model)\n",
    "\n",
    "import textattack\n",
    "from textattack.attack_recipes import IGAWang2019\n",
    "from textattack import Attacker\n",
    "\n",
    "data = [\n",
    "    ('this explanation is really bad', 0),\n",
    "    ('this tutorial is garbage i wont my money back', 0),\n",
    "    ('i did not like this tutorial 2/10', 0),\n",
    "    ('is nice to see philosophers doing machine learning', 1),\n",
    "    ('this is a great and wonderful example of nlp', 1),\n",
    "    ('this tutorial is great one of the best tutorials ever made', 1)\n",
    "]\n",
    "\n",
    "dataset = textattack.datasets.Dataset(data)\n",
    "attack = IGAWang2019.build(model_wrapper)\n",
    "attack_args = textattack.AttackArgs(\n",
    "    num_examples=6,\n",
    "    log_to_csv =\"textattack_logs_IGAWang2019.csv\"\n",
    ")\n",
    "attacker = Attacker(attack, dataset, attack_args)\n",
    "attacker.attack_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let us try another recipe!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Unknown if model of class <class 'keras.engine.functional.Functional'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "textattack: Logging to CSV at path textattack_logs_DeepWordBugGao2018.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): GreedyWordSwapWIR(\n",
      "    (wir_method):  unk\n",
      "  )\n",
      "  (goal_function):  UntargetedClassification\n",
      "  (transformation):  CompositeTransformation(\n",
      "    (0): WordSwapNeighboringCharacterSwap(\n",
      "        (random_one):  True\n",
      "      )\n",
      "    (1): WordSwapRandomCharacterSubstitution(\n",
      "        (random_one):  True\n",
      "      )\n",
      "    (2): WordSwapRandomCharacterDeletion(\n",
      "        (random_one):  True\n",
      "      )\n",
      "    (3): WordSwapRandomCharacterInsertion(\n",
      "        (random_one):  True\n",
      "      )\n",
      "    )\n",
      "  (constraints): \n",
      "    (0): LevenshteinEditDistance(\n",
      "        (max_edit_distance):  30\n",
      "        (compare_against_original):  True\n",
      "      )\n",
      "    (1): RepeatModification\n",
      "    (2): StopwordModification\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  17%|‚ñà‚ñã        | 1/6 [00:00<00:01,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[[0 (95%)]] --> [[1 (74%)]]\n",
      "\n",
      "this explanation is really [[bad]]\n",
      "\n",
      "this explanation is really [[baEd]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  33%|‚ñà‚ñà‚ñà‚ñé      | 2/6 [00:00<00:01,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[[0 (93%)]] --> [[1 (56%)]]\n",
      "\n",
      "this tutorial is [[garbage]] i wont my [[money]] back\n",
      "\n",
      "this tutorial is [[garabge]] i wont my [[lmoney]] back\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 1 / 0 / 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3/6 [00:01<00:01,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[[0 (81%)]] --> [[[FAILED]]]\n",
      "\n",
      "i did not like this tutorial 2/10\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 1 / 0 / 4:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 4/6 [00:01<00:00,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[[1 (97%)]] --> [[0 (65%)]]\n",
      "\n",
      "is [[nice]] to see philosophers doing machine learning\n",
      "\n",
      "is [[ince]] to see philosophers doing machine learning\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 2 / 0 / 5:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 5/6 [00:01<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[[1 (100%)]] --> [[[FAILED]]]\n",
      "\n",
      "this is a great and wonderful example of nlp\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 2 / 0 / 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[[1 (100%)]] --> [[0 (55%)]]\n",
      "\n",
      "this tutorial is [[great]] [[one]] of the [[best]] tutorials [[ever]] made\n",
      "\n",
      "this tutorial is [[grea]] [[on]] of the [[bYest]] tutorials [[Vever]] made\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 4      |\n",
      "| Number of failed attacks:     | 2      |\n",
      "| Number of skipped attacks:    | 0      |\n",
      "| Original accuracy:            | 100.0% |\n",
      "| Accuracy under attack:        | 33.33% |\n",
      "| Attack success rate:          | 66.67% |\n",
      "| Average perturbed word %:     | 22.77% |\n",
      "| Average num. words per input: | 8.33   |\n",
      "| Avg num queries:              | 15.67  |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x182a217d100>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x182c8e82370>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x182c8aaae50>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x182c6a775b0>,\n",
       " <textattack.attack_results.failed_attack_result.FailedAttackResult at 0x182a179c190>,\n",
       " <textattack.attack_results.successful_attack_result.SuccessfulAttackResult at 0x182a28c5a90>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textattack.attack_recipes import DeepWordBugGao2018\n",
    "\n",
    "attack = DeepWordBugGao2018.build(model_wrapper)\n",
    "attack_args = textattack.AttackArgs(\n",
    "    num_examples=6,\n",
    "    log_to_csv =\"textattack_logs_DeepWordBugGao2018.csv\"\n",
    ")\n",
    "attacker = Attacker(attack, dataset, attack_args)\n",
    "attacker.attack_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Language models are the foundation behind various applications such as Q&A, chatbots, machine translation, and text classification. However, the security vulnerabilities associated with ML-trained language models are still largely unknown, which is highly concerning.**\n",
    "\n",
    "**To remedy this, developers must use the same tools that attackers use to fool models. For example, creating adversarial examples with libraries like `textattack` (_which also provide data augmentation_) can supply adversarial databases to tune and improve language models, making them more robust.**\n",
    "\n",
    "**At the same time, other strategies are possible. As demonstrated by [Xiaosen Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang%2C+X), [Hao Jin](https://arxiv.org/search/cs?searchtype=author&query=Jin%2C+H), [Yichen Yang](https://arxiv.org/search/cs?searchtype=author&query=Yang%2C+Y), and [Kun He](https://arxiv.org/search/cs?searchtype=author&query=He%2C+K), since most of the attacks used in the literature are synonym-based attacks, [Synonym Encoding Methods](https://arxiv.org/abs/1812.05271) can help models to cluster synonyms to a unique encoding, thus eliminating possible adversarial perturbations.**\n",
    "\n",
    "---\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
