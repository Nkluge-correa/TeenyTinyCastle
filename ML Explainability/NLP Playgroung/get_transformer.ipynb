{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Language Models from Hugging Face ðŸ¤—\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n",
    "\n",
    "**Whit this notebook, you can specify which models you want to download from the [Hugging Face library](https://github.com/huggingface/transformers). The models can be downloaded and saved for future use (the original files will be cached in your `.cache\\huggingface` local folder). Models are saved in a `.pt` format (_machine learning model created using PyTorch_). Tokenizers are saved in a separate folder. Both will be saved (in this format) in your local folde enviormnet.**\n",
    "\n",
    "**You an use the language model trough an UI by using the `playground.py` (a `Dash.app`for colecting `prompts + generated_responses` form language models). Just run the `playground.py` to start a dash application on your localhost ([http://127.0.0.1:8050/](http://127.0.0.1:8050/)).**\n",
    "\n",
    "**First, we load the `models + tokenizer` straigth form `Huggingface`. You can also use the `pipeline` class to create a `generator` function for your model.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer,\n",
    "                     device=0 if torch.cuda.is_available() else -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The created `generator` can then be used to sample text by tunning the many arguments thatr control the sampling policy of the `generator`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator('Distilgpt2 is a language model that can',\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.3,\n",
    "    num_return_sequences=2,\n",
    "    top_k=10,\n",
    "    repetition_penalty=1.5)\n",
    "\n",
    "for i,_ in enumerate(output):\n",
    "    print(f'Generated Response [{i+1}]\\n')\n",
    "    print(output[i]['generated_text'] + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now you can save the `model + tokenizer` in your local enviroment for future use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'Distilgpt2.pt')\n",
    "tokenizer.save_pretrained('Distilgpt2_tokenizer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now you can load the model directly, to experiments, fine-tunning, and save it again as you wish.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "model = torch.load('Distilgpt2.pt')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Distilgpt2_tokenizer')\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer,\n",
    "                     device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "output = generator('Distilgpt2 is a language model that can',\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.3,\n",
    "    num_return_sequences=2,\n",
    "    top_k=10,\n",
    "    repetition_penalty=1.5)\n",
    "\n",
    "for i,_ in enumerate(output):\n",
    "    print(f'Generated Response [{i+1}]\\n')\n",
    "    print(output[i]['generated_text'] + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
