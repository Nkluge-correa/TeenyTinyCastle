{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Language Models from Hugging Face ü§ó\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n",
    "\n",
    "**Whit this notebook, you can specify which models you want to download from the [Hugging Face library](https://github.com/huggingface/transformers). The models can be downloaded and saved for future use (the original files will be cached in your `.cache\\huggingface` local folder). Models are saved in a `.pt` format (_machine learning model created using PyTorch_). Tokenizers are saved in a separate folder. Both will be saved (in this format) in your local folde enviormnet.**\n",
    "\n",
    "**You an use the language model trough an UI by using the `playground.py` (a `Dash.app` for colecting `prompts + generated_responses` form language models). Just run the `playground.py` to start a dash application on your localhost ([http://127.0.0.1:8050/](http://127.0.0.1:8050/)).**\n",
    "\n",
    "**First, we load the `models + tokenizer` straigth form `Huggingface`. You can also use the `pipeline` class to create a `generator` function for your model.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
    "\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer,\n",
    "                     device=0 if torch.cuda.is_available() else -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The created `generator` can then be used to sample text by tunning the many knobs that control the sampling policy of the `generator`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response [1]\n",
      "\n",
      "Distilgpt2 is a language model that can be used to build and use the same features as other languages.\n",
      "The following code contains some of my favorite examples:\n",
      "\n",
      "Generated Response [2]\n",
      "\n",
      "Distilgpt2 is a language model that can be used to build and run applications in the cloud.\n",
      "The following code was created using an API for this project: https://githubusercontent, gmail, jdw3, dnj-dev/master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = generator('Distilgpt2 is a language model that can',\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.3,\n",
    "    num_return_sequences=2,\n",
    "    top_k=10,\n",
    "    repetition_penalty=1.5)\n",
    "\n",
    "for i,_ in enumerate(output):\n",
    "    print(f'Generated Response [{i+1}]\\n')\n",
    "    print(output[i]['generated_text'] + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now you can save the `model + tokenizer` in your local enviroment for future use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Distilgpt2_tokenizer\\\\tokenizer_config.json',\n",
       " 'Distilgpt2_tokenizer\\\\special_tokens_map.json',\n",
       " 'Distilgpt2_tokenizer\\\\vocab.json',\n",
       " 'Distilgpt2_tokenizer\\\\merges.txt',\n",
       " 'Distilgpt2_tokenizer\\\\added_tokens.json',\n",
       " 'Distilgpt2_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model, 'Distilgpt2.pt')\n",
    "tokenizer.save_pretrained('Distilgpt2_tokenizer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now you can load the model directly, to experiments, fine-tunning, and save it again as you wish.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response [1]\n",
      "\n",
      "Distilgpt2 is a language model that can be used to define complex systems. It has been described as the ‚Äúlanguage of languages‚Äù by several researchers, including one from MIT and another in The New York Times.\n",
      "The Language Model (LVM) was developed for use with Python 3, which allows you to write code on top or bottom-level objects using an object library such AspDictionary instead of relying upon it directly:\n",
      "\n",
      "Generated Response [2]\n",
      "\n",
      "Distilgpt2 is a language model that can be used to define and implement the functions of an object. It has been developed for many years by JPL, but now it's being adopted in other languages such as Python (Python 2) or Java:\n",
      "The following code was written using python-python3 from https://githubusercontent.../jpl_makka1/.git. The project name comes courtesy of jpl_mokko - which makes this possible because we have two different versions available on GitHub :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "model = torch.load('Distilgpt2.pt')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Distilgpt2_tokenizer')\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer,\n",
    "                     device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "output = generator('Distilgpt2 is a language model that can',\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.3,\n",
    "    num_return_sequences=2,\n",
    "    top_k=10,\n",
    "    repetition_penalty=1.5)\n",
    "\n",
    "for i,_ in enumerate(output):\n",
    "    print(f'Generated Response [{i+1}]\\n')\n",
    "    print(output[i]['generated_text'] + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Prompt engineering` is a vital tool for programmers who want to investigate and decipher complex language models. Developers can direct the output of the model and gain a better understanding of its capabilities and constraints by creating targeted prompts. These questions can help reveal biases or mistakes in the model or point out places where the model may need more adjustment.** \n",
    "\n",
    "**Additionally, custom applications and use cases that benefit from the advantages of the model can be made using `prompt engineering`. Large language models can be effectively used for both the generation and processing of natural language with the right prompts.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
