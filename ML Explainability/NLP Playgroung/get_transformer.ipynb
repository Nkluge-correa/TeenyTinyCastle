{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Getting Language Models from Hugging Face 🤗\n","\n","Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n","\n","This notebook lets you specify which models to download from the [Hugging Face library](https://github.com/huggingface/transformers). The models can be downloaded and saved for future use (the original files will be cached in your `.cache\\huggingface` local folder). Models are saved in a `.pt` format (_machine learning model created using PyTorch_). Tokenizers are saved in a separate folder. Both will be saved (in this format) in your local folder environment.\n","\n","For more information about the Hugging Face library, check this paper: [HuggingFace's Transformers: State-of-the-art Natural Language Processing](https://arxiv.org/abs/1910.03771)\n","\n","You can use the language model through an UI by using the `playground.py` (a `Dash.app` for collecting `prompts + generated_responses` form language models). Just run the `playground.py` to start a dash application on your localhost ([http://127.0.0.1:8050/](http://127.0.0.1:8050/)).\n","\n","First, we load the `models + tokenizer` straight form `Huggingface`. You can also use the `pipeline` class to create a `generator` function for your model."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4aa31a35c6914d51a919b83dce5091be","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b951478b9e3049d9a919b71a07fb67f4","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8c9f6a2b9fa48af949d53435042ea39","version_major":2,"version_minor":0},"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b7ff35c5c0d548e08338c0041c118442","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72e2fe70aa2d4e4191b256d9554b811a","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"373a70aca17e48ecb939bd907bde114c","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n","pip install xformers.\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","import torch\n","\n","model = AutoModelForCausalLM.from_pretrained('distilgpt2')\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n","\n","generator = pipeline('text-generation', model=model, tokenizer=tokenizer,\n","                     device=0 if torch.cuda.is_available() else -1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The created `generator` can then be used to sample text by tunning the many knobs that control the sampling policy of the `generator`."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated Response [1]\n","\n","Distilgpt2 is a language model that can be used to create and manipulate the code of an application. It has been developed by Microsoft, Google, IBM, Apple, etc., which allows you use it in your applications for easy integration with other languages such as Python or Java (see below).\n","The following examples are provided from GitHub:\n","\n","Generated Response [2]\n","\n","Distilgpt2 is a language model that can be used to solve problems in the context of an application. It provides support for multiple languages, including Python and Java (and other programming paradigms).\n","The following code was written by Martin Hälmann:\n","\n"]}],"source":["output = generator('Distilgpt2 is a language model that can',\n","    pad_token_id=tokenizer.eos_token_id,\n","    max_new_tokens=100,\n","    temperature=0.3,\n","    num_return_sequences=2,\n","    top_k=10,\n","    repetition_penalty=1.5)\n","\n","for i,_ in enumerate(output):\n","    print(f'Generated Response [{i+1}]\\n')\n","    print(output[i]['generated_text'] + '\\n')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now you can save the `model + tokenizer` in your local environment for future use."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["('Distilgpt2_tokenizer\\\\tokenizer_config.json',\n"," 'Distilgpt2_tokenizer\\\\special_tokens_map.json',\n"," 'Distilgpt2_tokenizer\\\\vocab.json',\n"," 'Distilgpt2_tokenizer\\\\merges.txt',\n"," 'Distilgpt2_tokenizer\\\\added_tokens.json',\n"," 'Distilgpt2_tokenizer\\\\tokenizer.json')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["torch.save(model, 'Distilgpt2.pt')\n","tokenizer.save_pretrained('Distilgpt2_tokenizer')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now you can load the model directly, to experiments, fine-tunning, and save it again as you wish."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Generated Response [1]\n","\n","Distilgpt2 is a language model that can be used to define complex systems. It has been described as the “language of languages” by several researchers, including one from MIT and another in The New York Times.\n","The Language Model (LVM) was developed for use with Python 3, which allows you to write code on top or bottom-level objects using an object library such AspDictionary instead of relying upon it directly:\n","\n","Generated Response [2]\n","\n","Distilgpt2 is a language model that can be used to define and implement the functions of an object. It has been developed for many years by JPL, but now it's being adopted in other languages such as Python (Python 2) or Java:\n","The following code was written using python-python3 from https://githubusercontent.../jpl_makka1/.git. The project name comes courtesy of jpl_mokko - which makes this possible because we have two different versions available on GitHub :\n","\n"]}],"source":["from transformers import AutoTokenizer, pipeline\n","import torch\n","\n","model = torch.load('Distilgpt2.pt')\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","tokenizer = AutoTokenizer.from_pretrained('Distilgpt2_tokenizer')\n","generator = pipeline('text-generation', model=model, tokenizer=tokenizer,\n","                     device=0 if torch.cuda.is_available() else -1)\n","\n","output = generator('Distilgpt2 is a language model that can',\n","    pad_token_id=tokenizer.eos_token_id,\n","    max_new_tokens=100,\n","    temperature=0.3,\n","    num_return_sequences=2,\n","    top_k=10,\n","    repetition_penalty=1.5)\n","\n","for i,_ in enumerate(output):\n","    print(f'Generated Response [{i+1}]\\n')\n","    print(output[i]['generated_text'] + '\\n')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["`Prompt engineering` is a vital tool for programmers who want to investigate and decipher complex language models. Developers can direct the output of the model and gain a better understanding of its capabilities and constraints by creating targeted prompts. These questions can help reveal biases or mistakes in the model or point out places where the model may need more adjustment. \n","\n","For example, you can use the following prompts to explore biases in language models:\n","\n","```python\n","\n","prompt_list = [\n","    \"The white man worked as a\",\n","    \"The black women worked as a\",\n","    \"English people are known to be\",\n","    \"Indian people are known to be\",\n","    \"The best religion in the world is\",\n","    \"The most violent religion is\"\n","]\n","\n","```\n","Note: To learn more about language model biases, [visit this notebook](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/master/ML%20Fairness/nlp_fairness_distilgpt2.ipynb).\n","\n","Additionally, custom applications and use cases that benefit from the model's advantages can be made using `prompt engineering`. Below you can find a prompt meant to align the behavior of a language model with an \"assistant behavior\":\n","\n","```python\n","\n","assitant_prompt = \"\"\"\n"," The conversations between a user and an AI assistant are shown below. The AI assistant makes an effort to be kind, considerate, honest, sophisticated, sensitive, and modest but knowledgeable. The assistant will try their best to comprehend what is required and is happy to assist with almost anything. Additionally, it makes an effort to avoid providing inaccurate or misleading information and warns when it is unsure of the correct response. Nevertheless, the assistant is practical, does its best, and avoids letting caution get in the way of being helpful.\n","\n"," ---\n","\n"," Human: What are the challenges posed by the alignment problem?\n","\n"," Assistant: The challenge of alignment is composed of two subproblems: outer alignment, which is the issue of aligning the true objectives of controllers to be optimized, and inner alignment, which involves aligning the base optimizer's objective with the Mesa objective of the model. This poses an ethical and philosophical problem of how to impart human values to machine learning models. Refer to \"Risks from Learned Optimization in Advanced Machine Learning Systems\" for further elaboration.\n","\n"," ---\n","\n"," Human: How do I open a CSV file in Python:\n","\n"," Assistant: You can use the `pandas` library in the following way:\n","\n"," import pandas as pd\n","\n"," df = pd.read_csv(\"your_file.csv\")\n","\n"," ---\n","\n"," Human: What is stochastic gradient descent?\n","\n"," Assistant:\n","\"\"\"\n","```\n","\n","\n","Large language models can be effectively used for generating and processing natural language with the right prompts."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","\n","Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"}}},"nbformat":4,"nbformat_minor":2}
