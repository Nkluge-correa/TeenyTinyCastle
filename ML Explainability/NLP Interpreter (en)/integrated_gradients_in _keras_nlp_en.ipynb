{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating `Integrated Gradients` in Keras Language Models\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n",
    "\n",
    "**`Integrated Gradients` is a method to make a classification model interpretable, proposed in Sundararajan et al., [‚ÄúAxiomatic Attribution for Deep Networks‚Äù](https://arxiv.org/abs/1703.01365). This methodology uses the gradient to determine what _influence_ the individual inputs (_like words in a sentese_) have on the output of a model.**\n",
    "\n",
    "**This is similar to the concept of saliency maps, something we covered in [this](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/c603bb323ace7876d48abf8234d3f181c4b5d744/ML%20Explainability/CV%20Interpreter/CNN_attribution_maps_with_LIME.ipynb) (Grad-Cam) and [this](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/87b1e85fa62074af1459e863cac986dc973b4666/ML%20Explainability/CV%20Interpreter%20&%20Adversarial/lime_for_CV.ipynb) (LIME) notebook. `Integrated Gradients` is the same method we used to interpret the generated images of the `stable-diffusion` model in [this](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/cd18a9269ab705544f935dcf0a492f3d9222431e/ML%20Explainability/CV%20Interpreter/diffusion_interpreter.ipynb) notebook.**\n",
    "\n",
    "**We will be using the `alibi` library for this. [Alibi](https://docs.seldon.io/projects/alibi) is _\"an open source Python library aimed at machine learning model inspection and interpretation._\"**\n",
    "\n",
    "**In this notebook, we apply the integrated gradients method to a sentiment analysis model trained on a dataset we used in the `adversarial_text_attacks.ipynb'`, found [here](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/cd18a9269ab705544f935dcf0a492f3d9222431e/ML%20Adversarial/adversarial_text_attack.ipynb).**\n",
    "\n",
    "**The pre-trained model/tokenizer can be found in the `models folder`. The model comes in two versions. one with a sigmoid output, and the other with a softmax. Here we show how to use `IntegratedGradients` on both types of models.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is hard to say something about a model so simple\n",
      "\n",
      "Negative Sentiment üòî 100% | Positive Sentiment üòä 0%\n",
      "**************************************************\n",
      "this model is garbage, i wont my money back\n",
      "\n",
      "Negative Sentiment üòî 100% | Positive Sentiment üòä 0%\n",
      "**************************************************\n",
      "is nice to see philosophers doing machine learning\n",
      "\n",
      "Negative Sentiment üòî 0% | Positive Sentiment üòä 100%\n",
      "**************************************************\n",
      "this is a great and wonderful example of NLP\n",
      "\n",
      "Negative Sentiment üòî 0% | Positive Sentiment üòä 100%\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "\n",
    "model_path = 'models\\senti_model_sigmoid.h5'\n",
    "#model_path = 'models\\senti_model_softmax.h5'\n",
    "\n",
    "tokenizer_path = 'models\\\\tokenizer_senti_model_en.json'\n",
    "\n",
    "model = keras.models.load_model(model_path) \n",
    "\n",
    "with open(tokenizer_path) as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "\n",
    "strings = [\n",
    "    'is hard to say something about a model so simple',\n",
    "    'this model is garbage, i wont my money back',\n",
    "    'is nice to see philosophers doing machine learning',\n",
    "    'this is a great and wonderful example of NLP',\n",
    "]\n",
    "\n",
    "preds = model.predict(\n",
    "        keras.preprocessing.sequence.pad_sequences(\n",
    "                                                    tokenizer.texts_to_sequences(strings),\n",
    "                                                    maxlen=256,\n",
    "                                                    truncating='post'\n",
    "                                                ),\n",
    "    verbose=0)\n",
    "\n",
    "for i, string in enumerate(strings):\n",
    "    print(f'{string}\\n')\n",
    "    \n",
    "    # for sigmoid model\n",
    "    print(f'Negative Sentiment üòî {round((1 - preds[i][0]) * 100)}% | Positive Sentiment üòä {round(preds[i][0] * 100)}%\\n{\"*\" * 50}')\n",
    "\n",
    "    # for softmax model\n",
    "    #print(f'Negative Sentiment üòî {round((preds[i][0]) * 100)}% | Positive Sentiment üòä {round(preds[i][1] * 100)}%\\n{\"*\" * 50}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In language models, like text classification models, integrated gradients define an attribution value for each word in the input sentence. The attributions are calculated considering the integral of the model gradients with respect to the word embedding layer along a straight path from a baseline instance $x^‚Ä≤$ to the input instance $x$.**\n",
    "\n",
    "**Thus we can say the attribution given to an input is equal to the difference between the model output at the instance $x$ and the model output at the baseline $x^‚Ä≤$:**\n",
    "\n",
    "$$\n",
    "A(x, x') = F(x) -  F(x')\n",
    "$$\n",
    "\n",
    "**To utilize the `IntegratedGradients`class from `alibi`, we need to set some arguments first:**\n",
    "\n",
    "-   `model`: Tensorflow or Keras model.\n",
    "-   `layer`: Layer with respect to which the gradients are calculated. In the case of our language model, is the `Embedding` layer.\n",
    "-   `target_fn`: A scalar function that is applied to the predictions of the model (like ` np.argmax(predictions, axis=1)`).\n",
    "-   `method`: Method for the integral approximation (`riemann_left`,  `riemann_right`,  `riemann_middle`,  `riemann_trapezoid`,  `gausslegendre`).\n",
    "-   `n_steps`: Number of step in the path integral approximation from the baseline to the input instance.  \n",
    "-   `internal_batch_size`: Batch size for the internal batching.\n",
    "\n",
    "\n",
    "**Since the model uses a word to vector embedding with vector dimensionality of 50 and sequence length of 256 words, the dimensionality of the attributions is `(len(x_test_sample), 256, 50)` In order to obtain a single attribution value for each word, we sum all the attribution values for the 50 elements of each word‚Äôs vector representation.**\n",
    "\n",
    "**Bellow we create an `IntegratedGradients` object (`ig`) with these parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer = model.layers[1]\n",
    "n_steps = 50\n",
    "internal_batch_size = 256\n",
    "\n",
    "from alibi.explainers import IntegratedGradients\n",
    "\n",
    "ig  = IntegratedGradients(model,\n",
    "                        target_fn=None,\n",
    "                        layer=layer,\n",
    "                        n_steps=n_steps,\n",
    "                        method=\"gausslegendre\",\n",
    "                        internal_batch_size=internal_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**The integrated gradient attributions are calculated concerning the embedding layer for the number of samples we defined in our `x_test_sample` list. This could also be a partition if your testing set.**\n",
    "\n",
    "**With these samples, we use our model to generate a prediction array (`preds`), calling all the functions that turn/pad a `string` into a `sequence of tokens`.**\n",
    "\n",
    "**`ig.explain` (the actual explanation of our model), requires a list of elements (predicted_classes) of the model's output so it can compute the gradients.** We can achieve this by \"argmaxing\" the `preds` array, or by passing the `preds.argmax(axis=1)`function as the `target` parameter.\n",
    "\n",
    "**Here we are using the default baseline (`None`), which equates to a sequence of zeros (_this corresponds to a sequence of padding characters, a.k.a. no input_). The path integral is defined as a straight line from the baseline ($x$) to the input sample ($x'$).**\n",
    "\n",
    "**If you are using a model with a `softmax` output, you can set the `target` parameter to something like `preds.argmax(axis=1)`. If you are using a model with a `sigmoid` output, some basic list comprehension (`[0 if preds[i][0] < 0.5 else 1 for i in range(len(preds))]`) can give you list of predicted classes for your samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'IntegratedGradients',\n",
       " 'type': ['whitebox'],\n",
       " 'explanations': ['local'],\n",
       " 'params': {'target_fn': None,\n",
       "  'method': 'gausslegendre',\n",
       "  'n_steps': 50,\n",
       "  'internal_batch_size': 256,\n",
       "  'layer': 1},\n",
       " 'version': '0.8.0'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_sample = [\n",
    "    'One of the weakest entries in the J-horror remake sweepstakes, One Missed Call is undone by bland performances and shopworn shocks.'\n",
    "]\n",
    "preds = model.predict(\n",
    "        keras.preprocessing.sequence.pad_sequences(\n",
    "                                                    tokenizer.texts_to_sequences(x_test_sample),\n",
    "                                                    maxlen=256,\n",
    "                                                    truncating='post'\n",
    "                                                ),\n",
    "    verbose=0)\n",
    "\n",
    "target_function =  [0 if preds[i][0] < 0.5 else 1 for i in range(len(preds))]\n",
    "#target_function =  preds.argmax(axis=1) \n",
    "\n",
    "explanation = ig.explain(keras.preprocessing.sequence.pad_sequences(\n",
    "                                                    tokenizer.texts_to_sequences(x_test_sample),\n",
    "                                                    maxlen=256,\n",
    "                                                    truncating='post'\n",
    "                                                ),\n",
    "                         baselines=None,\n",
    "                         target=target_function,\n",
    "                         \n",
    "                         attribute_to_layer_inputs=False)\n",
    "explanation.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From this explanation object, we can recover a lot of useful information.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Classes\n",
      "[0]\n",
      "\n",
      "Embedded inputs\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0   28    6    2    1\n",
      "     1    9    2 1745  204 1164    1   28 1041  495    7    1   35 2151\n",
      "   425    3    1    1]]\n",
      "\n",
      "Baselines\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n",
      "\n",
      "The predictions of the model\n",
      "[[0.00025303]]\n"
     ]
    }
   ],
   "source": [
    "print('Target Classes')\n",
    "print(explanation.target)\n",
    "print('\\nEmbedded inputs')\n",
    "print(explanation.X)\n",
    "print('\\nBaselines')\n",
    "print(explanation.baselines)\n",
    "print('\\nThe predictions of the model')\n",
    "print(explanation.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But attributions are what we are interested in. And we can also retrieve this from our `explanation` object. We also need to sum all the attribution scores related to each of our embeddings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions shape: (1, 256)\n"
     ]
    }
   ],
   "source": [
    "attrs = explanation.attributions[0]\n",
    "attrs = attrs.sum(axis=2)\n",
    "print('Attributions shape:', attrs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we get rid of the padding tokens and take only the attributions that relate to the words in our input. This input (_given by the way the model was constructed_) has a limit of 256 tokens (_which in this case are words_).**\n",
    "\n",
    "**Since our testing_set has only one sample (`x_test_sample`), the index of this sample is 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 21)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 0\n",
    "words = x_test_sample[sample].split()\n",
    "\n",
    "if len(words) < len(attrs[sample]):\n",
    "    atributions = attrs[sample][-len(words):]\n",
    "else:\n",
    "    atributions = attrs[sample]\n",
    "    words = words[-len(atributions):]\n",
    "\n",
    "len(words), len(atributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To crate a visually intuitive way of interpreting this model's output, we will assign a color to each of the attribution scores, taking the `max` and `min` values to set a range of predefined colors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#fffdc6',\n",
       " '#fffdc6',\n",
       " '#fffdc6',\n",
       " '#fffdc6',\n",
       " '#fffdc6',\n",
       " '#fffdc6',\n",
       " '#fffdc6',\n",
       " '#fffdc6',\n",
       " '#ffffcc',\n",
       " '#fffdc8',\n",
       " '#fffcc4',\n",
       " '#fffbc2',\n",
       " '#fffec9',\n",
       " '#fff7b7',\n",
       " '#fff9be',\n",
       " '#ffeea3',\n",
       " '#cd0b22',\n",
       " '#800026',\n",
       " '#feb852',\n",
       " '#febb56',\n",
       " '#febb56']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from IPython.display import HTML\n",
    "\n",
    "minima = min(atributions)\n",
    "maxima = max(atributions)\n",
    "\n",
    "norm = matplotlib.colors.Normalize(vmin=minima, vmax=maxima, clip=True)\n",
    "mapper = cm.ScalarMappable(norm=norm, cmap=cm.YlOrRd)\n",
    "\n",
    "colors = [mcolors.to_hex(mapper.to_rgba(v)) for v in atributions]\n",
    "\n",
    "colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below we create a function that maps the generated colors to each work in the text sample, generating a colorful HTML representation of the attributions given to each word. Since we are using the `YlOrRd` color scale, words with _high positive attribution_ are colored in shades of _red_. Words with _middling attributions_ are colored _orange_, while _low attributions_ receive a _pale yellow_.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample\n",
      "**************************************************\n",
      "\n",
      "One of the weakest entries in the J-horror remake sweepstakes, One Missed Call is undone by bland performances and shopworn shocks.\n",
      "\n",
      "Prediction\n",
      "**************************************************\n",
      "\n",
      "Negative Sentiment üòî 100% | Positive Sentiment üòä 0%\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Attributions: <span style=\"color:#fffdc6\"><b>One</b></span> <span style=\"color:#fffdc6\"><b>of</b></span> <span style=\"color:#fffdc6\"><b>the</b></span> <span style=\"color:#fffdc6\"><b>weakest</b></span> <span style=\"color:#fffdc6\"><b>entries</b></span> <span style=\"color:#fffdc6\"><b>in</b></span> <span style=\"color:#fffdc6\"><b>the</b></span> <span style=\"color:#fffdc6\"><b>J-horror</b></span> <span style=\"color:#ffffcc\"><b>remake</b></span> <span style=\"color:#fffdc8\"><b>sweepstakes,</b></span> <span style=\"color:#fffcc4\"><b>One</b></span> <span style=\"color:#fffbc2\"><b>Missed</b></span> <span style=\"color:#fffec9\"><b>Call</b></span> <span style=\"color:#fff7b7\"><b>is</b></span> <span style=\"color:#fff9be\"><b>undone</b></span> <span style=\"color:#ffeea3\"><b>by</b></span> <span style=\"color:#cd0b22\"><b>bland</b></span> <span style=\"color:#800026\"><b>performances</b></span> <span style=\"color:#feb852\"><b>and</b></span> <span style=\"color:#febb56\"><b>shopworn</b></span> <span style=\"color:#febb56\"><b>shocks.</b></span>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_with_attributions = ' '.join([f'''<span style=\"color:{colors[i]}\"><b>{words[i]}</b></span>''' for i in range(len(words))])\n",
    "\n",
    "print(f'Sample\\n{\"*\" * 50}\\n\\n{x_test_sample[sample]}\\n')\n",
    "print(f'Prediction\\n{\"*\" * 50}\\n')\n",
    "\n",
    "# for softmax model\n",
    "#print(f'Negative Sentiment üòî {round((explanation.predictions[sample][0]) * 100)}% | Positive Sentiment üòä {round(explanation.predictions[sample][1] * 100)}%\\n\\n{\"*\" * 50}')\n",
    "\n",
    "# for sigmoid model\n",
    "print(f'Negative Sentiment üòî {round((1 - explanation.predictions[sample][0]) * 100)}% | Positive Sentiment üòä {round(explanation.predictions[sample][0] * 100)}%\\n{\"*\" * 50}')\n",
    "display(HTML(f'Attributions: {text_with_attributions}.'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above you can see what are the tokens that had _a greater influence on the prediction of our sentiment classifier_.** üé≠\n",
    "\n",
    "----\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
