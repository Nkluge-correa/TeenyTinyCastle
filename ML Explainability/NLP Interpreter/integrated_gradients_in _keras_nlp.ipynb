{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating `Integrated Gradients` in Keras Language Models\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n",
    "\n",
    "**`Integrated Gradients` is a method to make a classification model interpretable, proposed in Sundararajan et al., \"_[Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365)_\". This methodology uses the gradient to determine what _influence_ the individual parts of an input (_like words in a sentese, or pixels in an image_) have on the output of a model.**\n",
    "\n",
    "**This is similar to the concept of saliency maps, something we covered in [this](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/fa17764aa8800c388d0d298b750c686757e0861e/ML%20Explainability/NLP%20Interpreter/integrated_gradients_in%20_keras_nlp.ipynb) (Grad-Cam) and [this](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/fa17764aa8800c388d0d298b750c686757e0861e/ML%20Explainability/NLP%20Interpreter/lime_for_NLP.ipynb) (LIME) notebook. `Integrated Gradients` is the same method we used to interpret the generated images of the `stable-diffusion` model in [this](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/fa17764aa8800c388d0d298b750c686757e0861e/ML%20Explainability/CV%20Interpreter/diffusion_interpreter.ipynb) notebook.**\n",
    "\n",
    "**We will be using the `alibi` library for this. [Alibi](https://docs.seldon.io/projects/alibi) is _\"an open source Python library aimed at machine learning model inspection and interpretation._\"**\n",
    "\n",
    "**In this notebook, we apply the integrated gradients method to a sentiment analysis model trained in [this notebook](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/fa17764aa8800c388d0d298b750c686757e0861e/ML%20Explainability/NLP%20Interpreter/model_maker.ipynb).**\n",
    "\n",
    "**The trained model/tokenizer can be found in the `models folder`. The model comes in two versions. one with a sigmoid output, and the other with a softmax. Here we show how to use `IntegratedGradients` on both types of models.**\n",
    "\n",
    "**If you do not want to train the models, you can load the trained versions in the cell below. But first, you need to download them (instructions in the `models` folder.)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \"this explanation is really bad\"\n",
      "(Negative ðŸ˜” 95% | Positive ðŸ˜Š 5%)\n",
      "\n",
      "Review: \"i did not like this tutorial 2/10\"\n",
      "(Negative ðŸ˜” 81% | Positive ðŸ˜Š 19%)\n",
      "\n",
      "Review: \"this tutorial is garbage i wont my money back\"\n",
      "(Negative ðŸ˜” 93% | Positive ðŸ˜Š 7%)\n",
      "\n",
      "Review: \"is nice to see philosophers doing machine learning\"\n",
      "(Negative ðŸ˜” 3% | Positive ðŸ˜Š 97%)\n",
      "\n",
      "Review: \"this is a great and wonderful example of nlp\"\n",
      "(Negative ðŸ˜” 0% | Positive ðŸ˜Š 100%)\n",
      "\n",
      "Review: \"this tutorial is great one of the best tutorials ever made\"\n",
      "(Negative ðŸ˜” 0% | Positive ðŸ˜Š 100%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "\n",
    "model_path = 'models/senti_model_sigmoid_en.keras'\n",
    "#model_path = 'models/senti_model_softmax_en.keras'\n",
    "\n",
    "tokenizer_path = 'models/tokenizer_senti_model_en.json'\n",
    "\n",
    "model = tf.keras.models.load_model(model_path) \n",
    "\n",
    "with open(tokenizer_path) as fp:\n",
    "    data = json.load(fp)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "    word_index = tokenizer.word_index\n",
    "    fp.close()\n",
    "\n",
    "\n",
    "strings = [\n",
    "    'this explanation is really bad',\n",
    "    'i did not like this tutorial 2/10',\n",
    "    'this tutorial is garbage i wont my money back',\n",
    "    'is nice to see philosophers doing machine learning',\n",
    "    'this is a great and wonderful example of nlp',\n",
    "    'this tutorial is great one of the best tutorials ever made'\n",
    "]\n",
    "\n",
    "preds = model.predict(\n",
    "    tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences(strings),\n",
    "        maxlen=250,\n",
    "        truncating='post'\n",
    "    ), verbose=0)\n",
    "\n",
    "for i, string in enumerate(strings):\n",
    "    # for sigmoid model\n",
    "\n",
    "    print(f'Review: \"{string}\"\\n(Negative ðŸ˜” {round((1 - preds[i][0]) * 100)}% | Positive ðŸ˜Š {round(preds[i][0] * 100)}%)\\n')\n",
    "    \n",
    "    # for softmax model\n",
    "    #print(f'Review: \"{string}\"\\n(Negative ðŸ˜” {round((preds[i][0]) * 100)}% | Positive ðŸ˜Š {round(preds[i][1] * 100)}%)\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In language models, like text classification models, integrated gradients define an attribution value for each word in the input sentence. The attributions are calculated considering the integral of the model gradients with respect to the word embedding layer along a straight path from a baseline instance $x^â€²$ to the input instance $x$.**\n",
    "\n",
    "**Thus we can say the attribution given to an input is equal to the difference between the model output at the instance $x$ and the model output at the baseline $x^â€²$:**\n",
    "\n",
    "$$A(x, x') = F(x) -  F(x')$$\n",
    "\n",
    "**To utilize the `IntegratedGradients`class from `alibi`, we need to set some arguments first:**\n",
    "\n",
    "-   **`model`: Tensorflow or Keras model.**\n",
    "-   **`layer`: Layer with respect to which the gradients are calculated. In the case of our language model, is the `Embedding` layer.**\n",
    "-   **`target_fn`: A scalar function that is applied to the predictions of the model (like ` np.argmax(predictions, axis=1)`).**\n",
    "-   **`method`: Method for the integral approximation (`riemann_left`,  `riemann_right`,  `riemann_middle`,  `riemann_trapezoid`,  `gausslegendre`).**\n",
    "-   **`n_steps`: Number of step in the path integral approximation from the baseline to the input instance.**  \n",
    "-   **`internal_batch_size`: Batch size for the internal batching.**\n",
    "\n",
    "\n",
    "**Since the model uses a word to vector embedding with vector dimensionality of 50 and sequence length of 256 words, the dimensionality of the attributions is `(len(x_test_sample), 256, 50)` In order to obtain a single attribution value for each word, we sum all the attribution values for the 50 elements of each wordâ€™s vector representation.**\n",
    "\n",
    "**Bellow we create an `IntegratedGradients` object (`ig`) with these parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layer = model.layers[1]\n",
    "n_steps = 128\n",
    "internal_batch_size = 250\n",
    "\n",
    "from alibi.explainers import IntegratedGradients\n",
    "\n",
    "ig  = IntegratedGradients(model,\n",
    "                        target_fn=None,\n",
    "                        layer=layer,\n",
    "                        n_steps=n_steps,\n",
    "                        method=\"gausslegendre\",\n",
    "                        internal_batch_size=internal_batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**The integrated gradient attributions are calculated concerning the embedding layer for the number of samples we defined in our `x_test_sample` list. This could also be a partition if your testing set.**\n",
    "\n",
    "**With these samples, we use our model to generate a prediction array (`preds`), calling all the functions that turn/pad a `string` into a `sequence of tokens`.**\n",
    "\n",
    "**`ig.explain` (the actual explanation of our model), requires a list of elements (predicted_classes) of the model's output so it can compute the gradients.** We can achieve this by \"argmaxing\" the `preds` array, or by passing the `preds.argmax(axis=1)`function as the `target` parameter.\n",
    "\n",
    "**Here we are using the default baseline (`None`), which equates to a sequence of zeros (_this corresponds to a sequence of padding characters, a.k.a. no input_). The path integral is defined as a straight line from the baseline ($x$) to the input sample ($x'$).**\n",
    "\n",
    "**If you are using a model with a `softmax` output, you can set the `target` parameter to something like `preds.argmax(axis=1)`. If you are using a model with a `sigmoid` output, some basic list comprehension (`[0 if preds[i][0] < 0.5 else 1 for i in range(len(preds))]`) can give you list of predicted classes for your samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'IntegratedGradients',\n",
       " 'type': ['whitebox'],\n",
       " 'explanations': ['local'],\n",
       " 'params': {'target_fn': None,\n",
       "  'method': 'gausslegendre',\n",
       "  'n_steps': 128,\n",
       "  'internal_batch_size': 250,\n",
       "  'layer': 1},\n",
       " 'version': '0.8.0'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_sample = [\n",
    "    'One of the weakest entries in the J-horror remake sweepstakes, One Missed Call is undone by bland performances and shopworn shocks.'\n",
    "]\n",
    "preds = model.predict(\n",
    "    tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenizer.texts_to_sequences(x_test_sample),\n",
    "        maxlen=250,\n",
    "        truncating='post'\n",
    "    ), verbose=0)\n",
    "\n",
    "target_function =  [0 if preds[i][0] < 0.5 else 1 for i in range(len(preds))]\n",
    "#target_function =  preds.argmax(axis=1) \n",
    "\n",
    "explanation = ig.explain(tf.keras.preprocessing.sequence.pad_sequences(\n",
    "                            tokenizer.texts_to_sequences(x_test_sample),\n",
    "                            maxlen=250,\n",
    "                            truncating='post'),\n",
    "                         baselines=None,\n",
    "                         target=target_function,\n",
    "                         attribute_to_layer_inputs=False)\n",
    "explanation.meta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From this explanation object, we can recover a lot of useful information, but attributions are what we are interested in. And we can also retrieve this from our `explanation` object. We also need to sum all the attribution scores related to each of our embeddings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributions shape: (1, 250)\n"
     ]
    }
   ],
   "source": [
    "attrs = explanation.attributions[0]\n",
    "attrs = attrs.sum(axis=2)\n",
    "print('Attributions shape:', attrs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we get rid of the padding tokens and take only the attributions that relate to the words in our input. This input (_given by the way the model was constructed_) has a limit of 256 tokens (_which in this case are words_).**\n",
    "\n",
    "**Since our testing_set has only one sample (`x_test_sample`), the index of this sample is 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 21)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 0\n",
    "words = x_test_sample[sample].split()\n",
    "\n",
    "if len(words) < len(attrs[sample]):\n",
    "    atributions = attrs[sample][-len(words):]\n",
    "else:\n",
    "    atributions = attrs[sample]\n",
    "    words = words[-len(atributions):]\n",
    "\n",
    "len(words), len(atributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To crate a visually intuitive way of interpreting this model's output, we will assign a color to each of the attribution scores, taking the `max` and `min` values to set a range of predefined colors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#ffffcc',\n",
       " '#fffecb',\n",
       " '#fff8ba',\n",
       " '#fff6b6',\n",
       " '#ffffcc',\n",
       " '#fffec9',\n",
       " '#ffec9d',\n",
       " '#fff5b3',\n",
       " '#fee084',\n",
       " '#ffec9d',\n",
       " '#fff9bd',\n",
       " '#ffe58f',\n",
       " '#fd9e43',\n",
       " '#fff0a7',\n",
       " '#fee084',\n",
       " '#fedb7b',\n",
       " '#800026',\n",
       " '#ffea9b',\n",
       " '#fed06c',\n",
       " '#fd7435',\n",
       " '#e9261f']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "from IPython.display import HTML\n",
    "\n",
    "minima = min(atributions)\n",
    "maxima = max(atributions)\n",
    "\n",
    "norm = matplotlib.colors.Normalize(vmin=minima, vmax=maxima, clip=True)\n",
    "mapper = cm.ScalarMappable(norm=norm, cmap=cm.YlOrRd)\n",
    "\n",
    "colors = [mcolors.to_hex(mapper.to_rgba(v)) for v in atributions]\n",
    "\n",
    "colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below we create a function that maps the generated colors to each work in the text sample, generating a colorful HTML representation of the attributions given to each word. Since we are using the `YlOrRd` color scale, words with _high positive attribution_ are colored in shades of _red_. Words with _middling attributions_ are colored _orange_, while _low attributions_ receive a _pale yellow_.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: One of the weakest entries in the J-horror remake sweepstakes, One Missed Call is undone by bland performances and shopworn shocks.\n",
      "\n",
      "Prediction: (Negative ðŸ˜” 65% | Positive ðŸ˜Š 35%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Attributions: <span style=\"color:#ffffcc\"><b>One</b></span> <span style=\"color:#fffecb\"><b>of</b></span> <span style=\"color:#fff8ba\"><b>the</b></span> <span style=\"color:#fff6b6\"><b>weakest</b></span> <span style=\"color:#ffffcc\"><b>entries</b></span> <span style=\"color:#fffec9\"><b>in</b></span> <span style=\"color:#ffec9d\"><b>the</b></span> <span style=\"color:#fff5b3\"><b>J-horror</b></span> <span style=\"color:#fee084\"><b>remake</b></span> <span style=\"color:#ffec9d\"><b>sweepstakes,</b></span> <span style=\"color:#fff9bd\"><b>One</b></span> <span style=\"color:#ffe58f\"><b>Missed</b></span> <span style=\"color:#fd9e43\"><b>Call</b></span> <span style=\"color:#fff0a7\"><b>is</b></span> <span style=\"color:#fee084\"><b>undone</b></span> <span style=\"color:#fedb7b\"><b>by</b></span> <span style=\"color:#800026\"><b>bland</b></span> <span style=\"color:#ffea9b\"><b>performances</b></span> <span style=\"color:#fed06c\"><b>and</b></span> <span style=\"color:#fd7435\"><b>shopworn</b></span> <span style=\"color:#e9261f\"><b>shocks.</b></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "color": [
           "#ffffcc",
           "#fffecb",
           "#fff8ba",
           "#fff6b6",
           "#ffffcc",
           "#fffec9",
           "#ffec9d",
           "#fff5b3",
           "#fee084",
           "#ffec9d",
           "#fff9bd",
           "#ffe58f",
           "#fd9e43",
           "#fff0a7",
           "#fee084",
           "#fedb7b",
           "#800026",
           "#ffea9b",
           "#fed06c",
           "#fd7435",
           "#e9261f"
          ]
         },
         "orientation": "h",
         "type": "bar",
         "x": [
          -0.0123649705,
          -0.01164663,
          -0.0026683987,
          -0.00031484407,
          -0.013009379,
          -0.010983807,
          0.013495384,
          0.0012983419,
          0.028915294,
          0.014179082,
          -0.004345308,
          0.02260696,
          0.07545699,
          0.008648721,
          0.028602924,
          0.03403497,
          0.18623501,
          0.015694927,
          0.042279378,
          0.09663412,
          0.13083832
         ],
         "y": [
          "One",
          "of",
          "the",
          "weakest",
          "entries",
          "in",
          "the",
          "J-horror",
          "remake",
          "sweepstakes,",
          "One",
          "Missed",
          "Call",
          "is",
          "undone",
          "by",
          "bland",
          "performances",
          "and",
          "shopworn",
          "shocks."
         ]
        }
       ],
       "layout": {
        "paper_bgcolor": "rgba(0, 0, 0, 0)",
        "plot_bgcolor": "rgba(0, 0, 0, 0)",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "lakecolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#506784"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "dark"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "gridwidth": 2,
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "bordercolor": "rgb(17,17,17)",
           "borderwidth": 1,
           "tickwidth": 0
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "bgcolor": "rgb(17,17,17)",
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Atributions and Words"
        },
        "xaxis": {
         "griddash": "dash",
         "ticksuffix": ""
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_with_attributions = ' '.join([f'''<span style=\"color:{colors[i]}\"><b>{words[i]}</b></span>''' for i in range(len(words))])\n",
    "\n",
    "print(f'Sample: {x_test_sample[sample]}\\n')\n",
    "\n",
    "for i in range(len(preds)):\n",
    "    print(f'Prediction: (Negative ðŸ˜” {round((1 - preds[i][0]) * 100)}% | Positive ðŸ˜Š {round(preds[i][0] * 100)}%)\\n')\n",
    "    # for softmax model\n",
    "    #print(f'Prediction: (Negative ðŸ˜” {round((preds[i][0]) * 100)}% | Positive ðŸ˜Š {round(preds[i][1] * 100)}%)\\n')\n",
    "    display(HTML(f'Attributions: {text_with_attributions}'))\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(go.Bar(\n",
    "        x=atributions,\n",
    "        y=words,\n",
    "        orientation='h',\n",
    "        marker_color=colors))\n",
    "fig.update_xaxes(ticksuffix = \"\",\n",
    "                griddash='dash')\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_dark',\n",
    "    title_text=f'Atributions and Words',\n",
    "    paper_bgcolor='rgba(0, 0, 0, 0)',\n",
    "    plot_bgcolor='rgba(0, 0, 0, 0)')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above you can see what are the tokens that had _a greater influence on the prediction of our sentiment classifier_.** ðŸŽ­\n",
    "\n",
    "----\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
