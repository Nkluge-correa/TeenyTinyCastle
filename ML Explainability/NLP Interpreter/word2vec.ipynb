{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Word2vec` - Embedding Words in Vector Space Representation\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n",
    "\n",
    "**`Word2vec` is a popular natural language processing technique used to represent words in a high-dimensional vector space. It is a neural network-based approach that is used to create distributed representations of words based on their co-occurrence patterns in a given corpus of text.**\n",
    "\n",
    "**The basic idea behind `word2vec` is that words that are used in similar contexts tend to have similar meanings. So, if two words appear in similar contexts, they should be close to each other in the vector space. `Word2vec` uses neural networks to learn the relationships between words by analyzing the context in which they appear in a given text.**\n",
    "\n",
    "**`Word2vec` models can be used to perform `interpretability` research on text data/language models by extracting insights from the learned word embeddings. For `interpretability`, embedding models can be used for, for example:**\n",
    "\n",
    "1. **Visualize the learned word embeddings.** \n",
    "2. **Use word embeddings to perform tasks such as word similarity and analogy tests.** \n",
    "3. **Use the word embeddings to identify clusters of related words.**\n",
    "\n",
    "**However, we first need to know a little more about embeddings, vectors, and the `word2vec` model, proposed by [Tomas Mikolov](https://arxiv.org/search/cs?searchtype=author&query=Mikolov%2C+T), [Kai Chen](https://arxiv.org/search/cs?searchtype=author&query=Chen%2C+K), [Greg Corrado](https://arxiv.org/search/cs?searchtype=author&query=Corrado%2C+G), and [Jeffrey Dean](https://arxiv.org/search/cs?searchtype=author&query=Dean%2C+J), is a good introduction on the subject.**\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/fit/t/1600/480/1*T8WWibd7u8b7gfgeG0LgAA.gif\" width=400 />\n",
    "\n",
    "**There are two main techniques used in `word2vec`: Continuous Bag of Words (`CBOW`) and `Skip-gram`.**\n",
    "\n",
    "- **`CBOW` is an algorithm used to predict a target word based on its surrounding context words. The algorithm takes a window of context words as input and generates a probability distribution over the vocabulary of words for the target word.**\n",
    "- **`Skip-gram`, on the other hand, is an algorithm used to predict context words given a target word. The algorithm takes a target word as input and generates a probability distribution over the vocabulary of words for the context words.**\n",
    "\n",
    "**In this tutorial, we will explore a `skip-gram` approach. First, we will explore what skip-grams are, and finally, we will train a word2vec model with the [News Category Dataset](https://www.kaggle.com/datasets/rmisra/news-category-dataset?resource=download), availeble on `Kaggle`.**\n",
    "\n",
    "**While in `CBOW` we trie to predict a word based on the words that come before and after it, a `skip-gram` model seeks to predict the words that come before and after a given word (is basically the inverse of `CBOW`). The model is trained using special groups of words called `skip-grams`, which allow certain words to be skipped in the prediction process.**\n",
    "\n",
    "**Let us consider the following sentence:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"There is a missing word in this sentence.\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The context window for this sentence is defined by the window size. The window size determines the span of words on either side of a `target_word` that can be considered a context word. A window of 2 means we only look up to two words to the left and right, and so forth.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 skip_grams of window_size 2 in 'There is a missing word in this sentence.'.\n",
      "Sentence size:  8 \n",
      "\n",
      "('There', 'is')\n",
      "('There', 'a')\n",
      "('is', 'a')\n",
      "('is', 'missing')\n",
      "('is', 'There')\n",
      "('a', 'missing')\n",
      "('a', 'word')\n",
      "('a', 'There')\n",
      "('a', 'is')\n",
      "('missing', 'word')\n"
     ]
    }
   ],
   "source": [
    "skip_grams = []\n",
    "\n",
    "for i, word in enumerate(sentence.split()):\n",
    "    \n",
    "    for j in range(i+1, min(i+3, len(sentence.split()))):\n",
    "        skip_grams.append((word, sentence.split()[j]))\n",
    "\n",
    "    for j in range(max(i-2, 0), i):\n",
    "        skip_grams.append((word, sentence.split()[j]))\n",
    "\n",
    "print(f\"\"\"First 10 skip_grams of window_size 2 in '{sentence}'.\"\"\")\n",
    "print(\"Sentence size: \", len(sentence.split()), \"\\n\")\n",
    "\n",
    "for skip in skip_grams[:10]:\n",
    "    print(skip)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In simple terms, the `skip-gram` model tries to guess the words that are likely to appear around a given word. The goal is to make the model good at predicting these surrounding words. This objective can be written as the average log probability:**\n",
    "\n",
    "$$\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j\\neq0} \\log p(w_{t+j} | w_{t})$$\n",
    "\n",
    "\n",
    "**where:**\n",
    "\n",
    "- **$T$ is the total number of words in the training corpus.**\n",
    "- **$c$ is the size of the context window.**\n",
    "- **$w_t$ is the target word at position $t$ in the corpus.**\n",
    "- **$w_{t+j}$ is the context word at position $t+j$ in the same context window.**\n",
    "- **$p(w_{t+j} | w_{t})$ is the conditional probability of context word given the target word, which is estimated by the `skip-gram` model.**\n",
    "\n",
    "\n",
    "**The `softmax` formulation for the `skip-gram` model can be written as:**\n",
    "\n",
    "$$p(w_O | w_I) = \\frac{\\exp(v'{w_O} \\cdot v{w_I})}{\\sum_{w=1}^{W} \\exp(v'{w} \\cdot v{w_I})}$$\n",
    "\n",
    "**where:**\n",
    "\n",
    "- **$w_I$ is the input (target) word.**\n",
    "- **$w_O$ is the output (context) word.**\n",
    "- **$v_{w_I}$ and $v'_{w_O}$ are the input and output vector representations of words $w_I$ and $w_O$ respectively.**\n",
    "- **$W$ is the size of the vocabulary of words.**\n",
    "- **The dot (·) represents the dot product of two vectors.**\n",
    "\n",
    "**The numerator computes the similarity between the input and output word vectors, using the dot product. The denominator is a normalization term, which sums up the similarities of the input word with all the words in the vocabulary. The resulting probability distribution is over all the words in the vocabulary, and is used to estimate the conditional probability of observing an output word given an input word.**\n",
    "\n",
    "**To make the computation more efficient, we use a technique called noise contrastive estimation (`NCE`) instead of the full `softmax`. This is because the full `softmax` involves a lot of words and can be slow to calculate. `NCE` simplifies the process by using negative sampling.**\n",
    "\n",
    "**The idea behind negative sampling is to randomly select a few words that are not related to the target word and use them to train the model. The model learns to distinguish between the context word and these randomly selected words, which helps it to better understand the target word.**\n",
    "\n",
    "**In this simplified approach, we select a few random words (called negative samples) and try to train the model to distinguish them from the context word. A negative sample is a pair of words where the context word is not near the target word. For example, if the target word is \"_missing_\" and the context window is two, then a negative sample could be \"_algebra_\" because \"_algebra_\" is not in the `window_size` neighborhood of \"_missing_\" in our sentence example.**\n",
    "\n",
    "**In practice, our model will not be working with words, but with tokens. Thus, let us create a tokenization dictionary for our custom sentence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': 0, 'There': 1, 'is': 2, 'a': 3, 'missing': 4, 'word': 5, 'in': 6, 'this': 7, 'sentence.': 8}\n",
      "{0: '', 1: 'There', 2: 'is', 3: 'a', 4: 'missing', 5: 'word', 6: 'in', 7: 'this', 8: 'sentence.'}\n",
      "Our tokenized sequence:  [1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Decoded sequence:  ['There', 'is', 'a', 'missing', 'word', 'in', 'this', 'sentence.']\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1\n",
    "\n",
    "vocab[''] = 0  # a padding token\n",
    "\n",
    "for token in sentence.split():\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "\n",
    "print(vocab)\n",
    "print(inverse_vocab)\n",
    "\n",
    "print(\"Our tokenized sequence: \", [vocab[word] for word in sentence.split()])\n",
    "print(\"Decoded sequence: \", [inverse_vocab[index] for index in \\\n",
    "                             [vocab[word] for word in sentence.split()]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We could use the for loop implemented in our second code cell to create `skip-grams`. however, there is no need to reinvent the wheel. The `tf.keras.preprocessing.sequence` module provides the `tf.keras.preprocessing.sequence.skipgrams` that can do this heavy lifting for us. However, it does the same job as our two nested for loops.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(in, this)\n",
      "(this, word)\n",
      "(word, missing)\n",
      "(in, word)\n",
      "(in, sentence.)\n",
      "(a, missing)\n",
      "(missing, in)\n",
      "(is, a)\n",
      "(sentence., in)\n",
      "(this, sentence.)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tokenized_sentence = [vocab[word] for word in sentence.split()]\n",
    "\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      tokenized_sentence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=2,\n",
    "      negative_samples=0)\n",
    "\n",
    "for target, context in positive_skip_grams[:10]:\n",
    "  print(f\"({inverse_vocab[target]}, {inverse_vocab[context]})\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The `skip-grams` function looks for pairs of words that appear together within a certain window span. These pairs are called `positive skip-grams`.**\n",
    "\n",
    "**However, we also need negative samples. As before mentioned, these are pairs of words that don't appear together. To create negative samples, we randomly choose words from the vocabulary that are not in the same window as the `positive skip-grams`.**\n",
    "\n",
    "**To do this, we use a function called `tf.random.log_uniform_candidate_sampler`. This function randomly selects words from the vocabulary to create negative samples. We tell the function how many negative samples we want (`num_ns`), and also give it the positive skip-gram's target word and context word. The context word is marked as \"_true_\" so that it won't be chosen as a negative sample.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  There is a missing word in this sentence.\n",
      "Positive skip-grams: (in,this)\n",
      "Negative samples:  ['a', 'There', 'word', 'is']\n"
     ]
    }
   ],
   "source": [
    "# one psotitive skip-gram\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "# number of negative\n",
    "num_ns = 4\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class, \n",
    "    num_true=1, \n",
    "    num_sampled=num_ns, \n",
    "    unique=True, \n",
    "    range_max=vocab_size, \n",
    "    seed=42,\n",
    "    name=\"negative_sampling\" \n",
    ")\n",
    "\n",
    "print(\"Original Sentence: \", sentence)\n",
    "\n",
    "print(f\"Positive skip-grams: ({inverse_vocab[target_word]},{inverse_vocab[context_word]})\")\n",
    "\n",
    "print(\"Negative samples: \", [inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have both positive and negative samples, we can put them together to create a set of training examples. For each positive skip-gram pair (`target_word`, `context_word`), we also have `num_ns` negative samples (words that don't appear in the same window).**\n",
    "\n",
    "**We group these positive and negative samples together into a single set. Each positive sample is labeled as 1 and each negative sample is labeled as 0.**\n",
    "\n",
    "**So, for every target word, we end up with a set of positive skip-grams and negative samples that can be used to train the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One training sample: {\n",
      "target token    : 6\n",
      "target word     : in\n",
      "context tokens : [7 3 1 5 2]\n",
      "context words   : ['this', 'a', 'There', 'word', 'is']\n",
      "labels           : [1 0 0 0 0]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = tf.concat([tf.squeeze(context_class, 1), negative_sampling_candidates], 0)\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "target = target_word\n",
    "\n",
    "print(f\"\"\"\n",
    "One training sample: {{\n",
    "target token    : {target}\n",
    "target word     : {inverse_vocab[target_word]}\n",
    "context tokens : {context}\n",
    "context words   : {[inverse_vocab[c.numpy()] for c in context]}\n",
    "labels           : {label}\n",
    "}}\n",
    "\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When we have a large dataset, we also have a lot of words to work with. Some words, like \"_the_\", \"_is_\", and \"_on_\", appear very frequently and don't provide much useful information to the model.\n",
    "To deal with this, we can remove some of these very frequent words from the training data.**\n",
    "\n",
    "**The `tf.keras.preprocessing.sequence.skipgrams` function can be used to subsample these frequent words by giving it a list of probabilities that tell it how likely each word is to be sampled.\n",
    "To create this list of probabilities, we can use the `tf.keras.preprocessing.sequence.make_sampling_table` function. This function generates a list of probabilities based on the frequency of each word in the dataset.**\n",
    "\n",
    "**Now that we have described all the necessary steps to preprocess text data for training word embeddings using the `skip-gram` model, we can compile them into a function. Once this function is defined, we can use it in the later sections to preprocess our text data and prepare it for training our `word2vec` model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  \"\"\"\n",
    "    Generate training data for a skip-gram model using negative sampling.\n",
    "\n",
    "    Args:\n",
    "        sequences: A list of sequences, where each sequence is a list of integers \n",
    "        representing words. \n",
    "        window_size: An integer, the size of the window for generating skip-grams.\n",
    "        num_ns: An integer, the number of negative samples to use for each positive sample.\n",
    "        vocab_size: An integer, the size of the vocabulary.\n",
    "        seed: An integer, the random seed to use for sampling.\n",
    "\n",
    "    Returns:\n",
    "        Three lists: targets, contexts, and labels. \n",
    "        Targets is a list of integers representing target words, contexts is a list of lists \n",
    "        of integers representing context words and negative samples, and labels is a list of \n",
    "        lists of integers representing the labels for each context. Specifically, each label\n",
    "        list has a 1 in the first position (representing the positive sample) and 0s in the \n",
    "        remaining positions (representing the negative samples).\n",
    "  \"\"\"\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "\n",
    "      context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=seed,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we need some text. For this, we will use the [News Category Dataset](https://www.kaggle.com/datasets/rmisra/news-category-dataset?resource=download). The original dataset comes as an unformatted JSON file. However, you can download a properly formatted version (in a `pickle` format) on [this link](https://drive.google.com/uc?export=download&id=1EO_DBb-trFK-HWBWRMRhAKOnyMtf5GLL). Nonetheless, all credits go to the authors of the original dataset:**\n",
    "\n",
    "```markdown\n",
    "@article{misra2022news,\n",
    "  title={News Category Dataset},\n",
    "  author={Misra, Rishabh},\n",
    "  journal={arXiv preprint arXiv:2209.11429},\n",
    "  year={2022}\n",
    "}\n",
    "```\n",
    "\n",
    "**After downloading the dataset, run the cell below to create a dataset folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Folder Created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "with open('News_Category_Dataset_v3.pickle', 'rb') as fp:\n",
    "    news = pickle.load(fp)\n",
    "    fp.close()\n",
    "\n",
    "texts = []\n",
    "category = []\n",
    "\n",
    "# We are uniting the title and abstract to create a single string.\n",
    "for i in range(len(news['news'])):\n",
    "    texts.append(\n",
    "        f\"\"\"{news['news'][i]['headline']} {news['news'][i]['short_description']}\"\"\")\n",
    "    category.append(news['news'][i]['category'])\n",
    "\n",
    "# Saving strings with their associated category \n",
    "# in case we want to perform text classification.\n",
    "df = pd.DataFrame({\n",
    "    \"category\": category,\n",
    "    \"texts\": texts\n",
    "})\n",
    "\n",
    "df.to_csv('News_Category_Dataset_v3.csv', index=False)\n",
    "\n",
    "\n",
    "# Check whether the specified path exists or not\n",
    "if not os.path.exists(\"dataset/\"):\n",
    "\n",
    "    # Create a new directory because it does not exist\n",
    "    os.makedirs(\"dataset/\")\n",
    "\n",
    "    # saving all text samples as txt files, in different \n",
    "    # news category folders\n",
    "    for category in tqdm.tqdm(df.category.unique()):\n",
    "        os.mkdir(f\"dataset/{category}\")\n",
    "        dff = df[df['category'] == category]\n",
    "\n",
    "        for i, sample in enumerate(list(dff.texts)):\n",
    "            with open(f'dataset/{category}/{i}.txt', 'w', encoding='utf-8') as fp:\n",
    "                fp.write(sample)\n",
    "                fp.close()\n",
    "\n",
    "    print('Dataset Folder Created!')\n",
    "\n",
    "else:\n",
    "    print('Dataset already exists!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This dataset contains a lot of text (42 directories with 209.527 files). However, we will use only a portion of it (a little more than 35K samples).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1 directories.\n",
      "Found 35602 files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directories = [\"dataset/POLITICS\"]\n",
    "\n",
    "filenames = []\n",
    "\n",
    "for directory in directories:\n",
    "    for folder in os.listdir(directory):\n",
    "        filenames.append(os.path.join(directory, folder))\n",
    "\n",
    "print(f\"Using {len(directories)} directories.\")\n",
    "print(f\"Found {len(filenames)} files.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All of the found files are `txt` files with some text about some news topic.**\n",
    "\n",
    "**Now, let us shuffle the order of our samples, and create a dataset using the `tf.data.TextLineDataset`, which loads text from text files and creates a dataset where each line of the files becomes an element of the dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "random.shuffle(filenames)\n",
    "\n",
    "text_ds = tf.data.TextLineDataset(filenames)\n",
    "text_ds = text_ds.batch(1024)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we us the `tf.keras.layers.TextVectorization`, passing a `custom_standardization` function to lower strings and parse punctuations, to create a vectorization layer. Then we adapt the `TextVectorization` layer to our dataset and get our vocabulary out of it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Lower all strings and parse punctuation and symbols\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "# Maximum vocabulary size and will cut sequences with more than 100 tokens\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "\n",
    "# Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Fit the TextVectorization layer to the dataset\n",
    "vectorize_layer.adapt(text_ds)\n",
    "\n",
    "# Get words back from token indices\n",
    "word2vec_vocabulary = vectorize_layer.get_vocabulary()  \n",
    "\n",
    "# Save the vocabulary as a text file\n",
    "with open(f'word2vec_vocabulary.txt', 'w', encoding='utf-8') as fp:\n",
    "    for word in word2vec_vocabulary:\n",
    "        fp.write(\"%s\\n\" % word)\n",
    "    fp.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can use the `vectorize_layer` to create a set of tokenized sequences that represent each piece of text in our `text_ds`. Then, we can apply some operations like `Dataset.batch`, `Dataset.prefetch`, `Dataset.map`, and `Dataset.unbatch` to the collection of text data to make it easier to process. These operations help to group the data into smaller parts, optimize the processing order, and transform it into a format that can be fed into our `word2vec` model.**\n",
    "\n",
    "**In order to get a dataset ready for training a `word2vec` model, you need to convert the dataset into a list of sequences, where each sequence represents a sentence and is made up of numbers that correspond to the words in the sentence. This is necessary because when you train a `word2vec` model, you need to go through each sentence in the dataset and use it to create both positive and negative examples for the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  35626  sequences.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "text_vector_ds = text_ds.prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "\n",
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(\"We have \", len(sequences), \" sequences.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We now have a list called `sequences` which contains sentences that have been turned into sets of numbers/tokens. We can now our `generate_training_data` function that we defined earlier to create training examples for our `word2vec` model.** \n",
    "\n",
    "**Basically, this function looks at each word in each sentence and uses them to create examples that will teach the model how to predict words that are related to each other. The function creates three lists - target words, context words, and labels - and each list has the same number of items, which represents the total number of examples that the model will be trained on.**\n",
    "\n",
    "**When training a `word2vec` model, there are two important things to consider: how big the `window_size` of words is that you're looking at, and how many negative samples (`num_ns`) you're including.** \n",
    "\n",
    "**Depending on what you're trying to accomplish, different window sizes can be more useful. Generally, smaller window sizes (2-15) will give you embeddings where words with similar meanings are treated as interchangeable, even if they're opposite in meaning. Larger window sizes (15-50 or more) will give you embeddings where words that are related, but not necessarily interchangeable, will have higher similarity scores.**\n",
    "\n",
    "**For a more complete explanation of the effect `window size` has, [watch this video](https://www.youtube.com/watch?v=tAxrlAVw-Tk&t=648s).**\n",
    "\n",
    "**In terms of number of `num_ns`, the [original paper](https://arxiv.org/abs/1301.3781) prescribes 5-20 as being a good number of negative samples.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35626/35626 [02:22<00:00, 249.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: (636101,)\n",
      "Contexts: (636101, 5)\n",
      "Labels: (636101, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "window_size = 2\n",
    "num_ns = 4\n",
    "\n",
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=window_size,\n",
    "    num_ns=num_ns,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=42)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Targets: {targets.shape}\")\n",
    "print(f\"Contexts: {contexts.shape}\")\n",
    "print(f\"Labels: {labels.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Depending on the chosen `window_size` and `num_ns` the generation of our dataset can take a while. However, you can download `word2vec_vocabulary` and the `targets`, `contexts`, and `labels` files directly with [this link](https://drive.google.com/uc?export=download&id=1KtaBxlGb8y4Do9jjbYSuVnrwLZESJk_Q). We created two 10.000 word word2vec vocabularies using all of the below sections from our dataset:**\n",
    "\n",
    "```python\n",
    "[\"dataset/POLITICS\",\n",
    " \"dataset/WORLD NEWS\",\n",
    " \"dataset/ENTERTAINMENT\",\n",
    " \"dataset/ENVIRONMENT\",\n",
    " \"dataset/EDUCATION\",\n",
    " \"dataset/SCIENCE\",\n",
    " \"dataset/WELLNESS\"]\n",
    "```\n",
    "\n",
    "**We also created two different sets of [`targets`, `contexts`, `labels`]. One with a `window_size` of 2, and the other one with a `window_size` of 15. You can compare all of them to see how the increase in text data and `window_size` affect the `word2vec`model.**\n",
    "\n",
    "**An alternative dataset, paired whit vocabularies and word embeddings, is also available in Portuguese. To train it, we used the [`Fake.Br Corpus`](https://github.com/roneysco/Fake.br-Corpus).**\n",
    "\n",
    "**Now, let us load our one of our training datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: (1569949,)\n",
      "Contexts: (1569949, 5)\n",
      "Labels: (1569949, 5)\n",
      "Vocabulary Size: 10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "window_size = 2\n",
    "num_ns = 4\n",
    "\n",
    "with open(f'data/w2v_dataset_w{window_size}_nn4.npy', 'rb') as fp:\n",
    "    targets = np.load(fp)\n",
    "    contexts = np.load(fp)\n",
    "    labels = np.load(fp)\n",
    "    fp.close()\n",
    "\n",
    "with open('data/word2vec_vocabulary.txt', encoding='utf-8') as fp:\n",
    "    word2vec_vocabulary = [line.strip() for line in fp]\n",
    "    fp.close()\n",
    "\n",
    "print(f\"Targets: {targets.shape}\")\n",
    "print(f\"Contexts: {contexts.shape}\")\n",
    "print(f\"Labels: {labels.shape}\")\n",
    "print(f\"Vocabulary Size: {len(word2vec_vocabulary)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When you have a lot of training examples, it can be difficult for a computer to process them all at once. To make it easier, we can use the `tf.data.Dataset` API again and group the examples into smaller batches, which can be processed more efficiently.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(10000).batch(1024, drop_remainder=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As said before, the `word2vec` model is a tool that can help us tell which words go together by looking at how often they appear near each other in sentences. It does this by comparing the meanings of different words and figuring out which ones are similar.**\n",
    "\n",
    "**To train the model, we can give it pairs of words and ask it to predict whether they belong together or not. We can check if the model is correct by comparing its predictions to the actual pairs of words that we already know go together. The model gets better over time as it learns from more and more examples of word pairs.**\n",
    "\n",
    "**To create your `word2vec` model, you can use the `Keras Subclassing API` with different layers.**\n",
    "\n",
    "- **The first layer will be the `target_embedding` layer, responsible for finding the meaning of a word when it's used as a target. The size of this layer depends on the size of our vocabulary and the dimension of your `embeddings`.**\n",
    "- **The second layer will be the `context_embedding` layer, responsible for finding the meaning of a word when it's used in the context of another word. It has the same number of parameters as the target_embedding layer.**\n",
    "- **The `dots` layer is used to combine the `target` and `context` embeddings and calculate a dot product.**\n",
    "- **The `flatten` layer takes the output of the dots layer and makes it flat.**\n",
    "\n",
    "**You can then define a `call()`function that takes a pair of words (`target` and `context`) and passes them through the target and context `embedding layers`, performs a dot product with their output, and returns the flattened result.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Word2Vec(tf.keras.Model):\n",
    "  \n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = tf.keras.layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_target_embedding\")\n",
    "    self.context_embedding = tf.keras.layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1,\n",
    "                                       name=\"w2v_context_embedding\")\n",
    "\n",
    "  def call(self, pair):\n",
    "\n",
    "    target, context = pair\n",
    "\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "\n",
    "    word_emb = self.target_embedding(target)\n",
    "\n",
    "    context_emb = self.context_embedding(context)\n",
    "\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "\n",
    "    return dots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since our labels are already one-hot-encoded, we will use `CategoricalCrossEntropy` as an alternative to the negative sampling loss, and `Adam` as the optimizer. Now, we instantiate our word2vec class with an embedding dimension of 512 and a vocabulary size of 10.000 words. If you don't want to train this model, you can get the embeddings directly when you download [the available dataset for this notebook](https://drive.google.com/uc?export=download&id=1KtaBxlGb8y4Do9jjbYSuVnrwLZESJk_Q).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.10.1\n",
      "Eager mode:  True\n",
      "GPU is available\n",
      "Epoch 1/20\n",
      "1533/1533 [==============================] - 52s 34ms/step - loss: 1.4320 - accuracy: 0.3862\n",
      "Epoch 2/20\n",
      "1533/1533 [==============================] - 52s 34ms/step - loss: 1.1575 - accuracy: 0.5459\n",
      "Epoch 3/20\n",
      "1533/1533 [==============================] - 52s 34ms/step - loss: 0.9508 - accuracy: 0.6507\n",
      "Epoch 4/20\n",
      "1533/1533 [==============================] - 52s 34ms/step - loss: 0.7789 - accuracy: 0.7304\n",
      "Epoch 5/20\n",
      "1533/1533 [==============================] - 52s 34ms/step - loss: 0.6447 - accuracy: 0.7842\n",
      "Epoch 6/20\n",
      "1533/1533 [==============================] - 52s 34ms/step - loss: 0.5469 - accuracy: 0.8174\n",
      "Epoch 7/20\n",
      "1533/1533 [==============================] - 52s 34ms/step - loss: 0.4786 - accuracy: 0.8358\n",
      "Epoch 8/20\n",
      "1533/1533 [==============================] - 52s 34ms/step - loss: 0.4320 - accuracy: 0.8455\n",
      "Epoch 9/20\n",
      "1533/1533 [==============================] - 52s 34ms/step - loss: 0.4005 - accuracy: 0.8506\n",
      "Epoch 10/20\n",
      "1533/1533 [==============================] - 52s 34ms/step - loss: 0.3790 - accuracy: 0.8535\n",
      "Epoch 11/20\n",
      "1533/1533 [==============================] - 52s 34ms/step - loss: 0.3639 - accuracy: 0.8552\n",
      "Epoch 12/20\n",
      "1533/1533 [==============================] - 52s 34ms/step - loss: 0.3532 - accuracy: 0.8564\n",
      "Epoch 13/20\n",
      "1533/1533 [==============================] - 52s 34ms/step - loss: 0.3453 - accuracy: 0.8572\n",
      "Epoch 14/20\n",
      "1533/1533 [==============================] - 53s 34ms/step - loss: 0.3394 - accuracy: 0.8577\n",
      "Epoch 15/20\n",
      "1533/1533 [==============================] - 52s 34ms/step - loss: 0.3348 - accuracy: 0.8582\n",
      "Epoch 16/20\n",
      "1533/1533 [==============================] - 51s 33ms/step - loss: 0.3312 - accuracy: 0.8586\n",
      "Epoch 17/20\n",
      "1533/1533 [==============================] - 51s 33ms/step - loss: 0.3283 - accuracy: 0.8588\n",
      "Epoch 18/20\n",
      "1533/1533 [==============================] - 51s 33ms/step - loss: 0.3259 - accuracy: 0.8590\n",
      "Epoch 19/20\n",
      "1533/1533 [==============================] - 51s 33ms/step - loss: 0.3239 - accuracy: 0.8593\n",
      "Epoch 20/20\n",
      "1533/1533 [==============================] - 51s 33ms/step - loss: 0.3222 - accuracy: 0.8595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x213914bd070>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dimension = 512\n",
    "\n",
    "word2vec = Word2Vec(vocab_size, embedding_dimension)\n",
    "\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "\n",
    "word2vec.fit(dataset, verbose=1, epochs=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we can recover the embeddings from both the `target` and `context` embedding layers. These embeddings will now hold some information about the relationship of words in our text corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the embedding layer\n",
    "embeddings_target = word2vec.get_layer('w2v_target_embedding').get_weights()[0]\n",
    "embeddings_context = word2vec.get_layer('w2v_context_embedding').get_weights()[0]\n",
    "\n",
    "# save the embeddings as a numpy array\n",
    "with open('data/w2v_embeddings_w2.npy', 'wb') as fp:\n",
    "    np.save(fp, embeddings_target)\n",
    "    np.save(fp, embeddings_context)\n",
    "    fp. close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Again, you can directly load them if you downloaded our dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Embeddings shape: (10000, 512)\n",
      "Context Embeddings shape: (10000, 512)\n",
      "Vocabulary Size: 10000\n"
     ]
    }
   ],
   "source": [
    "with open('data/w2v_embeddings_w15.npy', 'rb') as fp:\n",
    "    embeddings_target = np.load(fp)\n",
    "    embeddings_context = np.load(fp)\n",
    "    fp.close()\n",
    "\n",
    "with open('data/word2vec_vocabulary.txt', encoding='utf-8') as fp:\n",
    "    word2vec_vocabulary = [line.strip() for line in fp]\n",
    "    fp.close()\n",
    "\n",
    "print(f\"Target Embeddings shape: {embeddings_target.shape}\")\n",
    "print(f\"Context Embeddings shape: {embeddings_context.shape}\")\n",
    "print(f\"Vocabulary Size: {len(word2vec_vocabulary)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we can pair our embeddings with our vocabulary.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of \"word: embedding\"\n",
    "word2vec_target_embeddings = {}\n",
    "word2vec_context_embeddings = {}\n",
    " \n",
    "# iterating through the elements of the vocabulary\n",
    "for i, word in enumerate(word2vec_vocabulary):\n",
    "    # here we skip the embedding/token 0 (\"\"), because is just the PAD token.\n",
    "    if i == 0:\n",
    "        continue\n",
    "    word2vec_target_embeddings[word] = embeddings_target[i]\n",
    "    word2vec_context_embeddings[word] = embeddings_context[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, we can perform some basic operations (`cosine similarity`) to try to understand and interpret what our model has learned, both for the `target` and `cosine` embeddings. While `target embeddings` hold information on \"_relatedness among words_\", `context embeddings` hold information on \"_what words usually accompany the target word_\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity for Target Embeddings\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Closest Match   |   Similarity Score |\n",
       "|:----------------|-------------------:|\n",
       "| donald          |           0.194642 |\n",
       "| counsel         |           0.194392 |\n",
       "| mexicos         |           0.192862 |\n",
       "| insider         |           0.184099 |\n",
       "| arpaio          |           0.183056 |\n",
       "| pence           |           0.181958 |\n",
       "| white           |           0.178896 |\n",
       "| trevor          |           0.17812  |\n",
       "| trumps          |           0.177152 |\n",
       "| scarborough     |           0.176633 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity for Context Embeddings\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| Closest Match   |   Similarity Score |\n",
       "|:----------------|-------------------:|\n",
       "| donald          |           0.67039  |\n",
       "| on              |           0.4248   |\n",
       "| with            |           0.400963 |\n",
       "| his             |           0.398482 |\n",
       "| tweeters        |           0.388154 |\n",
       "| trumps          |           0.386425 |\n",
       "| ¯ツ¯            |           0.385741 |\n",
       "| president       |           0.381984 |\n",
       "| he              |           0.376554 |\n",
       "| taunts          |           0.367066 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "from IPython.display import Markdown \n",
    "\n",
    "def compute_cosine_table(string, dictionary, \n",
    "                         vocabulary, top_n):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between a given word and all other words in a dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    string : str\n",
    "        The word to compare against.\n",
    "    dictionary : dict\n",
    "        A dictionary with words as keys and their corresponding embeddings as values.\n",
    "    vocabulary : list\n",
    "        A list of words in the dictionary.\n",
    "    top_n : int\n",
    "        The number of closest matches to return.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    A pandas DataFrame with the closest matches to the input word and their \n",
    "    corresponding similarity scores. The DataFrame is sorted in descending \n",
    "    order of similarity score and limited to the top_n matches.\n",
    "    The index of the DataFrame is set to the closest matches.\n",
    "    \"\"\"\n",
    "\n",
    "    l = vocabulary.copy()\n",
    "    l.remove(string)\n",
    "\n",
    "    cos = []\n",
    "    for word in l[1::]:\n",
    "\n",
    "        cosine = np.dot(dictionary[string],\n",
    "                dictionary[word])/(norm(dictionary[string])*norm(dictionary[word]))\n",
    "        cos.append(cosine)\n",
    "\n",
    "    return pd.DataFrame({\"Closest Match\": l[1::],f\"Similarity Score\": cos})\\\n",
    "        .sort_values(f\"Similarity Score\", ascending=False)\\\n",
    "        .set_index('Closest Match').head(top_n)\n",
    "\n",
    "word = \"trump\"\n",
    "\n",
    "df = compute_cosine_table(word, \n",
    "        word2vec_target_embeddings, \n",
    "        word2vec_vocabulary, 10)\n",
    "\n",
    "print(\"Cosine Similarity for Target Embeddings\")\n",
    "display(Markdown(df.to_markdown()))\n",
    "\n",
    "df = compute_cosine_table(word, \n",
    "        word2vec_context_embeddings, \n",
    "        word2vec_vocabulary, 10)\n",
    "\n",
    "print(\"Cosine Similarity for Context Embeddings\")\n",
    "display(Markdown(df.to_markdown()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can also perform basic arithmetic operations with these vector embeddings, which is another way to try to understand the knowledge they hold.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Closest Match   |   Similarity Score |\n",
       "|:----------------|-------------------:|\n",
       "| jazz            |           0.214929 |\n",
       "| christina       |           0.188508 |\n",
       "| duff            |           0.185525 |\n",
       "| peek            |           0.184137 |\n",
       "| centers         |           0.181551 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def find_closest_match(array, dictionary, vocabulary, \n",
    "                           word1, word2, top_n):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between a given array and all other word \n",
    "    embeddings in a dictionary except for two specified words.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    array : numpy.ndarray\n",
    "        An array representing the embedding of a word or phrase.\n",
    "    dictionary : dict\n",
    "        A dictionary with words as keys and their corresponding embeddings as values.\n",
    "    vocabulary : list\n",
    "        A list of words in the dictionary.\n",
    "    word1 : str\n",
    "        The first word to exclude from the matches.\n",
    "    word2 : str\n",
    "        The second word to exclude from the matches.\n",
    "    top_n : int\n",
    "        The number of closest matches to return.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        A pandas DataFrame with the closest matches to the input array and\n",
    "        their corresponding similarity scores. The DataFrame is sorted in \n",
    "        descending order of similarity score and limited to the top_n matches.\n",
    "        The index of the DataFrame is set to the closest matches.\n",
    "    \"\"\"\n",
    "\n",
    "    l = vocabulary.copy()\n",
    "    l.remove(word1)\n",
    "    l.remove(word2)\n",
    "\n",
    "    cos = []\n",
    "\n",
    "    for word in l[1::]:\n",
    "        cosine = np.dot(array,\n",
    "                dictionary[word])/(norm(array)*norm(dictionary[word]))\n",
    "        cos.append(cosine)\n",
    "\n",
    "    return pd.DataFrame({\"Closest Match\": l[1::],f\"Similarity Score\": cos})\\\n",
    "        .sort_values(f\"Similarity Score\", ascending=False)\\\n",
    "        .set_index('Closest Match').head(top_n)\n",
    "\n",
    "word1 = 'man'\n",
    "word2 = 'music'\n",
    "\n",
    "difference_vec = word2vec_target_embeddings[word1] + word2vec_target_embeddings[word2]\n",
    "\n",
    "df = find_closest_match(difference_vec, word2vec_target_embeddings, \n",
    "                           word2vec_vocabulary, word1, word2, 5)\n",
    "\n",
    "display(Markdown(df.to_markdown()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apperently, \"_man_\" + \"_music_\" get us close to \"_jazz_\" 🎶🎷.**\n",
    "\n",
    "**If you wanna learn more about the exploration of `word embeddings`, like projecting them into a 3D space, go to [`teeny-tiny_castle/ML Explainability/NLP Interpreter/`](https://github.com/Nkluge-correa/teeny-tiny_castle/tree/master/ML%20Explainability/NLP%20Interpreter) and check the [`investigating_word_embeddings`](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/f68411d51bb6c9bbc877a084d4233218d09acbf7/ML%20Explainability/NLP%20Interpreter/investigating_word_embeddings.ipynb) notebook.**\n",
    "\n",
    "---\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
