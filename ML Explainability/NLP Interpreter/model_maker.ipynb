{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["# Sentiment Analysis with a `Bidirectional LSTM`\n", "\n", "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n", "\n", "_Sentiment analysis_ is the use of [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing), [text analysis](https://en.wikipedia.org/wiki/Text_analytics), [computational linguistics](https://en.wikipedia.org/wiki/Computational_linguistics), and [biometrics](https://en.wikipedia.org/wiki/Biometrics) to systematically identify, extract, quantify, and study affective states and subjective information. \n", "\n", "Sentiment analysis is widely applied to [voice of the customer](https://en.wikipedia.org/wiki/Voice_of_the_customer) materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.\n", "\n", "![sentiment-analisys](https://miro.medium.com/proxy/1*_JW1JaMpK_fVGld8pd1_JQ.gif)\n", "\n", "In this notebook, we are creating a language model for sentiment analysis using `Keras API` and `TensorFlow`.\n", "\n", "We will be using a dataset that was put together by combining several datasets for sentiment classification available on [Kaggle](https://www.kaggle.com/):\n", "\n", "- The `IMDB 50K` [dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?select=IMDB+Dataset.csv): _0K movie reviews for natural language processing or Text analytics._\n", "- The `Twitter US Airline Sentiment` [dataset](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment): _originated from the  [Crowdflower's Data for Everyone library](http://www.crowdflower.com/data-for-everyone)._\n", "- Our `google_play_apps_review` _dataset: built using the `google_play_scraper` in [this notebook](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/64d0693c28786ce42149411bec8b3b42520fc4df/ML%20Explainability/NLP%20Interpreter%20(en)/scrape(en).ipynb)._\n", "- The `EcoPreprocessed` [dataset](https://www.kaggle.com/datasets/pradeeshprabhakar/preprocessed-dataset-sentiment-analysis): _scrapped amazon product reviews_.\n", "\n", "The final result is the `sentiment_analysis_dataset.csv` available in for download in [this](https://drive.google.com/uc?export=download&id=1_ijhnVLHddM7Cm3R3vfqBB-svw6iNfpv) link. Also available in [portuguese](https://drive.google.com/uc?export=download&id=1YCIzGqcdlHSy-GvghRp0U5USUhuOVEE3)!\n", "\n", "Both datasets already come preprocessed, and the `cleaning` function we used is this:\n", "\n", "```python\n", "\n", "import re\n", "from unidecode import unidecode\n", "\n", "def custom_standardization(input_data):\n", "    clean_text = input_data.lower().replace(\"<br />\", \" \")\n", "    clean_text = re.sub(r\"[-()\\\"#/@;:<>{}=~|.?,]\", ' ', clean_text)\n", "    clean_text = re.sub(' +', ' ', clean_text)\n", "    return unidecode(clean_text)\n", "\n", "```"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"data": {"text/html": ["<div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>review</th>\n", "      <th>sentiment</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>one of the other reviewers has mentioned that ...</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>a wonderful little production the filming tech...</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>i thought this was a wonderful way to spend ti...</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>basically there's a family where a little boy ...</td>\n", "      <td>0</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>petter mattei's love in the time of money is a...</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>...</th>\n", "      <td>...</td>\n", "      <td>...</td>\n", "    </tr>\n", "    <tr>\n", "      <th>85084</th>\n", "      <td>yaaa cool use last weeks give good response</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>85085</th>\n", "      <td>years daughter love alexa enjoy alexa</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>85086</th>\n", "      <td>yes popular but doesnt use except listen songs...</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>85087</th>\n", "      <td>yo alexa love</td>\n", "      <td>1</td>\n", "    </tr>\n", "    <tr>\n", "      <th>85088</th>\n", "      <td>yo yo yo love go if want one smart speaker val...</td>\n", "      <td>1</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "<p>85089 rows \u00d7 2 columns</p>\n", "</div>"], "text/plain": ["                                                  review  sentiment\n", "0      one of the other reviewers has mentioned that ...          1\n", "1      a wonderful little production the filming tech...          1\n", "2      i thought this was a wonderful way to spend ti...          1\n", "3      basically there's a family where a little boy ...          0\n", "4      petter mattei's love in the time of money is a...          1\n", "...                                                  ...        ...\n", "85084        yaaa cool use last weeks give good response          1\n", "85085              years daughter love alexa enjoy alexa          1\n", "85086  yes popular but doesnt use except listen songs...          1\n", "85087                                      yo alexa love          1\n", "85088  yo yo yo love go if want one smart speaker val...          1\n", "\n", "[85089 rows x 2 columns]"]}, "metadata": {}, "output_type": "display_data"}], "source": ["# PT-BR https://drive.google.com/uc?export=download&id=1YCIzGqcdlHSy-GvghRp0U5USUhuOVEE3\n", "# EN https://drive.google.com/uc?export=download&id=1_ijhnVLHddM7Cm3R3vfqBB-svw6iNfpv\n", "\n", "import pandas as pd\n", "import urllib.request\n", "\n", "urllib.request.urlretrieve(\n", "    'https://drive.google.com/uc?export=download&id=1_ijhnVLHddM7Cm3R3vfqBB-svw6iNfpv', \n", "    'sentiment_analysis_dataset_en.csv'\n", ")\n", "\n", "df = pd.read_csv('sentiment_analysis_dataset_en.csv')\n", "display(df)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["The following cells will train a `Bidirectional long-short term memory (bi-lstm)` for binary sentiment classification (Negative versus Positive).\n", "\n", "\n", "The `Embedding`, `Bidirectional`, and `LSTM` layers are commonly used in `RNNs` for processing sequential data such as text:\n", "\n", "- The `Embedding` layer is used to convert input text data into numerical vectors. It maps each word in the text to a fixed-size vector of real numbers, which can be learned during training or pre-trained on a large corpus of text. The purpose of the embedding layer is to capture the semantic meaning of words and represent them in a dense vector space, which can be used as input to the subsequent layers of the network.\n", "- The `Bidirectional` layer is used to improve the performance of `RNNs` by processing the input sequence in both directions, forward and backward. It consists of two separate `RNNs` that process the input sequence in opposite directions and concatenate the outputs of each time step. This allows the network to capture information from both past and future contexts, which can be particularly useful for tasks such as text classification and named entity recognition.\n", "- The `LSTM` layer is a type of `RNN` that is designed to overcome the limitations of traditional `RNNs`, such as the vanishing gradient problem. It has a more complex architecture that allows it to selectively forget or remember information from previous time steps, which makes it particularly effective for tasks that involve long-term dependencies, such as language modeling and machine translation The `LSTM` layer consists of memory cells that store information over time, input gates that regulate the flow of new information into the memory cells, and output gates that control the output of the layer. For more information, read the original article proposal for this arquitecture, \"_[Long Short-Term Memory](https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735)_.\"\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Version:  2.10.1\n", "Eager mode:  True\n", "GPU is available\n", "Model: \"model\"\n", "_________________________________________________________________\n", " Layer (type)                Output Shape              Param #   \n", "=================================================================\n", " input_1 (InputLayer)        [(None, None)]            0         \n", "                                                                 \n", " embedding (Embedding)       (None, None, 128)         640000    \n", "                                                                 \n", " bidirectional (Bidirectiona  (None, None, 128)        98816     \n", " l)                                                              \n", "                                                                 \n", " bidirectional_1 (Bidirectio  (None, 128)              98816     \n", " nal)                                                            \n", "                                                                 \n", " dense (Dense)               (None, 1)                 129       \n", "                                                                 \n", "=================================================================\n", "Total params: 837,761\n", "Trainable params: 837,761\n", "Non-trainable params: 0\n", "_________________________________________________________________\n", "Epoch 1/20\n", "1701/1701 [==============================] - 244s 138ms/step - loss: 0.4425 - accuracy: 0.7948 - val_loss: 0.3625 - val_accuracy: 0.8423\n", "Epoch 2/20\n", "1701/1701 [==============================] - 185s 109ms/step - loss: 0.3827 - accuracy: 0.8296 - val_loss: 0.3558 - val_accuracy: 0.8440\n", "Epoch 3/20\n", "1701/1701 [==============================] - 196s 116ms/step - loss: 0.2899 - accuracy: 0.8803 - val_loss: 0.3040 - val_accuracy: 0.8687\n", "Epoch 4/20\n", "1701/1701 [==============================] - 209s 123ms/step - loss: 0.2545 - accuracy: 0.8983 - val_loss: 0.3109 - val_accuracy: 0.8735\n", "Epoch 5/20\n", "1701/1701 [==============================] - 190s 112ms/step - loss: 0.2139 - accuracy: 0.9170 - val_loss: 0.3376 - val_accuracy: 0.8626\n", "Epoch 6/20\n", "1701/1701 [==============================] - ETA: 0s - loss: 0.1802 - accuracy: 0.9333Restoring model weights from the end of the best epoch: 3.\n", "1701/1701 [==============================] - 141s 83ms/step - loss: 0.1802 - accuracy: 0.9333 - val_loss: 0.3424 - val_accuracy: 0.8759\n", "Epoch 6: early stopping\n", "532/532 [==============================] - 19s 36ms/step - loss: 0.3035 - accuracy: 0.8718\n", "Final Loss: 0.3.\n", "Final Performance: 87.18 %.\n"]}], "source": ["import io\n", "import json\n", "import numpy as np\n", "import tensorflow as tf\n", "from sklearn.model_selection import train_test_split\n", "from keras_preprocessing.sequence import pad_sequences\n", "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n", "\n", "vocab_size = 5000\n", "embed_size = 128\n", "sequence_length = 250\n", "\n", "tokenizer = Tokenizer(num_words=vocab_size,\n", "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n", "                      lower=True,\n", "                      split=\" \",\n", "                      oov_token=\"<OOV>\")\n", "\n", "tokenizer.fit_on_texts(df.review)\n", "tokenizer_json = tokenizer.to_json()\n", "\n", "with io.open('models/tokenizer_senti_model_pt.json', 'w', encoding='utf-8') as fp:\n", "    fp.write(json.dumps(tokenizer_json, ensure_ascii=False))\n", "    fp.close()\n", "\n", "x_train, x_test, y_train, y_test = train_test_split(\n", "    df.review, df.sentiment, test_size=0.2, random_state=42)\n", "\n", "x_train = pad_sequences(\n", "    tokenizer.texts_to_sequences(x_train), \n", "    maxlen=sequence_length, \n", "    truncating='post')\n", "x_test = pad_sequences(\n", "    tokenizer.texts_to_sequences(x_test), \n", "    maxlen=sequence_length, \n", "    truncating='post')\n", "y_train = np.array(y_train).astype(float)\n", "y_test = np.array(y_test).astype(float)\n", "\n", "\n", "inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\n", "x = tf.keras.layers.Embedding(input_dim=vocab_size,\n", "                              output_dim=embed_size,\n", "                              input_length=sequence_length)(inputs)\n", "\n", "x = tf.keras.layers.Bidirectional(\n", "    tf.keras.layers.LSTM(64, return_sequences=True))(x)\n", "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n", "\n", "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n", "model = tf.keras.Model(inputs, outputs)\n", "\n", "model.compile(loss=tf.losses.BinaryCrossentropy(),\n", "              optimizer='adam',\n", "              metrics=['accuracy'])\n", "\n", "print(\"Version: \", tf.__version__)\n", "print(\"Eager mode: \", tf.executing_eagerly())\n", "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n", "model.summary()\n", "\n", "callbacks = [tf.keras.callbacks.ModelCheckpoint(\"models/senti_model_sigmoid_pt.keras\",\n", "                                                save_best_only=True),\n", "            tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n", "                                            patience=3,\n", "                                            verbose=1,\n", "                                            mode=\"auto\",\n", "                                            baseline=None,\n", "                                            restore_best_weights=True)]\n", "                                            \n", "                                            \n", "                                                \n", "model.fit(x_train,\n", "          y_train,\n", "          epochs=20,\n", "          validation_split=0.2,\n", "          callbacks=callbacks,\n", "          verbose=1)\n", "\n", "test_loss_score, test_acc_score = model.evaluate(x_test, y_test)\n", "\n", "print(f'Final Loss: {round(test_loss_score, 2)}.')\n", "print(f'Final Performance: {round(test_acc_score * 100, 2)} %.')\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["If you prefer, you can create the same model with a `softmax` output function."]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Version:  2.10.1\n", "Eager mode:  True\n", "GPU is available\n", "Model: \"model_2\"\n", "_________________________________________________________________\n", " Layer (type)                Output Shape              Param #   \n", "=================================================================\n", " input_3 (InputLayer)        [(None, None)]            0         \n", "                                                                 \n", " embedding_2 (Embedding)     (None, None, 128)         640000    \n", "                                                                 \n", " bidirectional_4 (Bidirectio  (None, None, 128)        98816     \n", " nal)                                                            \n", "                                                                 \n", " bidirectional_5 (Bidirectio  (None, 128)              98816     \n", " nal)                                                            \n", "                                                                 \n", " dense_2 (Dense)             (None, 2)                 258       \n", "                                                                 \n", "=================================================================\n", "Total params: 837,890\n", "Trainable params: 837,890\n", "Non-trainable params: 0\n", "_________________________________________________________________\n", "Epoch 1/20\n", "1702/1702 [==============================] - 126s 72ms/step - loss: 0.4925 - accuracy: 0.7631 - val_loss: 0.3952 - val_accuracy: 0.8243\n", "Epoch 2/20\n", "1702/1702 [==============================] - 125s 74ms/step - loss: 0.3304 - accuracy: 0.8601 - val_loss: 0.3393 - val_accuracy: 0.8589\n", "Epoch 3/20\n", "1702/1702 [==============================] - 147s 87ms/step - loss: 0.2689 - accuracy: 0.8908 - val_loss: 0.2957 - val_accuracy: 0.8746\n", "Epoch 4/20\n", "1702/1702 [==============================] - 163s 96ms/step - loss: 0.2359 - accuracy: 0.9066 - val_loss: 0.2880 - val_accuracy: 0.8811\n", "Epoch 5/20\n", "1702/1702 [==============================] - 142s 83ms/step - loss: 0.2041 - accuracy: 0.9215 - val_loss: 0.2912 - val_accuracy: 0.8795\n", "Epoch 6/20\n", "1702/1702 [==============================] - 137s 81ms/step - loss: 0.1734 - accuracy: 0.9346 - val_loss: 0.3220 - val_accuracy: 0.8770\n", "Epoch 7/20\n", "1702/1702 [==============================] - ETA: 0s - loss: 0.1432 - accuracy: 0.9481Restoring model weights from the end of the best epoch: 4.\n", "1702/1702 [==============================] - 137s 80ms/step - loss: 0.1432 - accuracy: 0.9481 - val_loss: 0.3569 - val_accuracy: 0.8768\n", "Epoch 7: early stopping\n", "532/532 [==============================] - 22s 41ms/step - loss: 0.2820 - accuracy: 0.8838\n", "Final Loss: 0.28.\n", "Final Performance: 88.38 %.\n"]}], "source": ["import io\n", "import json\n", "import numpy as np\n", "import tensorflow as tf\n", "from sklearn.model_selection import train_test_split\n", "from keras_preprocessing.sequence import pad_sequences\n", "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n", "\n", "vocab_size = 5000\n", "embed_size = 128\n", "sequence_length = 250\n", "\n", "tokenizer = Tokenizer(num_words=vocab_size,\n", "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n", "                      lower=True,\n", "                      split=\" \",\n", "                      oov_token=\"<OOV>\")\n", "\n", "tokenizer.fit_on_texts(df.review)\n", "tokenizer_json = tokenizer.to_json()\n", "\n", "with io.open('models/tokenizer_senti_model_pt.json', 'w', encoding='utf-8') as fp:\n", "    fp.write(json.dumps(tokenizer_json, ensure_ascii=False))\n", "    fp.close()\n", "\n", "x_train, x_test, y_train, y_test = train_test_split(\n", "    df.review, df.sentiment, test_size=0.2, random_state=42)\n", "\n", "x_train = pad_sequences(\n", "    tokenizer.texts_to_sequences(x_train), \n", "    maxlen=sequence_length, \n", "    truncating='post')\n", "x_test = pad_sequences(\n", "    tokenizer.texts_to_sequences(x_test), \n", "    maxlen=sequence_length, \n", "    truncating='post')\n", "y_train = np.array(y_train).astype(float)\n", "y_test = np.array(y_test).astype(float)\n", "\n", "inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\n", "x = tf.keras.layers.Embedding(input_dim=vocab_size,\n", "                              output_dim=embed_size,\n", "                              input_length=sequence_length)(inputs)\n", "\n", "\n", "x = tf.keras.layers.Bidirectional(\n", "    tf.keras.layers.LSTM(64, return_sequences=True))(x)\n", "x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n", "\n", "\n", "outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n", "model = tf.keras.Model(inputs, outputs)\n", "\n", "model.compile(loss='sparse_categorical_crossentropy',\n", "              optimizer='adam',\n", "              metrics=['accuracy'])\n", "\n", "print(\"Version: \", tf.__version__)\n", "print(\"Eager mode: \", tf.executing_eagerly())\n", "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n", "model.summary()\n", "\n", "callbacks = [tf.keras.callbacks.ModelCheckpoint(\"models/senti_model_softmax_pt.keras\",\n", "                                                save_best_only=True),\n", "            tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n", "                                            patience=3,\n", "                                            verbose=1,\n", "                                            mode=\"auto\",\n", "                                            baseline=None,\n", "                                            restore_best_weights=True)]\n", "                                                                                            \n", "model.fit(x_train,\n", "          y_train,\n", "          epochs=20,\n", "          validation_split=0.2,\n", "          callbacks=callbacks,\n", "          verbose=1)\n", "\n", "test_loss_score, test_acc_score = model.evaluate(x_test, y_test)\n", "\n", "print(f'Final Loss: {round(test_loss_score, 2)}.')\n", "print(f'Final Performance: {round(test_acc_score * 100, 2)} %.')"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["Congratulations, you have trained your own `Bi-LSTM`. \ud83d\ude43\n", "\n", "---\n", "\n", "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3.9.13 64-bit", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.13"}, "orig_nbformat": 4, "vscode": {"interpreter": {"hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"}}}, "nbformat": 4, "nbformat_minor": 2}