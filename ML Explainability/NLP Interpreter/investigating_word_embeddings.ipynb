{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Exploring Language Models through `Word Embeddings`\n","\n","Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n","\n","`Word embeddings` are a type of representation that captures the meaning and context of words in a numerical form. They are used extensively in `natural language processing` tasks such as language modeling, sentiment analysis, and machine translation. Word embeddings are typically learned from large text corpora using unsupervised learning techniques such as `word2vec`. However, embeddings can also be trained as just another layer in a neural network. The resulting word embeddings are high-dimensional vectors representing each word in the vocabulary.\n","\n","These vectors are designed so that words with similar meanings and contexts have similar vector representations, while words with different meanings and contexts have dissimilar representations.\n","\n","This property makes `word embeddings` a powerful tool for many NLP tasks, as they can be used to find semantic relationships between words, cluster similar words together, and even perform arithmetic operations such as word analogies. Something that can help us better understand how language models \"_understand_\" language. \n","\n","![word-embeddings](https://lena-voita.github.io/resources/lectures/word_emb/lookup_table.gif)\n","\n","In this notebook, we create a language model for sentiment analysis using `Keras API` and `TensorFlow`. We only want to train the embedding layer, so do not pay attention to the rest of the architecture (is merely a husk).\n","\n","We will be using a dataset that was put together by combining several datasets for sentiment classification available on [Kaggle](https://www.kaggle.com/):\n","\n","- The `IMDB 50K` [dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?select=IMDB+Dataset.csv): _0K movie reviews for natural language processing or Text analytics._\n","- The `Twitter US Airline Sentiment` [dataset](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment): _originated from the  [Crowdflower's Data for Everyone library](http://www.crowdflower.com/data-for-everyone)._\n","- Our `google_play_apps_review` _dataset: built using the `google_play_scraper` in [this notebook](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/64d0693c28786ce42149411bec8b3b42520fc4df/ML%20Explainability/NLP%20Interpreter%20(en)/scrape(en).ipynb)._\n","- The `EcoPreprocessed` [dataset](https://www.kaggle.com/datasets/pradeeshprabhakar/preprocessed-dataset-sentiment-analysis): _scrapped amazon product reviews_.\n","\n","The final result is the `sentiment_analysis_dataset.csv` available for download in [this link](https://drive.google.com/uc?export=download&id=1_ijhnVLHddM7Cm3R3vfqBB-svw6iNfpv). Also available in [portuguese](https://drive.google.com/uc?export=download&id=1YCIzGqcdlHSy-GvghRp0U5USUhuOVEE3)!\n","\n","Both datasets already come preprocessed, and the `cleaning` function we used is this:\n","\n","```python\n","\n","import re\n","from unidecode import unidecode\n","\n","def custom_standardization(input_data):\n","    clean_text = input_data.lower().replace(\"<br />\", \" \")\n","    clean_text = re.sub(r\"[-()\\\"#/@;:<>{}=~|.?,]\", ' ', clean_text)\n","    clean_text = re.sub(' +', ' ', clean_text)\n","    return unidecode(clean_text)\n","\n","```"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>one of the other reviewers has mentioned that ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>a wonderful little production the filming tech...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>i thought this was a wonderful way to spend ti...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>basically there's a family where a little boy ...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>petter mattei's love in the time of money is a...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>probably my all time favorite movie a story of...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>i sure would like to see a resurrection of a u...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>this show was an amazing fresh &amp; innovative id...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>encouraged by the positive comments about this...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>if you like original gut wrenching laughter yo...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                              review  sentiment\n","0  one of the other reviewers has mentioned that ...          1\n","1  a wonderful little production the filming tech...          1\n","2  i thought this was a wonderful way to spend ti...          1\n","3  basically there's a family where a little boy ...          0\n","4  petter mattei's love in the time of money is a...          1\n","5  probably my all time favorite movie a story of...          1\n","6  i sure would like to see a resurrection of a u...          1\n","7  this show was an amazing fresh & innovative id...          0\n","8  encouraged by the positive comments about this...          0\n","9  if you like original gut wrenching laughter yo...          1"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","import urllib.request\n","\n","# PT-BR https://drive.google.com/uc?export=download&id=1YCIzGqcdlHSy-GvghRp0U5USUhuOVEE3\n","# EN https://drive.google.com/uc?export=download&id=1_ijhnVLHddM7Cm3R3vfqBB-svw6iNfpv\n","\n","urllib.request.urlretrieve(\n","    'https://drive.google.com/uc?export=download&id=1_ijhnVLHddM7Cm3R3vfqBB-svw6iNfpv', \n","    'sentiment_analysis_dataset_en.csv'\n",")\n","\n","df = pd.read_csv('sentiment_analysis_dataset_en.csv')\n","\n","display(df.head(10))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Before training our `embeddings`, we need to extract a vocabulary of our corpus. And we will do that by using the `TextVectorization` layer from the [`Keras`](https://keras.io/) API. `TextVectorization` is a text preprocessing layer in `Keras`, which is used to vectorize the text data by converting text into a numerical format that can be used as input to a machine learning model. \n","\n","We will use ' word tokenization ' for the purposes of this notebook (interpret the embedding layer of a language model). However, there are many other types of possible tokenization schemes (e.g., `bi-gram tokenization`). Below are some common types of tokenization used in natural language processing:\n","\n","- `Word tokenization`: This involves breaking a text into words or tokens, where words are defined as sequences of characters separated by spaces or punctuation marks.\n","- `Character tokenization`: This involves breaking a text into individual characters. This approach can be useful when dealing with languages that do not use spaces to separate words.\n","- `Subword tokenization`: This involves breaking words into smaller units, called subwords or subword units, which can be used to build up words in a language. This approach is commonly used in neural machine translation.\n","- `Byte pair encoding`: This subword tokenization type uses pairs of consecutive bytes to represent subwords.\n","\n","In the cell below, we create our `TextVectorization` layer and adapt it to our text corpus, setting the vocabulary size to 10,000 words. After, we use `train_test_split` to break down our corpus into training and validation sets (no test set here because we actually only care about training the `embedding layer`).\n","```"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training Inputs:  (76580, 100)\n","Validation Inputs:  (8509, 100)\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","\n","\n","vocab_size = 10000\n","sequence_length = 100\n","\n","vectorization_layer = tf.keras.layers.TextVectorization(\n","    max_tokens=vocab_size,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length\n","    )\n","\n","vectorization_layer.adapt(df.review)\n","english_embedding_vocabulary = vectorization_layer.get_vocabulary()\n","\n","with open(r'models/english_embedding_vocabulary.txt', 'w', encoding='utf-8') as fp:\n","    for word in english_embedding_vocabulary:\n","        fp.write(\"%s\\n\" % word)\n","    fp.close()\n","\n","x_train, x_val, y_train, y_val = train_test_split(\n","    df.review, df.sentiment, test_size=0.1, random_state=42)\n","\n","x_train = vectorization_layer(x_train)\n","y_train = np.array(y_train).astype(float)\n","x_val = vectorization_layer(x_val)\n","y_val = np.array(y_val).astype(float)\n","\n","print('Training Inputs: ', x_train.shape)\n","print('Validation Inputs: ', x_val.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For the purposes of this notebook, we will create a simple model with a 16-embedding dimension.\n","\n","> Note: in Keras/TensorFlow, you can name the layers of your model (and the model itself) as you prefer. This is helpful if you need to retrieve a certain portion of your model later."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"EnglishEmbedding16\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input (InputLayer)          [(None, None)]            0         \n","                                                                 \n"," embedding (Embedding)       (None, None, 16)          160000    \n","                                                                 \n"," globa_average (GlobalAverag  (None, 16)               0         \n"," ePooling1D)                                                     \n","                                                                 \n"," dense (Dense)               (None, 16)                272       \n","                                                                 \n"," output (Dense)              (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 160,289\n","Trainable params: 160,289\n","Non-trainable params: 0\n","_________________________________________________________________\n"]},{"data":{"text/plain":["None"]},"metadata":{},"output_type":"display_data"}],"source":["embed_size = 16\n","\n","inputs = tf.keras.Input(shape=(None,), dtype=\"int32\", name='input')\n","x = tf.keras.layers.Embedding(input_dim=vocab_size,\n","                              output_dim=embed_size,\n","                              input_length=sequence_length,\n","                              name='embedding')(inputs)\n","\n","x = tf.keras.layers.GlobalAveragePooling1D(name='globa_average')(x)\n","x = tf.keras.layers.Dense(embed_size, activation=\"relu\", name='dense')(x)\n","\n","outputs = tf.keras.layers.Dense(1, name='output')(x)\n","\n","model_16 = tf.keras.Model(inputs, outputs)\n","\n","model_16._name=\"EnglishEmbedding16\"\n","\n","model_16.compile(loss=tf.losses.BinaryCrossentropy(from_logits = True),\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","display(model_16.summary())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We showed how to plot the history of your model's training using the `history` dictionary provided by the `fit` function in this [early notebook](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/55dec4959b0121ab7a441a4448c25dcf66d085da/ML%20Intro%20Course/8_Fashion_MNIST.ipynb). However, you can attain the same thing with only two lines of code and the `TensorBoard` package. You can log all these results by setting a `logs` folder, and instruct your model to save the relevant info to this folder via a Keras `callback`."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["%load_ext tensorboard\n","log_folder = 'logs'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Since our model only has a small number of units to form the latent dimension, it will quickly overfit. Using `EarlyStopping`, even with a max of 20 epochs, should stop the training under 10 epochs if we set the patience to something like 5 or 6."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Version:  2.10.1\n","Eager mode:  True\n","GPU is available\n","Epoch 1/20\n","2394/2394 [==============================] - 24s 9ms/step - loss: 0.4345 - accuracy: 0.7732 - val_loss: 0.3542 - val_accuracy: 0.8395\n","Epoch 2/20\n","2394/2394 [==============================] - 19s 8ms/step - loss: 0.3260 - accuracy: 0.8552 - val_loss: 0.3530 - val_accuracy: 0.8473\n","Epoch 3/20\n","2394/2394 [==============================] - 20s 8ms/step - loss: 0.3058 - accuracy: 0.8669 - val_loss: 0.3533 - val_accuracy: 0.8418\n","Epoch 4/20\n","2394/2394 [==============================] - 20s 8ms/step - loss: 0.2946 - accuracy: 0.8733 - val_loss: 0.3542 - val_accuracy: 0.8432\n","Epoch 5/20\n","2394/2394 [==============================] - 19s 8ms/step - loss: 0.2866 - accuracy: 0.8758 - val_loss: 0.3588 - val_accuracy: 0.8368\n","Epoch 6/20\n","2394/2394 [==============================] - 20s 8ms/step - loss: 0.2812 - accuracy: 0.8801 - val_loss: 0.3667 - val_accuracy: 0.8344\n","Epoch 7/20\n","2389/2394 [============================>.] - ETA: 0s - loss: 0.2751 - accuracy: 0.8827Restoring model weights from the end of the best epoch: 2.\n","2394/2394 [==============================] - 19s 8ms/step - loss: 0.2751 - accuracy: 0.8826 - val_loss: 0.3665 - val_accuracy: 0.8339\n","Epoch 7: early stopping\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x1aee066cb50>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["callbacks = [tf.keras.callbacks.ModelCheckpoint(\"models/english_embedding_vocabulary_16.keras\",\n","                                                save_best_only=True),\n","tf.keras.callbacks.TensorBoard(log_dir=\"logs\"),         \n","tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n","                                            patience=5,\n","                                            verbose=1,\n","                                            mode=\"auto\",\n","                                            baseline=None,\n","                                            restore_best_weights=True)]\n","\n","print(\"Version: \", tf.__version__)\n","print(\"Eager mode: \", tf.executing_eagerly())\n","print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n","\n","model_16.fit(x_train,\n","          y_train,\n","          epochs=20,\n","          validation_data=(x_val, y_val),\n","          callbacks=callbacks,\n","          verbose=1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let us now vizualize our training with the logs from our `tensorboard`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%tensorboard --logdir logs"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["If you do not want to train the models, you can load the trained versions in the cell below. But first, you need to download them (instructions in the `models` folder.)\n","\n","As said before, `word embeddings` are a powerful technique used in natural language processing to represent words or tokens in a high-dimensional space. By mapping each token to a point in this space, token embeddings provide a way to translate the meaning of words into a geometric representation. \n","\n","In this representation, tokens with similar meanings are clustered together, while tokens with different meanings are far apart. This allows language models to capture the relationships between words in a geometric sense. The high dimensionality of the space also allows for complex relationships to be represented, such as the relationships between synonyms and antonyms and even social constructs like gender. \n","\n","Below, we will load our saved model and its vocabulary and retrieve the trained word embeddings of our `embedding layer`. We will use our saved vocabulary and these vectors to create a dictionary of `{\"words\":\"embeddings\"}`."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Embeddings Dimensions:  (9999, 16)\n","Vocabulary Size:  9999\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","\n","model = tf.keras.models.load_model('models/english_embedding_vocabulary_16.keras')\n","\n","with open('models/english_embedding_vocabulary.txt', encoding='utf-8') as fp:\n","    english_embedding_vocabulary = [line.strip() for line in fp]\n","    fp.close()\n","\n","embeddings = model.get_layer('embedding').get_weights()[0]\n","\n","words_embeddings = {}\n"," \n","# iterating through the elements of list\n","for i, word in enumerate(english_embedding_vocabulary):\n","    # here we skip the embedding/token 0 (\"\"), because is just the PAD token.\n","    if i == 0:\n","        continue\n","    words_embeddings[word] = embeddings[i]\n","\n","print(\"Embeddings Dimensions: \", np.array(list(words_embeddings.values())).shape)\n","print(\"Vocabulary Size: \", len(words_embeddings.keys()))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["One way to explore the relationship of these embedding vectors is by how _similar_ they are.\n","\n","Cosine similarity is a similarity measure between two non-zero vectors of an inner product space. It measures the cosine of the angle between the two vectors and returns a value between -1 and 1, where 1 means identical, 0 means orthogonal, and -1 means opposite.\n","\n","We can use cosine similarity to compare word embeddings in natural language processing. By calculating the cosine similarity between two-word embeddings, we can measure how similar the two words are in meaning and context.\n","\n","The equation for cosine similarity is:\n","\n","$$\\text{cosine\\_similarity}(u, v) = \\frac{u \\cdot v}{\\lVert u \\rVert \\lVert v \\rVert} = \\cos(\\theta)$$\n","\n","\n","Where:\n","\n","- $u$ and $v$ are the two vectors being compared.\n","- $\\cdot$ represents the dot product operation. \n","- $\\lVert u \\rVert$ and $\\lVert v \\rVert$ are the magnitudes of the two vectors.\n","- $\\theta$ is the angle between them.\n","\n","To calculate the cosine similarity between two-word embeddings, we simply plug the embeddings as the vectors $u$ and $v$ in the above equation."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cosine Similarity between 'wonderful' with 'horrible': -0.9784237146377563\n","Cosine Similarity between 'good' with 'bad': -0.9407784342765808\n"]}],"source":["from numpy.linalg import norm\n","\n","def cosine_similarity(word1, word2, dictionary):\n","    \"\"\"\n","    Computes the cosine similarity between a two given words.\n","    \n","    Parameters:\n","    2 strings : str\n","        Two words to be compared.\n","    dictionary : dict\n","        A dictionary with words as keys and their corresponding embeddings as values.\n","    \n","    Returns:\n","    --------\n","    The Cosine Similarity Score (float).\n","    -----------\n","    \"\"\"\n","    return np.dot(dictionary[word1], dictionary[word2])/(norm(dictionary[word1])*norm(dictionary[word2]))\n","\n","cos = cosine_similarity(\"wonderful\", \"horrible\",words_embeddings) \n","print(f\"\"\"Cosine Similarity between 'wonderful' with 'horrible': {cos}\"\"\")\n","\n","cos = cosine_similarity(\"good\", \"bad\",words_embeddings) \n","print(f\"\"\"Cosine Similarity between 'good' with 'bad': {cos}\"\"\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The word embeddings for \"wonderful\" and \"horrible\", just as the embeddings for \"good\" and \"bad\", are almost opposite words (as they should be).\n","\n","However, similar adjectives (something really important for sentiment analysis models) should show a high positive value (and they do!)."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cosine Similarity between 'good' with 'beautiful': 0.9373324513435364\n"]}],"source":["cos = cosine_similarity(\"good\", \"beautiful\",words_embeddings) \n","print(f\"\"\"Cosine Similarity between 'good' with 'beautiful': {cos}\"\"\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, let us create a function to get the most similar word embeddings according to the cosine similarity measure. Ordering the vocabulary by similarity also gives us the most \"_different_\" word embeddings."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cosine Similarity (most different word embeddings:)\n"]},{"data":{"text/markdown":["| Closest Match   |   Similarity Score |\n","|:----------------|-------------------:|\n","| pleasure        |          -0.993557 |\n","| tear            |          -0.992812 |\n","| thank           |          -0.992007 |\n","| pleasant        |          -0.991167 |\n","| rewarded        |          -0.990919 |"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Cosine Similarity (most similar word embeddings:)\n"]},{"data":{"text/markdown":["| Closest Match   |   Similarity Score |\n","|:----------------|-------------------:|\n","| waste           |           0.996396 |\n","| hrs             |           0.99642  |\n","| sucks           |           0.996422 |\n","| hoping          |           0.996789 |\n","| terrible        |           0.997209 |"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"}],"source":["import numpy as np\n","import pandas as pd\n","from numpy.linalg import norm\n","from IPython.display import Markdown \n","\n","def compute_cosine_table(string, dictionary, \n","                         vocabulary):\n","    \"\"\"\n","    Computes the cosine similarity between a given word and all other words in a dictionary.\n","    \n","    Parameters:\n","    -----------\n","    string : str\n","        The word to compare against.\n","    dictionary : dict\n","        A dictionary with words as keys and their corresponding embeddings as values.\n","    vocabulary : list\n","        A list of words in the dictionary.\n","    \n","    Returns:\n","    --------\n","    A pandas DataFrame with the closest matches to the input word and their \n","    corresponding similarity scores. The index of the DataFrame is set \n","    to the closest matches.\n","    \"\"\"\n","\n","    l = vocabulary.copy()\n","    l.remove(string)\n","\n","    cos = []\n","    for word in l[1::]:\n","\n","        cosine = np.dot(dictionary[string],\n","                dictionary[word])/(norm(dictionary[string])*norm(dictionary[word]))\n","        cos.append(cosine)\n","\n","    return pd.DataFrame({\"Closest Match\": l[1::],f\"Similarity Score\": cos})\\\n","        .sort_values(f\"Similarity Score\", ascending=True)\\\n","        .set_index('Closest Match')\n","\n","df = compute_cosine_table(\"horrible\", \n","        words_embeddings, \n","        english_embedding_vocabulary)\n","\n","print(\"Cosine Similarity (most different word embeddings:)\")\n","display(Markdown(df.head(5).to_markdown()))\n","\n","print(\"Cosine Similarity (most similar word embeddings:)\")\n","display(Markdown(df.tail(5).to_markdown()))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\"_terrible_\" is the word embedding most similar to \"_horrible_\", wich \"meks sence\". However, the reason why \"_devito_\" s as antagonistic to \"_horrible_\" as \"_funniest_\" is open to interpretation (e.g., maybe the IMDB portion of our dataset is biased toward liking Danny DeVito.).\n","\n","Tensorflow makes available a tool for projecting `word embeddings` into a 3D space. You can use their [word projector](https://projector.tensorflow.org/) by saving your vocabulary and word embeddings into special files called `metadata.tsv` and `vectors.tsv`. Give it a try if you want to.\n","\n","Below, we save these files on the `TensorBoard` \"logs\" folder, and then use it to start the `tensorboard projector`, which is integrated in `TensorBoard`."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["from tensorboard.plugins import projector\n","\n","model = tf.keras.models.load_model('models/english_embedding_vocabulary_16.keras')\n","\n","with open('models/english_embedding_vocabulary.txt', encoding='utf-8') as fp:\n","    english_embedding_vocabulary = [line.strip() for line in fp]\n","    fp.close()\n","\n","# Save the weights we want to analyze as a `tf.variable``. Note that the first\n","# value represents the padding token, so we just skip it. \n","embeddings = tf.Variable(model.get_layer('embedding').get_weights()[0][1:])\n","checkpoint = tf.train.Checkpoint(embedding=embeddings)\n","checkpoint.save(\"logs/embedding.ckpt\")\n","\n","# Save labels separately on a line-by-line manner. Note that the first\n","# value represents the padding token, so we just skip it.\n","with open('logs/metadata.tsv', \"w\") as fp:\n","  for words in english_embedding_vocabulary[1:]:\n","    fp.write(\"{}\\n\".format(words))\n","\n","# Set up the configuration settings to start the projector.\n","config = projector.ProjectorConfig()\n","embedding = config.embeddings.add()\n","\n","embedding.tensor_name = \"logs/embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n","embedding.metadata_path = 'logs/metadata.tsv'\n","projector.visualize_embeddings(\"/logs\", config)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, just start the `TensorBoard`, chose the \"logs\" file as home directory, and select the `Projector` dashboard."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%tensorboard --logdir /logs"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["But we can also create our own projections! \n","\n","Let us create a 3D projection of our embedding space. This can allow us to visualize the embedding space, and the proximity between word embeddings, in a geometrical sense.\n","\n","Cosine similarity is a measure of (as the name says), similarity, and not of distance. However, we can bring our 16-dimensional space to a 3D embedding space where distances can be calculated visually. We can achieve this by using `t-SNE`, something we briefly explained in [this notebook](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/55dec4959b0121ab7a441a4448c25dcf66d085da/ML%20Intro%20Course/8_Fashion_MNIST.ipynb).\n","\n","Let us first turn our word embeddings and vocabulary into a DataFrame."]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>embedding_0</th>\n","      <th>embedding_1</th>\n","      <th>embedding_2</th>\n","      <th>embedding_3</th>\n","      <th>embedding_4</th>\n","      <th>embedding_5</th>\n","      <th>embedding_6</th>\n","      <th>embedding_7</th>\n","      <th>embedding_8</th>\n","      <th>embedding_9</th>\n","      <th>embedding_10</th>\n","      <th>embedding_11</th>\n","      <th>embedding_12</th>\n","      <th>embedding_13</th>\n","      <th>embedding_14</th>\n","      <th>embedding_15</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>the</th>\n","      <td>-0.076551</td>\n","      <td>-0.014924</td>\n","      <td>0.067767</td>\n","      <td>0.007883</td>\n","      <td>-0.016938</td>\n","      <td>0.268177</td>\n","      <td>0.034460</td>\n","      <td>-0.048748</td>\n","      <td>0.036562</td>\n","      <td>-0.015586</td>\n","      <td>-0.021275</td>\n","      <td>-0.058763</td>\n","      <td>-0.019428</td>\n","      <td>-0.112663</td>\n","      <td>-0.064345</td>\n","      <td>0.112612</td>\n","    </tr>\n","    <tr>\n","      <th>and</th>\n","      <td>-0.029695</td>\n","      <td>0.041913</td>\n","      <td>0.070794</td>\n","      <td>-0.018414</td>\n","      <td>0.035200</td>\n","      <td>0.250137</td>\n","      <td>-0.021369</td>\n","      <td>-0.022792</td>\n","      <td>0.000395</td>\n","      <td>0.018333</td>\n","      <td>0.013924</td>\n","      <td>-0.025006</td>\n","      <td>-0.108997</td>\n","      <td>-0.071116</td>\n","      <td>-0.037261</td>\n","      <td>0.122586</td>\n","    </tr>\n","    <tr>\n","      <th>a</th>\n","      <td>-0.093505</td>\n","      <td>0.011508</td>\n","      <td>-0.039603</td>\n","      <td>-0.004146</td>\n","      <td>-0.004097</td>\n","      <td>0.314304</td>\n","      <td>-0.002159</td>\n","      <td>-0.057122</td>\n","      <td>-0.028662</td>\n","      <td>0.023549</td>\n","      <td>0.064215</td>\n","      <td>-0.035054</td>\n","      <td>0.002804</td>\n","      <td>-0.086173</td>\n","      <td>-0.019819</td>\n","      <td>0.081942</td>\n","    </tr>\n","    <tr>\n","      <th>to</th>\n","      <td>-0.090330</td>\n","      <td>0.012954</td>\n","      <td>0.038419</td>\n","      <td>0.056030</td>\n","      <td>-0.042045</td>\n","      <td>0.376835</td>\n","      <td>0.024688</td>\n","      <td>-0.058342</td>\n","      <td>-0.048245</td>\n","      <td>0.085957</td>\n","      <td>0.003867</td>\n","      <td>-0.027270</td>\n","      <td>-0.081651</td>\n","      <td>-0.115291</td>\n","      <td>-0.002087</td>\n","      <td>0.116711</td>\n","    </tr>\n","    <tr>\n","      <th>of</th>\n","      <td>-0.082830</td>\n","      <td>0.011401</td>\n","      <td>0.049799</td>\n","      <td>-0.015487</td>\n","      <td>-0.040420</td>\n","      <td>0.154868</td>\n","      <td>0.021330</td>\n","      <td>-0.010035</td>\n","      <td>-0.012892</td>\n","      <td>0.026445</td>\n","      <td>0.030382</td>\n","      <td>-0.038912</td>\n","      <td>-0.073732</td>\n","      <td>-0.038778</td>\n","      <td>-0.050055</td>\n","      <td>0.037295</td>\n","    </tr>\n","    <tr>\n","      <th>is</th>\n","      <td>-0.032904</td>\n","      <td>0.060920</td>\n","      <td>0.040417</td>\n","      <td>0.043450</td>\n","      <td>0.016395</td>\n","      <td>0.314284</td>\n","      <td>0.028085</td>\n","      <td>-0.065494</td>\n","      <td>0.032696</td>\n","      <td>-0.010763</td>\n","      <td>0.026772</td>\n","      <td>0.000486</td>\n","      <td>-0.010015</td>\n","      <td>-0.095048</td>\n","      <td>-0.048950</td>\n","      <td>0.041197</td>\n","    </tr>\n","    <tr>\n","      <th>in</th>\n","      <td>-0.049310</td>\n","      <td>0.071329</td>\n","      <td>0.041701</td>\n","      <td>0.030343</td>\n","      <td>0.019761</td>\n","      <td>0.264715</td>\n","      <td>0.020251</td>\n","      <td>-0.091105</td>\n","      <td>0.005416</td>\n","      <td>0.059387</td>\n","      <td>0.001725</td>\n","      <td>-0.036669</td>\n","      <td>-0.066259</td>\n","      <td>-0.089162</td>\n","      <td>-0.033258</td>\n","      <td>0.056866</td>\n","    </tr>\n","    <tr>\n","      <th>i</th>\n","      <td>0.010590</td>\n","      <td>-0.022202</td>\n","      <td>0.000624</td>\n","      <td>-0.035277</td>\n","      <td>-0.008554</td>\n","      <td>0.165885</td>\n","      <td>0.006468</td>\n","      <td>-0.073313</td>\n","      <td>0.029857</td>\n","      <td>-0.016918</td>\n","      <td>0.018820</td>\n","      <td>-0.066553</td>\n","      <td>-0.091927</td>\n","      <td>-0.053967</td>\n","      <td>-0.126327</td>\n","      <td>0.076479</td>\n","    </tr>\n","    <tr>\n","      <th>it</th>\n","      <td>0.034613</td>\n","      <td>0.026286</td>\n","      <td>0.045939</td>\n","      <td>-0.014202</td>\n","      <td>0.018118</td>\n","      <td>0.255664</td>\n","      <td>-0.037551</td>\n","      <td>-0.037671</td>\n","      <td>0.047684</td>\n","      <td>-0.013476</td>\n","      <td>0.076358</td>\n","      <td>-0.117027</td>\n","      <td>-0.120247</td>\n","      <td>-0.097833</td>\n","      <td>-0.074516</td>\n","      <td>0.054467</td>\n","    </tr>\n","    <tr>\n","      <th>this</th>\n","      <td>-0.063159</td>\n","      <td>-0.029197</td>\n","      <td>-0.038168</td>\n","      <td>0.108564</td>\n","      <td>-0.039919</td>\n","      <td>0.372734</td>\n","      <td>-0.000651</td>\n","      <td>0.033397</td>\n","      <td>-0.051223</td>\n","      <td>0.040571</td>\n","      <td>-0.001527</td>\n","      <td>0.005195</td>\n","      <td>-0.002434</td>\n","      <td>-0.086870</td>\n","      <td>-0.042902</td>\n","      <td>0.064033</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n","the     -0.076551    -0.014924     0.067767     0.007883    -0.016938   \n","and     -0.029695     0.041913     0.070794    -0.018414     0.035200   \n","a       -0.093505     0.011508    -0.039603    -0.004146    -0.004097   \n","to      -0.090330     0.012954     0.038419     0.056030    -0.042045   \n","of      -0.082830     0.011401     0.049799    -0.015487    -0.040420   \n","is      -0.032904     0.060920     0.040417     0.043450     0.016395   \n","in      -0.049310     0.071329     0.041701     0.030343     0.019761   \n","i        0.010590    -0.022202     0.000624    -0.035277    -0.008554   \n","it       0.034613     0.026286     0.045939    -0.014202     0.018118   \n","this    -0.063159    -0.029197    -0.038168     0.108564    -0.039919   \n","\n","      embedding_5  embedding_6  embedding_7  embedding_8  embedding_9  \\\n","the      0.268177     0.034460    -0.048748     0.036562    -0.015586   \n","and      0.250137    -0.021369    -0.022792     0.000395     0.018333   \n","a        0.314304    -0.002159    -0.057122    -0.028662     0.023549   \n","to       0.376835     0.024688    -0.058342    -0.048245     0.085957   \n","of       0.154868     0.021330    -0.010035    -0.012892     0.026445   \n","is       0.314284     0.028085    -0.065494     0.032696    -0.010763   \n","in       0.264715     0.020251    -0.091105     0.005416     0.059387   \n","i        0.165885     0.006468    -0.073313     0.029857    -0.016918   \n","it       0.255664    -0.037551    -0.037671     0.047684    -0.013476   \n","this     0.372734    -0.000651     0.033397    -0.051223     0.040571   \n","\n","      embedding_10  embedding_11  embedding_12  embedding_13  embedding_14  \\\n","the      -0.021275     -0.058763     -0.019428     -0.112663     -0.064345   \n","and       0.013924     -0.025006     -0.108997     -0.071116     -0.037261   \n","a         0.064215     -0.035054      0.002804     -0.086173     -0.019819   \n","to        0.003867     -0.027270     -0.081651     -0.115291     -0.002087   \n","of        0.030382     -0.038912     -0.073732     -0.038778     -0.050055   \n","is        0.026772      0.000486     -0.010015     -0.095048     -0.048950   \n","in        0.001725     -0.036669     -0.066259     -0.089162     -0.033258   \n","i         0.018820     -0.066553     -0.091927     -0.053967     -0.126327   \n","it        0.076358     -0.117027     -0.120247     -0.097833     -0.074516   \n","this     -0.001527      0.005195     -0.002434     -0.086870     -0.042902   \n","\n","      embedding_15  \n","the       0.112612  \n","and       0.122586  \n","a         0.081942  \n","to        0.116711  \n","of        0.037295  \n","is        0.041197  \n","in        0.056866  \n","i         0.076479  \n","it        0.054467  \n","this      0.064033  "]},"metadata":{},"output_type":"display_data"}],"source":["# we are starting from index position 1 because the first element is the padding token.\n","\n","df = pd.DataFrame(embeddings[1::], \n","                  columns=[f'embedding_{i}' for i in range(embeddings[1::].shape[1])],\n","                  index=list(words_embeddings.keys())[1::])\n","\n","display(df.head(10))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, let us implement `t-SNE` for dimensionality reduction. But before doing that, we need to keep some things in mind.\n","\n","`t-SNE` (t-distributed Stochastic Neighbor Embedding) is optimized using gradient descent, being also sensitive to the choice of hyperparameters. For example, the choice of perplexity and number of iterations in `t-SNE` depends on the specific dataset and the goals of the analysis. However, here are some general guidelines that can help you choose appropriate values:\n","\n","### `Perplexity`\n","\n","- `Perplexity` controls the balance between local and global aspects of the data. A `perplexity` value of $5$ to $50$ is often used for small to medium datasets.\n","- A higher `perplexity` value may be needed for larger datasets to capture global structure. However, a `perplexity` value that is too high may result in the loss of local structure. Generally, a `perplexity` value of $50$ to $100$ is often used for larger datasets.\n","\n","### `Iterations`\n","\n","- The number of `iterations` determines the amount of computation and time needed to optimize the `t-SNE` algorithm.\n","- For small to medium datasets, a number of `iterations` between $1000$ to $5000$ is often sufficient to obtain a stable embedding.\n","- A higher number of `iterations` may be needed for larger datasets to obtain a stable embedding. However, a very high number of `iterations` may result in overfitting, where the embedding captures noise in the data instead of the underlying structure.\n","\n","You can learn how to \"_[Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/)_\" with this publication."]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[t-SNE] Computing 121 nearest neighbors...\n","[t-SNE] Indexed 9999 samples in 0.000s...\n","[t-SNE] Computed neighbors for 9999 samples in 0.290s...\n","[t-SNE] Computed conditional probabilities for sample 1000 / 9999\n","[t-SNE] Computed conditional probabilities for sample 2000 / 9999\n","[t-SNE] Computed conditional probabilities for sample 3000 / 9999\n","[t-SNE] Computed conditional probabilities for sample 4000 / 9999\n","[t-SNE] Computed conditional probabilities for sample 5000 / 9999\n","[t-SNE] Computed conditional probabilities for sample 6000 / 9999\n","[t-SNE] Computed conditional probabilities for sample 7000 / 9999\n","[t-SNE] Computed conditional probabilities for sample 8000 / 9999\n","[t-SNE] Computed conditional probabilities for sample 9000 / 9999\n","[t-SNE] Computed conditional probabilities for sample 9999 / 9999\n","[t-SNE] Mean sigma: 0.047531\n","[t-SNE] KL divergence after 250 iterations with early exaggeration: 78.511284\n","[t-SNE] KL divergence after 1800 iterations: 2.132993\n"]},{"data":{"text/plain":["'Word Embeddings in 3D.html'"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.manifold import TSNE\n","import plotly.express as px\n","import plotly.offline as py\n","\n","\n","tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=2000)\n","tsne_results = tsne.fit_transform(df.values)\n","\n","\n","fig = px.scatter_3d(\n","    tsne_results, x=0, y=1, z=2, color=df.index,\n","    labels={'0': 't-SNE 1', '1': 't-SNE 2', '2': 't-SNE 3'}\n",")\n","fig.update_layout(template='plotly_white',\n","                  title=f'Word Embeddings in 3D',\n","                  paper_bgcolor='rgba(0, 0, 0, 0)',\n","                  plot_bgcolor='rgba(0, 0, 0, 0)')\n","\n","fig.show()\n","py.plot(fig, filename='Word Embeddings in 3D.html', auto_open=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["To measure the distance between two vectors, you can use the `numpy.linalg.norm` function, which calculates the Euclidean distance between two vectors.\n","\n","This will output the distance between the two vectors $a$ and $b$.\n","\n","The equation for measuring the Euclidean distance between two 3D vectors can be expressed as follows:\n","\n","$$\\text{distance} = \\| \\mathbf{a} - \\mathbf{b} \\| = \\sqrt{(a_1-b_1)^2 + (a_2-b_2)^2 + (a_3-b_3)^2}$$\n","\n","where:\n","\n","- $\\mathbf{a}$ and $\\mathbf{b}$ are the two 3D vectors.\n","- $a_1, a_2, a_3$ and $b_1, b_2, b_3$ are their respective components."]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Distance from \"good\" to \"bad\":  114.304016\n","Distance from \"wonderfully\" to \"horrible\":  104.82702\n","Distance from \"terrible\" to \"horrible\":  4.1755075\n","Distance from \"terrible\" to \"bad\":  34.340137\n"]}],"source":["from numpy.linalg import norm\n","\n","tsne_df = pd.DataFrame(tsne_results, index=df.index)\n","\n","def calculate_embedding_distance(word1, word2, df):\n","    \"\"\"\n","    Computes the Euclidean distance between two given vectors.\n","    \n","    Parameters:\n","    2 strings : str\n","        Two words to be compared.\n","    DataFrame : pandas.DataFrame\n","        A pandas.DataFrame where the index are words and the\n","        columns are the unitary vector values.\n","    Returns:\n","    --------\n","    The Euclidean distance (float).\n","    -----------\n","    \"\"\"\n","    return norm(df.loc[word1].values - df.loc[word2].values)\n","\n","distance = calculate_embedding_distance(\"good\", \"bad\", tsne_df)\n","print(f\"\"\"Distance from \"good\" to \"bad\": {distance}\"\"\")\n","\n","distance = calculate_embedding_distance(\"wonderfully\", \"horrible\", tsne_df)\n","print(f\"\"\"Distance from \"wonderfully\" to \"horrible\": {distance}\"\"\")\n","\n","distance = calculate_embedding_distance(\"terrible\", \"horrible\", tsne_df)\n","print(f\"\"\"Distance from \"terrible\" to \"horrible\": {distance}\"\"\")\n","\n","distance = calculate_embedding_distance(\"terrible\", \"bad\", tsne_df)\n","print(f\"\"\"Distance from \"terrible\" to \"bad\": {distance}\"\"\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Putting together everything we learned thus far, we can start creating similar sub-clusters. For example, we can use cosine similarity to compute what words are more similar to \"devito\".\n","\n","Let us create a list with the 30 most similar (and different) word embeddings of the \"devito\" word embedding."]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["df_cosine_similarity = compute_cosine_table(\"devito\", english_embedding_vocabulary)\n","\n","word_list = []\n","\n","for word in list(df_cosine_similarity.head(30).index):\n","    word_list.append(word)\n","\n","for word in list(df_cosine_similarity.tail(30).index):\n","    word_list.append(word)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, we will select only the 3D projections of these words (a.k.a. the \"_devito_\" similarity cluster)."]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>X</th>\n","      <th>Y</th>\n","      <th>Z</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>journey</th>\n","      <td>-77.105576</td>\n","      <td>9.452806</td>\n","      <td>-6.690478</td>\n","    </tr>\n","    <tr>\n","      <th>worries</th>\n","      <td>-74.205162</td>\n","      <td>-7.504502</td>\n","      <td>12.889542</td>\n","    </tr>\n","    <tr>\n","      <th>appreciate</th>\n","      <td>-77.210968</td>\n","      <td>4.283458</td>\n","      <td>0.652169</td>\n","    </tr>\n","    <tr>\n","      <th>wax</th>\n","      <td>-76.621010</td>\n","      <td>4.872957</td>\n","      <td>-2.495610</td>\n","    </tr>\n","    <tr>\n","      <th>funniest</th>\n","      <td>-74.653740</td>\n","      <td>-6.989667</td>\n","      <td>17.096430</td>\n","    </tr>\n","    <tr>\n","      <th>bourne</th>\n","      <td>-77.230721</td>\n","      <td>-5.434280</td>\n","      <td>8.964151</td>\n","    </tr>\n","    <tr>\n","      <th>thanks</th>\n","      <td>-70.037895</td>\n","      <td>-3.553062</td>\n","      <td>16.745853</td>\n","    </tr>\n","    <tr>\n","      <th>edge</th>\n","      <td>-78.159035</td>\n","      <td>4.993025</td>\n","      <td>-3.280926</td>\n","    </tr>\n","    <tr>\n","      <th>fortunately</th>\n","      <td>-76.363831</td>\n","      <td>6.520238</td>\n","      <td>-2.631117</td>\n","    </tr>\n","    <tr>\n","      <th>devito</th>\n","      <td>-76.851402</td>\n","      <td>-6.721216</td>\n","      <td>7.851744</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     X         Y          Z\n","journey     -77.105576  9.452806  -6.690478\n","worries     -74.205162 -7.504502  12.889542\n","appreciate  -77.210968  4.283458   0.652169\n","wax         -76.621010  4.872957  -2.495610\n","funniest    -74.653740 -6.989667  17.096430\n","bourne      -77.230721 -5.434280   8.964151\n","thanks      -70.037895 -3.553062  16.745853\n","edge        -78.159035  4.993025  -3.280926\n","fortunately -76.363831  6.520238  -2.631117\n","devito      -76.851402 -6.721216   7.851744"]},"metadata":{},"output_type":"display_data"}],"source":["\n","def select_word_projections (word_list, df):\n","    \"\"\"\n","    Select only the vectors associated with a specific word in a \n","    pandas.DataFrame.\n","    \n","    Parameters:\n","    list : list\n","        A list of words to be selected.\n","    DataFrame : pandas.DataFrame\n","        A pandas.DataFrame where the index are words and the\n","        columns are the unitary vector values.\n","    Returns:\n","    --------\n","    A pandas.DataFrame with all the \"X\", \"Y\", \"Z\"\n","    components of the selected words.\n","    -----------\n","    \"\"\"\n","    df = tsne_df.copy().reset_index()\n","    arr = np.array([0,0,0])\n","\n","    for word in word_list:\n","        index = df.loc[df['index'] == word].index[0]\n","        arr = np.vstack((arr, df.drop('index', axis=1).values[index,:]))\n","\n","    arr = np.delete(arr, (0), axis=0)\n","\n","    return pd.DataFrame(arr, index=word_list)\n","\n","cluster = select_word_projections (word_list, tsne_df)\n","\n","display(cluster.head(10))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, let us visualize these word embedding projections in 3D space."]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"hovertemplate":"index=0<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"0","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"0","scene":"scene","showlegend":true,"type":"scatter3d","x":[28.337194442749023],"y":[28.095767974853516],"z":[22.686826705932617]},{"hovertemplate":"index=unlikeable<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"unlikeable","marker":{"color":"#EF553B","symbol":"circle"},"mode":"markers","name":"unlikeable","scene":"scene","showlegend":true,"type":"scatter3d","x":[28.022117614746094],"y":[29.894086837768555],"z":[22.831947326660156]},{"hovertemplate":"index=stinker<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"stinker","marker":{"color":"#00cc96","symbol":"circle"},"mode":"markers","name":"stinker","scene":"scene","showlegend":true,"type":"scatter3d","x":[28.328554153442383],"y":[22.480024337768555],"z":[27.096784591674805]},{"hovertemplate":"index=americanair<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"americanair","marker":{"color":"#ab63fa","symbol":"circle"},"mode":"markers","name":"americanair","scene":"scene","showlegend":true,"type":"scatter3d","x":[31.260292053222656],"y":[17.27120590209961],"z":[26.15913963317871]},{"hovertemplate":"index=irritated<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"irritated","marker":{"color":"#FFA15A","symbol":"circle"},"mode":"markers","name":"irritated","scene":"scene","showlegend":true,"type":"scatter3d","x":[41.91030502319336],"y":[30.75183868408203],"z":[4.599804401397705]},{"hovertemplate":"index=worst<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"worst","marker":{"color":"#19d3f3","symbol":"circle"},"mode":"markers","name":"worst","scene":"scene","showlegend":true,"type":"scatter3d","x":[30.57346534729004],"y":[16.82040023803711],"z":[27.600217819213867]},{"hovertemplate":"index=unwatchable<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"unwatchable","marker":{"color":"#FF6692","symbol":"circle"},"mode":"markers","name":"unwatchable","scene":"scene","showlegend":true,"type":"scatter3d","x":[28.476438522338867],"y":[22.991622924804688],"z":[28.893115997314453]},{"hovertemplate":"index=shoddy<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"shoddy","marker":{"color":"#B6E880","symbol":"circle"},"mode":"markers","name":"shoddy","scene":"scene","showlegend":true,"type":"scatter3d","x":[28.668678283691406],"y":[25.516860961914062],"z":[25.92743682861328]},{"hovertemplate":"index=paint<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"paint","marker":{"color":"#FF97FF","symbol":"circle"},"mode":"markers","name":"paint","scene":"scene","showlegend":true,"type":"scatter3d","x":[43.565643310546875],"y":[31.29891586303711],"z":[4.300244331359863]},{"hovertemplate":"index=potential<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"potential","marker":{"color":"#FECB52","symbol":"circle"},"mode":"markers","name":"potential","scene":"scene","showlegend":true,"type":"scatter3d","x":[35.056793212890625],"y":[34.33269500732422],"z":[12.2561616897583]},{"hovertemplate":"index=trite<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"trite","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"trite","scene":"scene","showlegend":true,"type":"scatter3d","x":[25.97524070739746],"y":[33.173728942871094],"z":[19.550302505493164]},{"hovertemplate":"index=useless<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"useless","marker":{"color":"#EF553B","symbol":"circle"},"mode":"markers","name":"useless","scene":"scene","showlegend":true,"type":"scatter3d","x":[27.425533294677734],"y":[18.520919799804688],"z":[31.79578971862793]},{"hovertemplate":"index=flightr<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"flightr","marker":{"color":"#00cc96","symbol":"circle"},"mode":"markers","name":"flightr","scene":"scene","showlegend":true,"type":"scatter3d","x":[25.759384155273438],"y":[20.900054931640625],"z":[28.304702758789062]},{"hovertemplate":"index=uninteresting<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"uninteresting","marker":{"color":"#ab63fa","symbol":"circle"},"mode":"markers","name":"uninteresting","scene":"scene","showlegend":true,"type":"scatter3d","x":[27.970407485961914],"y":[24.51845359802246],"z":[27.94234275817871]},{"hovertemplate":"index=terrible<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"terrible","marker":{"color":"#FFA15A","symbol":"circle"},"mode":"markers","name":"terrible","scene":"scene","showlegend":true,"type":"scatter3d","x":[28.671175003051758],"y":[16.892818450927734],"z":[30.71419906616211]},{"hovertemplate":"index=disconnected<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"disconnected","marker":{"color":"#19d3f3","symbol":"circle"},"mode":"markers","name":"disconnected","scene":"scene","showlegend":true,"type":"scatter3d","x":[27.476350784301758],"y":[33.18753433227539],"z":[19.11810302734375]},{"hovertemplate":"index=dimensional<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"dimensional","marker":{"color":"#FF6692","symbol":"circle"},"mode":"markers","name":"dimensional","scene":"scene","showlegend":true,"type":"scatter3d","x":[38.3597297668457],"y":[37.96091079711914],"z":[5.26893949508667]},{"hovertemplate":"index=miserably<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"miserably","marker":{"color":"#B6E880","symbol":"circle"},"mode":"markers","name":"miserably","scene":"scene","showlegend":true,"type":"scatter3d","x":[25.77490997314453],"y":[36.72134017944336],"z":[18.331995010375977]},{"hovertemplate":"index=worse<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"worse","marker":{"color":"#FF97FF","symbol":"circle"},"mode":"markers","name":"worse","scene":"scene","showlegend":true,"type":"scatter3d","x":[30.115129470825195],"y":[23.40570068359375],"z":[29.609149932861328]},{"hovertemplate":"index=horrible<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"horrible","marker":{"color":"#FECB52","symbol":"circle"},"mode":"markers","name":"horrible","scene":"scene","showlegend":true,"type":"scatter3d","x":[29.71476173400879],"y":[20.899200439453125],"z":[30.171340942382812]},{"hovertemplate":"index=lacks<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"lacks","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"lacks","scene":"scene","showlegend":true,"type":"scatter3d","x":[27.75799560546875],"y":[25.85382080078125],"z":[24.004772186279297]},{"hovertemplate":"index=absurd<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"absurd","marker":{"color":"#EF553B","symbol":"circle"},"mode":"markers","name":"absurd","scene":"scene","showlegend":true,"type":"scatter3d","x":[31.60247230529785],"y":[35.715274810791016],"z":[16.431331634521484]},{"hovertemplate":"index=uninspired<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"uninspired","marker":{"color":"#00cc96","symbol":"circle"},"mode":"markers","name":"uninspired","scene":"scene","showlegend":true,"type":"scatter3d","x":[26.47586441040039],"y":[21.958877563476562],"z":[26.77507209777832]},{"hovertemplate":"index=redeeming<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"redeeming","marker":{"color":"#ab63fa","symbol":"circle"},"mode":"markers","name":"redeeming","scene":"scene","showlegend":true,"type":"scatter3d","x":[25.558027267456055],"y":[24.877958297729492],"z":[28.621719360351562]},{"hovertemplate":"index=miscast<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"miscast","marker":{"color":"#FFA15A","symbol":"circle"},"mode":"markers","name":"miscast","scene":"scene","showlegend":true,"type":"scatter3d","x":[26.677974700927734],"y":[26.585086822509766],"z":[25.99358558654785]},{"hovertemplate":"index=poorly<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"poorly","marker":{"color":"#19d3f3","symbol":"circle"},"mode":"markers","name":"poorly","scene":"scene","showlegend":true,"type":"scatter3d","x":[28.502588272094727],"y":[20.552406311035156],"z":[28.495634078979492]},{"hovertemplate":"index=lacked<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"lacked","marker":{"color":"#FF6692","symbol":"circle"},"mode":"markers","name":"lacked","scene":"scene","showlegend":true,"type":"scatter3d","x":[42.15123748779297],"y":[32.93186569213867],"z":[4.2073974609375]},{"hovertemplate":"index=watchable<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"watchable","marker":{"color":"#B6E880","symbol":"circle"},"mode":"markers","name":"watchable","scene":"scene","showlegend":true,"type":"scatter3d","x":[26.316923141479492],"y":[32.72248077392578],"z":[22.835643768310547]},{"hovertemplate":"index=waste<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"waste","marker":{"color":"#FF97FF","symbol":"circle"},"mode":"markers","name":"waste","scene":"scene","showlegend":true,"type":"scatter3d","x":[29.833974838256836],"y":[18.298606872558594],"z":[31.289072036743164]},{"hovertemplate":"index=fails<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"fails","marker":{"color":"#FECB52","symbol":"circle"},"mode":"markers","name":"fails","scene":"scene","showlegend":true,"type":"scatter3d","x":[28.012056350708008],"y":[27.08220100402832],"z":[27.356109619140625]},{"hovertemplate":"index=perfect<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"perfect","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"perfect","scene":"scene","showlegend":true,"type":"scatter3d","x":[-72.8226089477539],"y":[-7.627018928527832],"z":[16.271942138671875]},{"hovertemplate":"index=thirst<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"thirst","marker":{"color":"#EF553B","symbol":"circle"},"mode":"markers","name":"thirst","scene":"scene","showlegend":true,"type":"scatter3d","x":[-78.60050964355469],"y":[4.203240394592285],"z":[-1.1438729763031006]},{"hovertemplate":"index=surprised<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"surprised","marker":{"color":"#00cc96","symbol":"circle"},"mode":"markers","name":"surprised","scene":"scene","showlegend":true,"type":"scatter3d","x":[-76.01590728759766],"y":[-6.96560525894165],"z":[13.468422889709473]},{"hovertemplate":"index=crossfire<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"crossfire","marker":{"color":"#ab63fa","symbol":"circle"},"mode":"markers","name":"crossfire","scene":"scene","showlegend":true,"type":"scatter3d","x":[-74.75045013427734],"y":[0.11839685589075089],"z":[1.493553638458252]},{"hovertemplate":"index=rewarded<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"rewarded","marker":{"color":"#FFA15A","symbol":"circle"},"mode":"markers","name":"rewarded","scene":"scene","showlegend":true,"type":"scatter3d","x":[-77.86294555664062],"y":[0.512961745262146],"z":[-1.0823631286621094]},{"hovertemplate":"index=thank<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"thank","marker":{"color":"#19d3f3","symbol":"circle"},"mode":"markers","name":"thank","scene":"scene","showlegend":true,"type":"scatter3d","x":[-68.7548599243164],"y":[-4.346294403076172],"z":[16.130958557128906]},{"hovertemplate":"index=detract<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"detract","marker":{"color":"#FF6692","symbol":"circle"},"mode":"markers","name":"detract","scene":"scene","showlegend":true,"type":"scatter3d","x":[-78.42449188232422],"y":[-2.3971142768859863],"z":[5.568938255310059]},{"hovertemplate":"index=masterpiece<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"masterpiece","marker":{"color":"#B6E880","symbol":"circle"},"mode":"markers","name":"masterpiece","scene":"scene","showlegend":true,"type":"scatter3d","x":[-75.1122055053711],"y":[-9.054410934448242],"z":[13.578644752502441]},{"hovertemplate":"index=scoop<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"scoop","marker":{"color":"#FF97FF","symbol":"circle"},"mode":"markers","name":"scoop","scene":"scene","showlegend":true,"type":"scatter3d","x":[-76.26014709472656],"y":[6.614981174468994],"z":[-0.8381622433662415]},{"hovertemplate":"index=pleasantly<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"pleasantly","marker":{"color":"#FECB52","symbol":"circle"},"mode":"markers","name":"pleasantly","scene":"scene","showlegend":true,"type":"scatter3d","x":[-75.5488052368164],"y":[-8.489334106445312],"z":[15.50845718383789]},{"hovertemplate":"index=mainstream<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"mainstream","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"mainstream","scene":"scene","showlegend":true,"type":"scatter3d","x":[-69.14849090576172],"y":[20.987062454223633],"z":[-10.310991287231445]},{"hovertemplate":"index=unique<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"unique","marker":{"color":"#EF553B","symbol":"circle"},"mode":"markers","name":"unique","scene":"scene","showlegend":true,"type":"scatter3d","x":[-77.35496520996094],"y":[4.262742519378662],"z":[-1.1106425523757935]},{"hovertemplate":"index=hilarious<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"hilarious","marker":{"color":"#00cc96","symbol":"circle"},"mode":"markers","name":"hilarious","scene":"scene","showlegend":true,"type":"scatter3d","x":[-75.57557678222656],"y":[-6.49660062789917],"z":[15.252713203430176]},{"hovertemplate":"index=ruby<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"ruby","marker":{"color":"#ab63fa","symbol":"circle"},"mode":"markers","name":"ruby","scene":"scene","showlegend":true,"type":"scatter3d","x":[-67.68760681152344],"y":[12.747835159301758],"z":[-1.951573133468628]},{"hovertemplate":"index=flawless<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"flawless","marker":{"color":"#FFA15A","symbol":"circle"},"mode":"markers","name":"flawless","scene":"scene","showlegend":true,"type":"scatter3d","x":[-76.2109146118164],"y":[-6.930458068847656],"z":[10.793371200561523]},{"hovertemplate":"index=jackass<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"jackass","marker":{"color":"#19d3f3","symbol":"circle"},"mode":"markers","name":"jackass","scene":"scene","showlegend":true,"type":"scatter3d","x":[-62.785369873046875],"y":[20.78851318359375],"z":[-8.821208953857422]},{"hovertemplate":"index=anyways<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"anyways","marker":{"color":"#FF6692","symbol":"circle"},"mode":"markers","name":"anyways","scene":"scene","showlegend":true,"type":"scatter3d","x":[-76.62700653076172],"y":[0.36110442876815796],"z":[1.1176433563232422]},{"hovertemplate":"index=samuel<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"samuel","marker":{"color":"#B6E880","symbol":"circle"},"mode":"markers","name":"samuel","scene":"scene","showlegend":true,"type":"scatter3d","x":[-75.64381408691406],"y":[8.16041374206543],"z":[-5.275892734527588]},{"hovertemplate":"index=simplicity<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"simplicity","marker":{"color":"#FF97FF","symbol":"circle"},"mode":"markers","name":"simplicity","scene":"scene","showlegend":true,"type":"scatter3d","x":[-42.42349624633789],"y":[5.878757953643799],"z":[5.093257427215576]},{"hovertemplate":"index=pleasant<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"pleasant","marker":{"color":"#FECB52","symbol":"circle"},"mode":"markers","name":"pleasant","scene":"scene","showlegend":true,"type":"scatter3d","x":[-76.36117553710938],"y":[-4.456920623779297],"z":[5.870057106018066]},{"hovertemplate":"index=journey<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"journey","marker":{"color":"#636efa","symbol":"circle"},"mode":"markers","name":"journey","scene":"scene","showlegend":true,"type":"scatter3d","x":[-77.10557556152344],"y":[9.452805519104004],"z":[-6.690478324890137]},{"hovertemplate":"index=worries<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"worries","marker":{"color":"#EF553B","symbol":"circle"},"mode":"markers","name":"worries","scene":"scene","showlegend":true,"type":"scatter3d","x":[-74.20516204833984],"y":[-7.504501819610596],"z":[12.889541625976562]},{"hovertemplate":"index=appreciate<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"appreciate","marker":{"color":"#00cc96","symbol":"circle"},"mode":"markers","name":"appreciate","scene":"scene","showlegend":true,"type":"scatter3d","x":[-77.21096801757812],"y":[4.2834577560424805],"z":[0.652169406414032]},{"hovertemplate":"index=wax<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"wax","marker":{"color":"#ab63fa","symbol":"circle"},"mode":"markers","name":"wax","scene":"scene","showlegend":true,"type":"scatter3d","x":[-76.62100982666016],"y":[4.8729567527771],"z":[-2.4956095218658447]},{"hovertemplate":"index=funniest<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"funniest","marker":{"color":"#FFA15A","symbol":"circle"},"mode":"markers","name":"funniest","scene":"scene","showlegend":true,"type":"scatter3d","x":[-74.65373992919922],"y":[-6.9896674156188965],"z":[17.0964298248291]},{"hovertemplate":"index=bourne<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"bourne","marker":{"color":"#19d3f3","symbol":"circle"},"mode":"markers","name":"bourne","scene":"scene","showlegend":true,"type":"scatter3d","x":[-77.23072052001953],"y":[-5.4342803955078125],"z":[8.964151382446289]},{"hovertemplate":"index=thanks<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"thanks","marker":{"color":"#FF6692","symbol":"circle"},"mode":"markers","name":"thanks","scene":"scene","showlegend":true,"type":"scatter3d","x":[-70.03789520263672],"y":[-3.5530619621276855],"z":[16.745853424072266]},{"hovertemplate":"index=edge<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"edge","marker":{"color":"#B6E880","symbol":"circle"},"mode":"markers","name":"edge","scene":"scene","showlegend":true,"type":"scatter3d","x":[-78.1590347290039],"y":[4.993024826049805],"z":[-3.280925989151001]},{"hovertemplate":"index=fortunately<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"fortunately","marker":{"color":"#FF97FF","symbol":"circle"},"mode":"markers","name":"fortunately","scene":"scene","showlegend":true,"type":"scatter3d","x":[-76.36383056640625],"y":[6.520237922668457],"z":[-2.631117343902588]},{"hovertemplate":"index=devito<br>X=%{x}<br>Y=%{y}<br>Z=%{z}<extra></extra>","legendgroup":"devito","marker":{"color":"#FECB52","symbol":"circle"},"mode":"markers","name":"devito","scene":"scene","showlegend":true,"type":"scatter3d","x":[-76.85140228271484],"y":[-6.721215724945068],"z":[7.851744174957275]}],"layout":{"legend":{"title":{"text":"index"},"tracegroupgap":0},"margin":{"t":60},"paper_bgcolor":"rgba(0, 0, 0, 0)","plot_bgcolor":"rgba(0, 0, 0, 0)","scene":{"domain":{"x":[0,1],"y":[0,1]},"xaxis":{"title":{"text":"X"}},"yaxis":{"title":{"text":"Y"}},"zaxis":{"title":{"text":"Z"}}},"template":{"data":{"bar":[{"error_x":{"color":"#f2f5fa"},"error_y":{"color":"#f2f5fa"},"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"baxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"heatmapgl":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmapgl"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"line":{"color":"#283442"}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"line":{"color":"#283442"}},"type":"scattergl"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#506784"},"line":{"color":"rgb(17,17,17)"}},"header":{"fill":{"color":"#2a3f5f"},"line":{"color":"rgb(17,17,17)"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#f2f5fa","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#f2f5fa"},"geo":{"bgcolor":"rgb(17,17,17)","lakecolor":"rgb(17,17,17)","landcolor":"rgb(17,17,17)","showlakes":true,"showland":true,"subunitcolor":"#506784"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"dark"},"paper_bgcolor":"rgb(17,17,17)","plot_bgcolor":"rgb(17,17,17)","polar":{"angularaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","radialaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"yaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"zaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"}},"shapedefaults":{"line":{"color":"#f2f5fa"}},"sliderdefaults":{"bgcolor":"#C8D4E3","bordercolor":"rgb(17,17,17)","borderwidth":1,"tickwidth":0},"ternary":{"aaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"baxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","caxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"title":{"x":0.05},"updatemenudefaults":{"bgcolor":"#506784","borderwidth":0},"xaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2}}},"title":{"text":"<b>The \"<i>devito</i>\" Similarity Cluster</b>"}}}},"metadata":{},"output_type":"display_data"}],"source":["\n","fig = px.scatter_3d(\n","    cluster, x=\"0\", y=\"1\", z=\"2\", color=cluster.index,\n","    labels={'0': 'X', '1': 'Y', '2': 'Z'}\n",")\n","fig.update_layout(template='plotly_dark',\n","                  title=f'<b>The \"<i>devito</i>\" Similarity Cluster</b>',\n","                  paper_bgcolor='rgba(0, 0, 0, 0)',\n","                  plot_bgcolor='rgba(0, 0, 0, 0)')\n","\n","fig.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In summary, the exploration of `word embeddings` and `embedding layers` can provide valuable insights into how language models are representing language. These insights can be used to improve the interpretability and explainability of such models, as well as to identify potential biases or limitations in them.\n","\n","There are other ways we can use `word embeddings` and `embedding layers` besides semantic similarity analysis. For example:\n","\n","- `Embedding layers` can be used in deep learning models to extract features from text inputs. By analyzing the weights associated with each feature, researchers can identify which words or phrases are most important for predicting a particular outcome. Something we have already done in our `integrated gradients` [notebook](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/55dec4959b0121ab7a441a4448c25dcf66d085da/ML%20Explainability/NLP%20Interpreter/integrated_gradients_in%20_keras_nlp.ipynb).\n","- Researchers can modify the embeddings of individual words or phrases and observe how the model's predictions change. This is one of the ways to adversarially attack language models. Something we explored in this [notebook](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/55dec4959b0121ab7a441a4448c25dcf66d085da/ML%20Adversarial/adversarial_text_attack.ipynb).\n","\n","Another good example of how this type of research can help improve our understanding of such models is the recent discovery of  \"_[Glitch Tokens](https://www.alignmentforum.org/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)_\" (like \"_SolidGoldMagikarp_\") that provoke unpredictable behavior in some of the most powerful language models ever created.\n","\n","---\n","\n","Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle)."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"}}},"nbformat":4,"nbformat_minor":2}
