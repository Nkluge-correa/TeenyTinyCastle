{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping with `google_play_scraper`\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n",
    "\n",
    "**Web scraping**, **web harvesting**, or **web data extraction** is [data scraping](https://en.wikipedia.org/wiki/Data_scraping \"Data scraping\") used for [extracting data](https://en.wikipedia.org/wiki/Data_extraction \"Data extraction\") from [websites](https://en.wikipedia.org/wiki/Website \"Website\"). Web scraping software may directly access the [World Wide Web](https://en.wikipedia.org/wiki/World_Wide_Web \"World Wide Web\") using the [Hypertext Transfer Protocol](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol \"Hypertext Transfer Protocol\") or a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a [bot](https://en.wikipedia.org/wiki/Internet_bot \"Internet bot\") or [web crawler](https://en.wikipedia.org/wiki/Web_crawler \"Web crawler\"). It is a form of copying in which specific data is gathered and copied from the web, typically into a central local [database](https://en.wikipedia.org/wiki/Database \"Database\") or spreadsheet, for later [retrieval](https://en.wikipedia.org/wiki/Data_retrieval \"Data retrieval\") or [analysis](https://en.wikipedia.org/wiki/Data_analysis \"Data analysis\") (and in this case, _training a ML model_).\n",
    "\n",
    "- In this notebook, we are using the `google_play_scraper` to create and automate our _crawler_.\n",
    "\n",
    "![scrappe](https://contraponto.digital/wp-content/uploads/2022/02/web-.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "from langdetect import detect\n",
    "import pandas as pd\n",
    "from google_play_scraper import Sort, reviews, app\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "# ----------------------------------------------------------------------------------------#\n",
    "#\n",
    "# Scrape apps\n",
    "#\n",
    "# ----------------------------------------------------------------------------------------#\n",
    "\n",
    "apps_ids = [\n",
    "    'br.com.brainweb.ifood',\n",
    "    'com.cerveceriamodelo.modelonow',\n",
    "    'com.mcdo.mcdonalds',\n",
    "    'habibs.alphacode.com.br',\n",
    "    'com.xiaojukeji.didi.brazil.customer',\n",
    "    'com.ubercab.eats',\n",
    "    'com.grability.rappi',\n",
    "    'burgerking.com.br.appandroid',\n",
    "    'com.instagram.android',\n",
    "    'com.tinder',\n",
    "    'com.facebook.katana',\n",
    "    'com.google.android.youtube',\n",
    "    'com.zhiliaoapp.musically',\n",
    "    'com.ubercab',\n",
    "    'com.twitter.android',\n",
    "    'org.telegram.messenger',\n",
    "]\n",
    "\n",
    "# ----------------------------------------------------------------------------------------#\n",
    "#\n",
    "# Raw Data\n",
    "#\n",
    "# ----------------------------------------------------------------------------------------#\n",
    "\n",
    "app_infos = []\n",
    "\n",
    "for ap in tqdm(apps_ids):\n",
    "    info = app(ap, lang='pt', country='br')\n",
    "    del info['comments']\n",
    "    app_infos.append(info)\n",
    "\n",
    "app_reviews = []\n",
    "\n",
    "for ap in tqdm(apps_ids):\n",
    "    for score in list(range(1, 6)):\n",
    "        for sort_order in [Sort.MOST_RELEVANT]:\n",
    "            rvs, _ = reviews(\n",
    "                ap,\n",
    "                lang='pt',\n",
    "                country='br',\n",
    "                sort=sort_order,\n",
    "                count=0 if score == 3 else 1000,\n",
    "                filter_score_with=score\n",
    "            )\n",
    "            for r in rvs:\n",
    "                r['sortOrder'] = 'most_relevant'\n",
    "                r['appId'] = ap\n",
    "            app_reviews.extend(rvs)\n",
    "\n",
    "data = pd.DataFrame(app_reviews)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------#\n",
    "#\n",
    "# Clean Data\n",
    "# Here are a couple of simple preprocessing rules to work with scrapped text.\n",
    "#\n",
    "# ----------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "data['lang'] = 0\n",
    "for i in range(0, len(data)):\n",
    "    data['content'][i] = clean(data['content'][i], no_emoji=True)\n",
    "    if len(data['content'][i]) <= 10:\n",
    "        data['lang'][i] = 'NaN'\n",
    "    else:\n",
    "        x = detect(data['content'][i])\n",
    "        data['lang'][i] = x\n",
    "\n",
    "# ----------------------------------------------------------------------------------------#\n",
    "#\n",
    "# Select only reviews in portuguese (some other languages may have found their way in...)\n",
    "#\n",
    "# ----------------------------------------------------------------------------------------#\n",
    "\n",
    "data = data[data['lang'] == 'pt']\n",
    "\n",
    "\n",
    "data = data.drop(['reviewId', 'userName', 'userImage',\n",
    "                  'thumbsUpCount', 'reviewCreatedVersion', 'at',\n",
    "                  'replyContent', 'repliedAt', 'sortOrder'], axis=1)\n",
    "\n",
    "\n",
    "def to_target(rating):\n",
    "    rating = int(rating)\n",
    "    if rating <= 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "data = data.rename(columns={'score': 'review'})\n",
    "data['score'] = data.review.apply(to_target)\n",
    "\n",
    "\n",
    "l = list(data['content'])\n",
    "new_l = []\n",
    "for review in l:\n",
    "    new_review = review.translate(str.maketrans('', '', string.punctuation))\n",
    "    new_review = new_review.lower()\n",
    "    new_review = unidecode.unidecode(new_review)\n",
    "    new_l.append(new_review)\n",
    "data['content'] = new_l\n",
    "\n",
    "data.to_excel('google_play_apps_review(pt).xlsx', index=None, header=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
