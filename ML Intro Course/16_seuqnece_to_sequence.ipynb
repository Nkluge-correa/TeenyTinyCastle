{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Translation with `RNNs` and `Transformer` Models \n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n",
    "\n",
    "**Language translation is the process of converting a text written in one language (the source language) into another language (the target language). In the context of machine learning (ML), language translation typically refers to the use of ML algorithms and models to automate this process.**\n",
    "\n",
    "**There are several key challenges in building an effective ML-based language translation system. One is the need to handle the variability and complexity of natural language, which can involve ambiguous or context-dependent meanings, idioms, and other factors that can make translation difficult.** \n",
    "\n",
    "**Another challenge is the need to handle the vast number of possible language pairs, as well as the need to handle multiple target languages for a given source language. Despite these challenges, ML-based language translation systems have made significant progress in recent years and are now used in a variety of applications, including the translation of websites, documents, and spoken language in real time.**\n",
    "\n",
    "**One of the biggest and most current advances in language modeling and _sequence-to-sequence_ machine translation was made possible by the invention of the `transformer` architecture.**\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" alt=\"drawing\" height=\"450\"/>\n",
    "\n",
    "**A `transformer` model is a type of neural network architecture first described in the 2017 paper \"_[Attention Is All You Need](https://arxiv.org/abs/1706.03762)_\" by Vaswani et al. Transformers were originally used for machine translation, but this adaptable architecture is now used in a wide range of fields and problems.**\n",
    "\n",
    "**In contrast to conventional convolutional neural networks (`CNNs`) or recurrent neural networks (`RNNs`), the `transformer` model uses `self-attention` mechanisms to dynamically weigh the input features and compute a weighted sum of the input features. In the context of natural language processing, where the meaning of a word can depend on the context of the other words in the sentence, this enables the model to capture long-range dependencies in the input data more effectively.**\n",
    "\n",
    "**This notebook will show sequence-to-sequence modeling on a machine translation task, which is exactly the task for which the `Transformer` was developed. For compassion, we'll develop a recurrent sequence model (`GRU`) and the full `Transformer` architecture.**\n",
    "\n",
    "**We’ll be working with an `English-to-Portuguese` translation dataset available [here](https://www.kaggle.com/datasets/nageshsingh/englishportuguese-translation). The downloaded text file contains one example per line: an English sentence, followed by a tab character, followed by the corresponding Portuguese sentence.**\n",
    "\n",
    "**Let us load our dataset and print the first 10 samples. We also need to prepare the samples of our target language with special tokes (`[start]` and `[end]`), to help the generative part of our models to understand _when a sentence starts and ends_.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go.', '[start] Vai. [end]'),\n",
       " ('Go.', '[start] Vá. [end]'),\n",
       " ('Hi.', '[start] Oi. [end]'),\n",
       " ('Run!', '[start] Corre! [end]'),\n",
       " ('Run!', '[start] Corra! [end]'),\n",
       " ('Run!', '[start] Corram! [end]'),\n",
       " ('Run.', '[start] Corre! [end]'),\n",
       " ('Run.', '[start] Corra! [end]'),\n",
       " ('Run.', '[start] Corram! [end]'),\n",
       " ('Who?', '[start] Quem? [end]')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with open(\"data/eng-por.txt\", encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "text_pairs = []\n",
    "for line in lines: \n",
    "    english, portuguese, _ = line.split(\"\\t\")\n",
    "    portuguese = \"[start] \" + portuguese + \" [end]\"\n",
    "    text_pairs.append((english, portuguese))\n",
    "display(text_pairs[: 10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's divide our datset them into the customary training, validation, and test sets after shuffling them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The learned vocabulary for each language needs to be separated into two separate TextVectorization layers, which need to be created now (English and Portuguese). The \"`[start]`\" and \"`[end]`\" tokens that we have inserted must also be kept. The characters \"`[ ]`\" would normally be stripped, but we want to leave them in.**\n",
    "\n",
    "**So that we can reuse the vocab learned, we will save them as `txt` files. In these files are all the words learned by the `TextVectorization` class.**\n",
    "\n",
    "**Both vocabularies will have a maximum of 20,000 words, and all sequences will be truncated after 20 tokens/words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "vocab_size = 20000\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length\n",
    "    )\n",
    "\n",
    "target_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    "    )\n",
    "\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_portuguese_texts = [pair[1] for pair in train_pairs]\n",
    "\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_portuguese_texts)\n",
    "\n",
    "portuguese_vocab = target_vectorization.get_vocabulary()\n",
    "\n",
    "with open(r'models/portuguese_vocabulary.txt', 'w', encoding='utf-8') as fp:\n",
    "    for word in portuguese_vocab:\n",
    "        fp.write(\"%s\\n\" % word)\n",
    "    fp.close()\n",
    "\n",
    "english_vocab = source_vectorization.get_vocabulary()\n",
    "\n",
    "with open(r'models/english_vocabulary.txt', 'w', encoding='utf-8') as fp:\n",
    "    for word in english_vocab:\n",
    "        fp.write(\"%s\\n\" % word)\n",
    "    fp.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we will create a `data pipeline` using our data. The dataset will be transformed into a \"`(inputs, target)`\" of `batch_size = 32` where the target is the Portuguese sentence offset by one step ahead (the next token in the input sequence). Meanwhile, inputs are a dictionary with two keys, \"encoder inputs\" (the English sentence) and \"decoder inputs\" (the Portuguese sentence).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "def format_dataset(eng, por):\n",
    "    eng = source_vectorization(eng)\n",
    "    por = target_vectorization(por)\n",
    "    return ({\n",
    "    \"english\": eng,\n",
    "    \"portuguese\": por[:, :-1],\n",
    "    }, por[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, por_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    por_texts = list(por_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, por_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(42).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can train our first model, which will be an `RNN`. `Recurrent neural networks` dominated `sequence-to-sequence` learning from 2015–2017 before being surpassed by the Transformer. They were the basis for many real-world machine-translation systems, like the [Google Translate circa 2017](https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation), which was powered by a stack of eight large `LSTM` layers.**\n",
    "\n",
    "**But for this notebook, we will use [`GRUs`](https://en.wikipedia.org/wiki/Gated_recurrent_unit) instead of an [`LSTM`](https://en.wikipedia.org/wiki/Long_short-term_memory). The GRU is like an `LSTM` with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate.**\n",
    "\n",
    "**If you wish, you can stack more `GRUs` layers in the encoder and decoder parts. As it is, this model has already more than 40M parameters. Stacking recurrent layers will slow training down. Given the _sequence processing nature_ of `RNNs`, there are limits two how much we can parallelize.**\n",
    "\n",
    "**We are given each learned word in our vocabulary an embedding dimension of 256. This is a dense vector that represents the \"_relationships of a word with another_.\" The dense internal layer of our network will have 1024 nodes per `GRU`.**\n",
    "\n",
    "> **Note: An `embedding` layer is a neural network layer that maps categorical variables, such as words in a vocabulary, into a continuous vector space. The goal of the embedding layer is to represent each word or categorical variable in a way that captures its relevant semantic and syntactic properties.**\n",
    "\n",
    "> **Note: The `latent dimension` of a language model refers to the underlying vector space that represents the learned semantic and syntactic features of a language. In other words, it is the number of hidden variables used to represent the language model's internal state.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " english (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " portuguese (InputLayer)        [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, None, 256)    5120000     ['english[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, None, 256)    5120000     ['portuguese[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 1024)        7876608     ['embedding_2[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " gru_3 (GRU)                    (None, None, 1024)   3938304     ['embedding_3[0][0]',            \n",
      "                                                                  'bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, None, 1024)   0           ['gru_3[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, None, 20000)  20500000    ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 42,554,912\n",
      "Trainable params: 42,554,912\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\") \n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source) \n",
    "encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x)\n",
    "\n",
    "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"portuguese\") \n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target) \n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source) \n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x) \n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)\n",
    "\n",
    "seq2seq_rnn.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "seq2seq_rnn.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now all that is left to do is to train our `seq2seq_rnn`. Be mindful that without a GPU, the training of this model could take many hours.**\n",
    "\n",
    "**If you wish to skip training and use the pre-trained model (`GRU_eng_por.keras`), you can download it in this [link](https://drive.google.com/uc?export=download&id=1q2S7xdKdsRCg5PSthvxNnUTuzo8gYUnx).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.10.1\n",
      "Eager mode:  True\n",
      "GPU is available\n",
      "Epoch 1/15\n",
      "3695/3695 [==============================] - 1138s 305ms/step - loss: 1.3829 - accuracy: 0.5019 - val_loss: 1.1052 - val_accuracy: 0.5909\n",
      "Epoch 2/15\n",
      "3695/3695 [==============================] - 1138s 308ms/step - loss: 1.1060 - accuracy: 0.5992 - val_loss: 1.0455 - val_accuracy: 0.6304\n",
      "Epoch 3/15\n",
      "3695/3695 [==============================] - 1137s 308ms/step - loss: 1.0689 - accuracy: 0.6287 - val_loss: 1.0486 - val_accuracy: 0.6432\n",
      "Epoch 4/15\n",
      "3695/3695 [==============================] - 1143s 309ms/step - loss: 1.0639 - accuracy: 0.6431 - val_loss: 1.0517 - val_accuracy: 0.6495\n",
      "Epoch 5/15\n",
      "3695/3695 [==============================] - 1137s 308ms/step - loss: 1.0624 - accuracy: 0.6504 - val_loss: 1.0569 - val_accuracy: 0.6523\n",
      "Epoch 6/15\n",
      "3695/3695 [==============================] - 1131s 306ms/step - loss: 1.0613 - accuracy: 0.6543 - val_loss: 1.0570 - val_accuracy: 0.6548\n",
      "Epoch 7/15\n",
      "3695/3695 [==============================] - 1131s 306ms/step - loss: 1.0615 - accuracy: 0.6555 - val_loss: 1.0601 - val_accuracy: 0.6539\n",
      "Epoch 8/15\n",
      "3695/3695 [==============================] - 1132s 306ms/step - loss: 1.0641 - accuracy: 0.6553 - val_loss: 1.0655 - val_accuracy: 0.6538\n",
      "Epoch 9/15\n",
      "3695/3695 [==============================] - 1135s 307ms/step - loss: 1.0679 - accuracy: 0.6545 - val_loss: 1.0674 - val_accuracy: 0.6534\n",
      "Epoch 10/15\n",
      "1385/3695 [==========>...................] - ETA: 11:01 - loss: 1.0720 - accuracy: 0.6525"
     ]
    }
   ],
   "source": [
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"models/GRU_eng_por.keras\",\n",
    "                                                save_best_only=True),\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                            patience=10,\n",
    "                                            verbose=1,\n",
    "                                            mode=\"auto\",\n",
    "                                            baseline=None,\n",
    "                                            restore_best_weights=True)]\n",
    "\n",
    "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy is a crude way to monitor validation-set performance during this kind of task. On average, this model predicts the next word in the Portuguese sentence correctly $65\\%$ of the time. However, in practice, next-token accuracy isn’t a great metric for machine translation models. During inference, you’re generating the target sentence from scratch, and you can’t rely on previously generated tokens (a.k.a. $100\\%$ correctness does not mean you have a good translator). In real-world machine translation applications, we would likely use “_BLEU scores_” to evaluate our models.**\n",
    "\n",
    "**`BLEU` (_bilingual evaluation understudy_) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine's output and that of a human: \"_the closer a machine translation is to a professional human translation, the better it is_\" – this is the central idea behind BLEU.**\n",
    "\n",
    "**To test our model, we can use sentences from our test set, or write some English sentences ourselves. Given an English sentence, we will feed the decoder block a `[start]` token, together with the encoded version of the English sentence, and the decoder will auto-regressively generate the next token, append to the decoded sequence (`[[start], [new token], ...]`), repeatedly, until the `[end]` token is generated, or the model reaches gets stomped by our maximum sentence length.** \n",
    "\n",
    "**Bellow, we import our trained model and vocabularies and test it on some sample text. The trained (or downloaded) model should be in the `models` folder.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:\n",
      "What is its name?\n",
      "Portuguese transalation:\n",
      "[start] qual é o nome [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "How old are you?\n",
      "Portuguese transalation:\n",
      "[start] quantos anos você tem [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "I know you know where Mary is.\n",
      "Portuguese transalation:\n",
      "[start] eu sei que você sabe onde maria está [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "We will show Tom.\n",
      "Portuguese transalation:\n",
      "[start] nós vamos tom [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "What do you all do?\n",
      "Portuguese transalation:\n",
      "[start] o que vocês faz [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "Don't do it!\n",
      "Portuguese transalation:\n",
      "[start] não faça isso [end]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "seq2seq_rnn = tf.keras.models.load_model(\"models/GRU_eng_por.keras\")\n",
    "\n",
    "with open('models/portuguese_vocabulary.txt', encoding='utf-8',  errors='backslashreplace') as fp:\n",
    "    portuguese_vocab = [line.strip() for line in fp]\n",
    "    fp.close()\n",
    "\n",
    "with open('models/english_vocabulary.txt', encoding='utf-8',  errors='backslashreplace') as fp:\n",
    "    english_vocab = [line.strip() for line in fp]\n",
    "    fp.close()\n",
    "\n",
    "\n",
    "target_vectorization = tf.keras.layers.TextVectorization(max_tokens=20000,\n",
    "                                        output_mode=\"int\",\n",
    "                                        output_sequence_length=21,\n",
    "                                        standardize=custom_standardization,\n",
    "                                        vocabulary=portuguese_vocab)\n",
    "\n",
    "source_vectorization = tf.keras.layers.TextVectorization(max_tokens=20000,\n",
    "                                        output_mode=\"int\",\n",
    "                                        output_sequence_length=20,\n",
    "                                        vocabulary=english_vocab)\n",
    "\n",
    "portuguese_index_lookup = dict(zip(range(len(portuguese_vocab)), portuguese_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence], verbose=0) \n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :]) \n",
    "        sampled_token = portuguese_index_lookup[sampled_token_index] \n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\": \n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "eng_sentences =[\"What is its name?\",\n",
    "                \"How old are you?\",\n",
    "                \"I know you know where Mary is.\",\n",
    "                \"We will show Tom.\",\n",
    "                \"What do you all do?\",\n",
    "                \"Don't do it!\"]\n",
    "\n",
    "for sentence in eng_sentences:\n",
    "    print(f\"English sentence:\\n{sentence}\")\n",
    "    print(f'Portuguese transalation:\\n{decode_sequence(sentence)}')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see, translations are far from perfect. To improve this model, we could:**\n",
    "\n",
    "- **Use a deep stack of recurrent layers for both the encoder and the decoder.** \n",
    "- **We could use an `LSTM` instead of a `GRU`.**\n",
    "\n",
    "**However, `RNNs` have limitations when it comes to sequence-to-sequence tasks and language modeling in general. For example,  due to their propensity to gradually forget the past, `RNNs` struggle to handle extremely long sequences. Consequently, `RNN`-based models are unable to retain long-term context, which is necessary for translating lengthy documents (which is one of the reasons why many online translation tools are limited by the size of the input).**\n",
    "\n",
    "**It is because of these limitations that the machine learning community has embraced the `Transformer` architecture for sequence-to-sequence problems. And sequence-to-sequence learning is the task where Transformers excel the most.**\n",
    "\n",
    "**Bellow, you can see the full sequence-to-sequence Transformer.**\n",
    "\n",
    "![transformer-block](https://d2l.ai/_images/transformer.svg)\n",
    "\n",
    "**_source_: [Dive into Deep Learning 11. Attention Mechanisms and Transformers - 11.7. The Transformer Architecture](https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html).**\n",
    "\n",
    "**The transformer model consists of two main components: the `encoder` and the `decoder`.**\n",
    "\n",
    "**The `encoder` is responsible for converting the input sequence into a set of internal representations that capture the relevant information in the input. It does this by applying a series of `self-attention` and `feedforward layers` to the input sequence. The `self-attention` layers allow the model to dynamically weigh the input features and compute a weighted sum of the features, which allows the model to capture long-range dependencies in the input data.**\n",
    "\n",
    "**The `decoder` is responsible for generating the output sequence based on the internal representations produced by the `encoder`. It does this by applying a series of `self-attention` and f`eedforward layers` to the internal representations and a set of additional \"_context_\" vectors that are computed from the `encoder` output. The `decoder` also uses an `attention` mechanism to weigh the context vectors and incorporate information from the `encoder` output into the generation of the output sequence (a type of residual connection shared by both `transformer` blocks).**\n",
    "\n",
    "**Together, the `encoder` and `decoder` allow the `transformer` model to effectively process and translate input sequences and capture long-range dependencies in the data.**\n",
    "\n",
    "**For an extremely _comprehensive_ and _ilustrated_ explanation of what is \"_attention_\" or how a \"_transformer works_\", we recommend the work of _Jay Alammar_:**\n",
    "\n",
    "- **[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/).**\n",
    "- **[The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/).**\n",
    "\n",
    "**But the general \"_gist_\" of attention and self attention is this:**\n",
    "\n",
    "**In a `transformer` model, `self-attention` is calculated using three input matrices: the query matrix ($Q$), the key matrix ($K$), and the value matrix ($V$). The `self-attention` mechanism computes a weighted sum of the value matrix ($V$) based on the similarity between the queries ($Q$) and the keys ($K$).**\n",
    "\n",
    "**The query, key, and value matrices are typically derived from the input data by applying a linear transformation to the input features. The dimensions of the matrices depend on the size of the input and the number of attention \"_heads_\" used by the model.**\n",
    "\n",
    "**To compute `self-attention`, the model first computes the dot product of the query and key matrices, which produces a matrix of dot products. The dot products are then divided by the square root of the dimensionality of the key matrix to ensure that the `self-attention` weights are well-behaved. Finally, the dot products are passed through a `softmax` function to produce a matrix of attention weights that sum to $1$.**\n",
    "\n",
    "**The attention weights are then used to weight the value matrix ($V$) and compute a weighted sum of the values, which is then used as the output of the `self-attention` mechanism. This process is repeated for each attention head and the output of all the attention heads is concatenated to produce the final `self-attention` output.**\n",
    "\n",
    "**The `self-attention` mechanism allows the model to dynamically weigh the input features and compute a weighted sum of the features based on the similarity between the queries and the keys. This allows the model to capture long-range dependencies in the input data and effectively process input sequences of variable length.**\n",
    "\n",
    "**Here is the equation for self-attention calculation in transformer models:**\n",
    "\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$\n",
    "\n",
    "**where** \n",
    "- **$Q$ is the query matrix.**\n",
    "- **$K$ is the key matrix.**\n",
    "- **$V$ is the value matrix.**\n",
    "- **$d_k$ is the dimensionality of the key matrix.**\n",
    "\n",
    "## Transformer Encoder\n",
    "\n",
    "**Fundamentally, the `encoder` part is a \"_text classification_\" machine. It's a very generic module that takes in a sequence and learns how to transform it into a more useful representation.**\n",
    "\n",
    "**We used the `decoder` part to make a toxicity classifier in this [notebook](xxx). And [Ai.ra](https://github.com/Nkluge-correa/Aira-EXPERT), AIRES closed domain chatbot, was built (in its last interaction), as a `decoder-only transformer`.**\n",
    "\n",
    "**The `decoder` block is made up of dense layers that process sequence tokens independently of one another, as well as an attention layer that examines the tokens as a group. You could rearrange the tokens in a sequence and still get the same pairwise attention scores.**\n",
    "\n",
    "**`Self-attention` is a set-processing mechanism that focuses on the relationships between pairs of sequence elements; unlike `RNNs`, it is not concerned with the order of the tokens.**\n",
    "\n",
    "**Below we implement the `encoder` block using the subclass features of the `Keras API`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding\n",
    "\n",
    "**As said before, `self-attention` is an order-agnostic technique. However, as done by [Vaswaniet al.](https://arxiv.org/abs/1706.03762), we can manually inject order information in the representations through _positional encoding_ (i.e., give the model access to word-order information).**\n",
    "\n",
    "**The original \"_Attention is all you need_\" paper added to the word embeddings a vector containing values in the range [-1, 1] that varied cyclically depending on the position (a cosine function).**\n",
    "\n",
    "**We’ll simply use the `tf.keras.layers.Embedding` layer to create a positional encoding parallel to the normal `Embedding` layer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder\n",
    "\n",
    "**The `transformer decoder` is the model's second half. It reads tokens $0....N$ in the target sequence and tries to predict token $N+1$, just like the `RNN decoder`. While doing so, it employs neural attention to determine which tokens in the encoded source sentence are most closely related to the target token it is currently attempting to predict.**\n",
    "\n",
    "**An extra bit of complication that the decoder brings is the idea of _causal padding_. Causal padding is critical for successfully training sequence-to-sequence Transformers.**\n",
    "\n",
    "**Causal padding is used to ensure that the network does not violate the temporal or causal order of the data. Unlike an `RNN`, which looks at its input one step at a time and thus only has access to steps $0....N$ to generate output step $N+1$, the `TransformerDecoder` can look at the entire target sequence at once (without casual padding). If it could use all of its input, it would simply learn to copy input step $N+1$. As a result, the model would achieve perfect training accuracy while learning nothing useful. And that is why we need causal padding. To \"_hide the future_\" from out model during training.**\n",
    "\n",
    "![causal-attention-padding](https://jalammar.github.io/images/gpt2/transformer-attention-mask.png)\n",
    "\n",
    "**_Source:_ [The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When we put all of the puzzle pieces together, we get the complete end-to-end Transformer. It simply combines the components we've created thus far: the `PositionalEmbedding` layers, the `TransformerEncoder`, and the `TransformerDecoder`. Similarly to our GRU model, we could stack these decoder and encoder blocks one on top of the other to create a more robust model.**\n",
    "\n",
    "**However, let us keep it small, and use just one `encoder-decoder` block. The original Transformer model consists of 6 stacked `encoder-decoder` blocks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " english (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " portuguese (InputLayer)        [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding (Position  (None, None, 256)   5125120     ['english[0][0]']                \n",
      " alEmbedding)                                                                                     \n",
      "                                                                                                  \n",
      " positional_embedding_1 (Positi  (None, None, 256)   5125120     ['portuguese[0][0]']             \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " transformer_encoder (Transform  (None, None, 256)   3155456     ['positional_embedding[0][0]']   \n",
      " erEncoder)                                                                                       \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, None, 256)   5259520     ['positional_embedding_1[0][0]', \n",
      " erDecoder)                                                       'transformer_encoder[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 256)    0           ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, None, 20000)  5140000     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,805,216\n",
      "Trainable params: 23,805,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "vocab_size = 20000\n",
    "sequence_length = 20\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"portuguese\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) \n",
    "\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "transformer.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see, this model is almost half the size of our RNN, and will be a little bit faster to train it! However, if you wish to skip training and use the pre-trained model (`transformer_eng_por.keras`), you can download it in this [link](https://drive.google.com/uc?export=download&id=1NTTDKZZ41Vw-3IVMB_jLstqlqPp77vqy).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.10.1\n",
      "Eager mode:  True\n",
      "GPU is available\n",
      "Epoch 1/30\n",
      "3695/3695 [==============================] - 602s 162ms/step - loss: 1.4840 - accuracy: 0.4742 - val_loss: 1.1986 - val_accuracy: 0.5541\n",
      "Epoch 2/30\n",
      "3695/3695 [==============================] - 603s 163ms/step - loss: 1.1662 - accuracy: 0.5791 - val_loss: 1.0980 - val_accuracy: 0.6068\n",
      "Epoch 3/30\n",
      "3695/3695 [==============================] - 603s 163ms/step - loss: 1.1069 - accuracy: 0.6144 - val_loss: 1.0850 - val_accuracy: 0.6247\n",
      "Epoch 4/30\n",
      "3695/3695 [==============================] - 602s 163ms/step - loss: 1.0882 - accuracy: 0.6326 - val_loss: 1.0726 - val_accuracy: 0.6365\n",
      "Epoch 5/30\n",
      "3695/3695 [==============================] - 604s 163ms/step - loss: 1.0702 - accuracy: 0.6451 - val_loss: 1.0600 - val_accuracy: 0.6459\n",
      "Epoch 6/30\n",
      "3695/3695 [==============================] - 604s 163ms/step - loss: 1.0511 - accuracy: 0.6548 - val_loss: 1.0492 - val_accuracy: 0.6525\n",
      "Epoch 7/30\n",
      "3695/3695 [==============================] - 602s 163ms/step - loss: 1.0323 - accuracy: 0.6628 - val_loss: 1.0347 - val_accuracy: 0.6566\n",
      "Epoch 8/30\n",
      "3695/3695 [==============================] - 600s 162ms/step - loss: 1.0153 - accuracy: 0.6695 - val_loss: 1.0251 - val_accuracy: 0.6615\n",
      "Epoch 9/30\n",
      "3695/3695 [==============================] - 599s 162ms/step - loss: 1.0006 - accuracy: 0.6757 - val_loss: 1.0163 - val_accuracy: 0.6668\n",
      "Epoch 10/30\n",
      "3695/3695 [==============================] - 589s 159ms/step - loss: 0.9874 - accuracy: 0.6804 - val_loss: 1.0158 - val_accuracy: 0.6668\n",
      "Epoch 11/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.9738 - accuracy: 0.6855 - val_loss: 1.0004 - val_accuracy: 0.6705\n",
      "Epoch 12/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.9617 - accuracy: 0.6895 - val_loss: 0.9934 - val_accuracy: 0.6728\n",
      "Epoch 13/30\n",
      "3695/3695 [==============================] - 596s 161ms/step - loss: 0.9515 - accuracy: 0.6930 - val_loss: 0.9934 - val_accuracy: 0.6739\n",
      "Epoch 14/30\n",
      "3695/3695 [==============================] - 596s 161ms/step - loss: 0.9410 - accuracy: 0.6970 - val_loss: 0.9915 - val_accuracy: 0.6737\n",
      "Epoch 15/30\n",
      "3695/3695 [==============================] - 596s 161ms/step - loss: 0.9312 - accuracy: 0.7004 - val_loss: 0.9863 - val_accuracy: 0.6772\n",
      "Epoch 16/30\n",
      "3695/3695 [==============================] - 596s 161ms/step - loss: 0.9223 - accuracy: 0.7036 - val_loss: 0.9804 - val_accuracy: 0.6763\n",
      "Epoch 17/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.9142 - accuracy: 0.7065 - val_loss: 0.9829 - val_accuracy: 0.6767\n",
      "Epoch 18/30\n",
      "3695/3695 [==============================] - 592s 160ms/step - loss: 0.9067 - accuracy: 0.7088 - val_loss: 0.9820 - val_accuracy: 0.6821\n",
      "Epoch 19/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.8995 - accuracy: 0.7116 - val_loss: 0.9875 - val_accuracy: 0.6759\n",
      "Epoch 20/30\n",
      "3695/3695 [==============================] - 596s 161ms/step - loss: 0.8927 - accuracy: 0.7138 - val_loss: 0.9806 - val_accuracy: 0.6800\n",
      "Epoch 21/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.8870 - accuracy: 0.7160 - val_loss: 0.9822 - val_accuracy: 0.6814\n",
      "Epoch 22/30\n",
      "3695/3695 [==============================] - 593s 160ms/step - loss: 0.8825 - accuracy: 0.7177 - val_loss: 0.9803 - val_accuracy: 0.6804\n",
      "Epoch 23/30\n",
      "3695/3695 [==============================] - 596s 161ms/step - loss: 0.8770 - accuracy: 0.7196 - val_loss: 0.9807 - val_accuracy: 0.6811\n",
      "Epoch 24/30\n",
      "3695/3695 [==============================] - 590s 160ms/step - loss: 0.8727 - accuracy: 0.7203 - val_loss: 0.9739 - val_accuracy: 0.6801\n",
      "Epoch 25/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.8708 - accuracy: 0.7217 - val_loss: 0.9853 - val_accuracy: 0.6768\n",
      "Epoch 26/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.8683 - accuracy: 0.7222 - val_loss: 0.9838 - val_accuracy: 0.6819\n",
      "Epoch 27/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.8669 - accuracy: 0.7229 - val_loss: 0.9804 - val_accuracy: 0.6824\n",
      "Epoch 28/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.8665 - accuracy: 0.7227 - val_loss: 0.9903 - val_accuracy: 0.6807\n",
      "Epoch 29/30\n",
      "3695/3695 [==============================] - 596s 161ms/step - loss: 0.8655 - accuracy: 0.7231 - val_loss: 0.9975 - val_accuracy: 0.6787\n",
      "Epoch 30/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.8630 - accuracy: 0.7237 - val_loss: 0.9812 - val_accuracy: 0.6790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x237fd64ae20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"models/transformer_eng_por.keras\",\n",
    "                                                save_best_only=True),\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                            patience=10,\n",
    "                                            verbose=1,\n",
    "                                            mode=\"auto\",\n",
    "                                            baseline=None,\n",
    "                                            restore_best_weights=True)]\n",
    "\n",
    "transformer.fit(train_ds, epochs=30, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bellow, we import our trained model and vocabularies and test it on some sample text. Since we are using Keras subclassess, we will need to load our models like this:**\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "transformer = keras.models.load_model(\"transformer-eng-pot.keras\", \n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder, \n",
    "        \"PositionalEmbedding\": PositionalEmbedding,\n",
    "        \"TransformerDecoder\": TransformerDecoder})\n",
    "                                                 \n",
    "```\n",
    "\n",
    "**The trained (or downloaded) model should be in the `models` folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:\n",
      "What is its name?\n",
      "Portuguese transalation:\n",
      "[start] qual é o nome dele [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "How old are you?\n",
      "Portuguese transalation:\n",
      "[start] quantos anos você tem [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "I know you know where Mary is.\n",
      "Portuguese transalation:\n",
      "[start] eu sei que você sabe onde mary está [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "We will show Tom.\n",
      "Portuguese transalation:\n",
      "[start] vamos ligar para o tom [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "What do you all do?\n",
      "Portuguese transalation:\n",
      "[start] o que vocês todos nós têm feito [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "Don't do it!\n",
      "Portuguese transalation:\n",
      "[start] não faça isso [end]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "import keras\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "transformer = keras.models.load_model(\"models/transformer_eng_por.keras\", \n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder, \n",
    "        \"PositionalEmbedding\": PositionalEmbedding,\n",
    "        \"TransformerDecoder\": TransformerDecoder})\n",
    "\n",
    "with open('models/portuguese_vocabulary.txt', encoding='utf-8', errors='backslashreplace') as fp:\n",
    "    portuguese_vocab = [line.strip() for line in fp]\n",
    "    fp.close()\n",
    "\n",
    "with open('models/english_vocabulary.txt', encoding='utf-8', errors='backslashreplace') as fp:\n",
    "    english_vocab = [line.strip() for line in fp]\n",
    "    fp.close()\n",
    "\n",
    "\n",
    "target_vectorization = tf.keras.layers.TextVectorization(max_tokens=20000,\n",
    "                                        output_mode=\"int\",\n",
    "                                        output_sequence_length=21,\n",
    "                                        standardize=custom_standardization,\n",
    "                                        vocabulary=portuguese_vocab)\n",
    "\n",
    "source_vectorization = tf.keras.layers.TextVectorization(max_tokens=20000,\n",
    "                                        output_mode=\"int\",\n",
    "                                        output_sequence_length=20,\n",
    "                                        vocabulary=english_vocab)\n",
    "\n",
    "portuguese_index_lookup = dict(zip(range(len(portuguese_vocab)), portuguese_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = portuguese_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "eng_sentences =[\"What is its name?\",\n",
    "                \"How old are you?\",\n",
    "                \"I know you know where Mary is.\",\n",
    "                \"We will show Tom.\",\n",
    "                \"What do you all do?\",\n",
    "                \"Don't do it!\"]\n",
    "\n",
    "for sentence in eng_sentences:\n",
    "    print(f\"English sentence:\\n{sentence}\")\n",
    "    print(f'Portuguese transalation:\\n{decode_sequence(sentence)}')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can now compare the output of both models, and the `transformer` (_being lighter and faster to train_) shows a better performance than the `RNN`. However, there is a problem with the translation of the first sentence.**\n",
    "\n",
    "**While the `RNN` translates `What is its name?` to `[start] o que é o seu nome [end]`, the `transformer` model makes a gender assumption, even though the source sentence wasn’t gendered (`[start] qual é o nome dele [end]`). Errors like these are not uncommon in NLP, algorithmic bias being one of the great problems associated with the use of language models in real applications.**\n",
    "\n",
    "**In AI literature, we call this a \"_hallucination_.\" When a machine learning model \"_hallucinates_\" something, it means that the model is producing output that is not based on the input data and is not representative of the underlying distribution of the data. This can occur when the model is overfitting to the training data and has learned to memorize specific examples rather than generalizing to new data.**\n",
    "\n",
    "**Hallucinations can occur in a variety of forms, depending on the type of ML model and the task it is trained to perform. For example, in the case of image classification, a model might hallucinate an object or pattern that is not present in the input image. In natural language processing, a model might generate nonsensical or unrelated text.**\n",
    "\n",
    "**Hallucinations can be a sign of overfitting and can indicate that the model is not generalizing well to new data. To prevent hallucinations, it is important to use techniques such as regularization and early stopping to prevent overfitting and to train models with curated, high-quality, datasets to prevent the positioning of the model with unwanted biases.**\n",
    "\n",
    "---\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
