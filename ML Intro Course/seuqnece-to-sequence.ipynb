{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language translation with `transformer` models \n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n",
    "\n",
    "**Language translation is the process of converting a text written in one language (the source language) into another language (the target language). In the context of machine learning (ML), language translation typically refers to the use of ML algorithms and models to automate this process.**\n",
    "\n",
    "**There are several key challenges in building an effective ML-based language translation system. One is the need to handle the variability and complexity of natural language, which can involve ambiguous or context-dependent meanings, idioms, and other factors that can make translation difficult. Another challenge is the need to handle the vast number of possible language pairs, as well as the need to handle multiple target languages for a given source language. Despite these challenges, ML-based language translation systems have made significant progress in recent years and are now used in a variety of applications, including the translation of websites, documents, and spoken language in real time.**\n",
    "\n",
    "**One of the biggest and most current advances in language modeling and _sequence-to-sequence_ machine translation was made possible by the invention of the `transformer` architecture.**\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" alt=\"drawing\" height=\"450\"/>\n",
    "\n",
    "**A transformer model is a type of neural network architecture first described in the 2017 paper \"_[Attention Is All You Need](https://arxiv.org/abs/1706.03762)_\" by Vaswani et al. Transformers were originally used for machine translation, but this adaptable architecture is now used in a wide range of fields and problems.**\n",
    "\n",
    "**In contrast to conventional convolutional neural networks (CNNs) or recurrent neural networks (RNNs), the transformer model uses self-attention mechanisms to dynamically weigh the input features and compute a weighted sum of the input features. In the context of natural language processing, where the meaning of a word can depend on the context of the other words in the sentence, this enables the model to capture long-range dependencies in the input data more effectively.**\n",
    "\n",
    "**This notebook will show sequence-to-sequence modeling on a machine translation task, which is exactly the task for which the Transformer was developed. For compassion, we'll develop a recurrent sequence model (GRU) and the full Transformer architecture.**\n",
    "\n",
    "**We’ll be working with an English-to-Portuguese translation dataset available [here](https://www.kaggle.com/datasets/nageshsingh/englishportuguese-translation). The downloaded text file contains one example per line: an English sentence, followed by a tab character, followed by the corresponding Portuguese sentence.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Go.', '[start] Vai. [end]'),\n",
       " ('Go.', '[start] Vá. [end]'),\n",
       " ('Hi.', '[start] Oi. [end]'),\n",
       " ('Run!', '[start] Corre! [end]'),\n",
       " ('Run!', '[start] Corra! [end]'),\n",
       " ('Run!', '[start] Corram! [end]'),\n",
       " ('Run.', '[start] Corre! [end]'),\n",
       " ('Run.', '[start] Corra! [end]'),\n",
       " ('Run.', '[start] Corram! [end]'),\n",
       " ('Who?', '[start] Quem? [end]'),\n",
       " ('Who?', '[start] Que [end]'),\n",
       " ('Wow!', '[start] Uau! [end]'),\n",
       " ('Wow!', '[start] Nossa! [end]'),\n",
       " ('Wow!', '[start] Wow! [end]'),\n",
       " ('Fire!', '[start] Fogo! [end]'),\n",
       " ('Fire!', '[start] Incêndio! [end]'),\n",
       " ('Fire!', '[start] Chama! [end]'),\n",
       " ('Help!', '[start] Ajuda! [end]'),\n",
       " ('Help!', '[start] Socorro! [end]'),\n",
       " ('Jump!', '[start] Pule! [end]'),\n",
       " ('Jump!', '[start] Pula! [end]'),\n",
       " ('Jump.', '[start] Pulem. [end]'),\n",
       " ('Jump.', '[start] Pule. [end]'),\n",
       " ('Jump.', '[start] Pulam. [end]'),\n",
       " ('Stop!', '[start] Pare! [end]'),\n",
       " ('Stop!', '[start] Parem! [end]'),\n",
       " ('Stop!', '[start] Parada! [end]'),\n",
       " ('Stop!', '[start] Ponto! [end]'),\n",
       " ('Stop!', '[start] Pará! [end]'),\n",
       " ('Wait!', '[start] Espere! [end]'),\n",
       " ('Wait!', '[start] Aguarde! [end]'),\n",
       " ('Wait.', '[start] Espere! [end]'),\n",
       " ('Wait.', '[start] Esperem. [end]'),\n",
       " ('Go on.', '[start] Siga em frente. [end]'),\n",
       " ('Go on.', '[start] Vá! [end]'),\n",
       " ('Go on.', '[start] Continue. [end]'),\n",
       " ('Go on.', '[start] Siga adiante. [end]'),\n",
       " ('Hello!', '[start] Oi. [end]'),\n",
       " ('Hello!', '[start] Alô. [end]'),\n",
       " ('Hello!', '[start] Olá! [end]'),\n",
       " ('Hello!', '[start] Alô! [end]'),\n",
       " ('I ran.', '[start] Eu corri. [end]'),\n",
       " ('I see.', '[start] Eu sei. [end]'),\n",
       " ('I see.', '[start] Eu entendo. [end]'),\n",
       " ('I see.', '[start] Estou vendo. [end]'),\n",
       " ('I see.', '[start] Eu vejo. [end]'),\n",
       " ('I see.', '[start] Eu assisto. [end]'),\n",
       " ('I try.', '[start] Eu tento. [end]'),\n",
       " ('I try.', '[start] Tento. [end]'),\n",
       " ('I won!', '[start] Ganhei! [end]'),\n",
       " ('I won.', '[start] Eu venci. [end]'),\n",
       " ('Oh no!', '[start] Ah não! [end]'),\n",
       " ('Relax.', '[start] Relaxe! [end]'),\n",
       " ('Relax.', '[start] Relaxa! [end]'),\n",
       " ('Relax.', '[start] Relaxe. [end]'),\n",
       " ('Shoot!', '[start] Tiro! [end]'),\n",
       " ('Smile.', '[start] Sorria. [end]'),\n",
       " ('Smile.', '[start] Sorriam. [end]'),\n",
       " ('Attack!', '[start] Atacar! [end]'),\n",
       " ('Attack!', '[start] Ataquem! [end]'),\n",
       " ('Attack!', '[start] Ataque! [end]'),\n",
       " ('Cheers!', '[start] Saúde! [end]'),\n",
       " ('Freeze!', '[start] Parado! [end]'),\n",
       " ('Get up.', '[start] Levante-se! [end]'),\n",
       " ('Get up.', '[start] Levantem-se! [end]'),\n",
       " ('Get up.', '[start] Levanta-te. [end]'),\n",
       " ('Get up.', '[start] Levante-se. [end]'),\n",
       " ('Get up.', '[start] Levanta-te! [end]'),\n",
       " ('Go now.', '[start] Vá agora. [end]'),\n",
       " ('Got it!', '[start] Entendi. [end]'),\n",
       " ('Got it!', '[start] Eu entendi! [end]'),\n",
       " ('Got it!', '[start] Saquei! [end]'),\n",
       " ('Got it?', '[start] Entendeu? [end]'),\n",
       " ('He ran.', '[start] Ele correu. [end]'),\n",
       " ('He ran.', '[start] Ele corria. [end]'),\n",
       " ('Hop in.', '[start] Sobe aí! [end]'),\n",
       " ('Hop in.', '[start] Entra aí! [end]'),\n",
       " ('Hug me.', '[start] Me abrace. [end]'),\n",
       " ('I fell.', '[start] Eu caí. [end]'),\n",
       " ('I knit.', '[start] Eu tricoto. [end]'),\n",
       " ('I know.', '[start] Eu sei. [end]'),\n",
       " ('I know.', '[start] Sei. [end]'),\n",
       " ('I left.', '[start] Eu saí. [end]'),\n",
       " ('I lied.', '[start] Eu menti. [end]'),\n",
       " ('I paid.', '[start] Eu paguei. [end]'),\n",
       " ('I quit.', '[start] Eu me demito. [end]'),\n",
       " ('I work.', '[start] Eu estou trabalhando. [end]'),\n",
       " (\"I'm OK.\", '[start] Estou bem. [end]'),\n",
       " (\"I'm OK.\", '[start] Eu vou bem. [end]'),\n",
       " (\"I'm up.\", '[start] Estou acordado. [end]'),\n",
       " ('Listen.', '[start] Escute! [end]'),\n",
       " ('Listen.', '[start] Ouça-me! [end]'),\n",
       " ('Listen.', '[start] Escuta! [end]'),\n",
       " ('Listen.', '[start] Escutem! [end]'),\n",
       " ('Listen.', '[start] Ouça isso! [end]'),\n",
       " ('Listen.', '[start] Escutem-me! [end]'),\n",
       " ('Listen.', '[start] Escute. [end]'),\n",
       " ('Listen.', '[start] Escuta. [end]'),\n",
       " ('Listen.', '[start] Escutem. [end]'),\n",
       " ('Listen.', '[start] Escutai. [end]'),\n",
       " ('No way!', '[start] De jeito nenhum! [end]'),\n",
       " ('No way!', '[start] Impossível! [end]'),\n",
       " ('No way!', '[start] De maneira alguma! [end]'),\n",
       " ('No way!', '[start] De modo algum! [end]'),\n",
       " ('No way!', '[start] Sem chance! [end]'),\n",
       " ('Really?', '[start] Sério? [end]'),\n",
       " ('Really?', '[start] É mesmo? [end]'),\n",
       " ('Really?', '[start] Mesmo? [end]'),\n",
       " ('Really?', '[start] É sério? [end]'),\n",
       " ('Thanks.', '[start] Obrigado! [end]'),\n",
       " ('Thanks.', '[start] Obrigada! [end]'),\n",
       " ('Thanks.', '[start] Obrigado. [end]'),\n",
       " ('Thanks.', '[start] Obrigada. [end]'),\n",
       " ('Try it.', '[start] Tenta-o. [end]'),\n",
       " ('Try it.', '[start] Prove-o. [end]'),\n",
       " ('Try it.', '[start] Prove-a. [end]'),\n",
       " ('We hid.', '[start] Nós escondemos. [end]'),\n",
       " ('We hid.', '[start] Nós nos escondemos. [end]'),\n",
       " ('We ran.', '[start] Nós corremos. [end]'),\n",
       " ('We ran.', '[start] Corremos. [end]'),\n",
       " ('We try.', '[start] Tentamos. [end]'),\n",
       " ('We try.', '[start] Nós tentamos. [end]'),\n",
       " ('We won.', '[start] Vencemos. [end]'),\n",
       " ('We won.', '[start] Nós vencemos. [end]'),\n",
       " ('Why me?', '[start] Por que eu? [end]'),\n",
       " ('Ask Tom.', '[start] Pergunte a Tom. [end]'),\n",
       " ('Ask Tom.', '[start] Peça a Tom. [end]'),\n",
       " ('Ask Tom.', '[start] Pergunta para o Tom. [end]'),\n",
       " ('Ask Tom.', '[start] Peça para o Tom. [end]'),\n",
       " ('Ask Tom.', '[start] Peça ao Tom. [end]'),\n",
       " ('Ask Tom.', '[start] Perguntem ao Tom. [end]'),\n",
       " ('Be calm.', '[start] Fique calmo. [end]'),\n",
       " ('Be calm.', '[start] Tenha calma. [end]'),\n",
       " ('Be cool.', '[start] Seja legal. [end]'),\n",
       " ('Be fair.', '[start] Sede justos! [end]'),\n",
       " ('Be fair.', '[start] Sede justas! [end]'),\n",
       " ('Be fair.', '[start] Sejam justos! [end]'),\n",
       " ('Be fair.', '[start] Sejam justas! [end]'),\n",
       " ('Be fair.', '[start] Seja justo! [end]'),\n",
       " ('Be fair.', '[start] Seja justa! [end]'),\n",
       " ('Be fair.', '[start] Sê justo! [end]'),\n",
       " ('Be fair.', '[start] Sê justa! [end]'),\n",
       " ('Be good.', '[start] Seja bom. [end]'),\n",
       " ('Be good.', '[start] Seja boa. [end]'),\n",
       " ('Be good.', '[start] Sejam bons. [end]'),\n",
       " ('Be good.', '[start] Sejam boas. [end]'),\n",
       " ('Be good.', '[start] Sê bom. [end]'),\n",
       " ('Be good.', '[start] Sê boa. [end]'),\n",
       " ('Be good.', '[start] Sede bons. [end]'),\n",
       " ('Be good.', '[start] Sede boas. [end]'),\n",
       " ('Be kind.', '[start] Seja gentil. [end]'),\n",
       " ('Be kind.', '[start] Sejam gentis. [end]'),\n",
       " ('Be kind.', '[start] Sede gentis. [end]'),\n",
       " ('Be kind.', '[start] Sê gentil. [end]'),\n",
       " ('Be nice.', '[start] Seja legal. [end]'),\n",
       " ('Be nice.', '[start] Sejam legais. [end]'),\n",
       " ('Beat it.', '[start] Se manda! [end]'),\n",
       " ('Beat it.', '[start] Vaza! [end]'),\n",
       " ('Call me.', '[start] Me liga. [end]'),\n",
       " ('Call us.', '[start] Ligue para a gente. [end]'),\n",
       " ('Come in.', '[start] Entre. [end]'),\n",
       " ('Come in.', '[start] Entre! [end]'),\n",
       " ('Come in.', '[start] Entrem! [end]'),\n",
       " ('Come in.', '[start] Entra. [end]'),\n",
       " ('Come on!', '[start] Vamos! [end]'),\n",
       " ('Come on!', '[start] Venha! [end]'),\n",
       " ('Come on!', '[start] Qual é! [end]'),\n",
       " ('Come on.', '[start] Vamos! [end]'),\n",
       " ('Drop it!', '[start] Solte-o! [end]'),\n",
       " ('Drop it!', '[start] Largue isso. [end]'),\n",
       " ('Get Tom.', '[start] Pegue Tom. [end]'),\n",
       " ('Get out!', '[start] Saia! [end]'),\n",
       " ('Get out!', '[start] Saia daqui! [end]'),\n",
       " ('Get out!', '[start] Fora! [end]'),\n",
       " ('Get out!', '[start] Sai. [end]'),\n",
       " ('Get out!', '[start] Saiam! [end]'),\n",
       " ('Get out.', '[start] Saia. [end]'),\n",
       " ('Get out.', '[start] Sai. [end]'),\n",
       " ('Go away!', '[start] Afaste-se! [end]'),\n",
       " ('Go away.', '[start] Cai fora! [end]'),\n",
       " ('Go away.', '[start] Vá embora! [end]'),\n",
       " ('Go away.', '[start] Fora! [end]'),\n",
       " ('Go away.', '[start] Se manda! [end]'),\n",
       " ('Go away.', '[start] Vá embora. [end]'),\n",
       " ('Go away.', '[start] Afaste-se! [end]'),\n",
       " ('Go away.', '[start] Cai fora daqui! [end]'),\n",
       " ('Go home.', '[start] Vão para casa. [end]'),\n",
       " ('Go home.', '[start] Vá para casa. [end]'),\n",
       " ('Goodbye!', '[start] Tchau! [end]'),\n",
       " ('Goodbye!', '[start] Até mais! [end]'),\n",
       " ('Goodbye!', '[start] Até a vista! [end]'),\n",
       " ('Goodbye!', '[start] Até logo! [end]'),\n",
       " ('Goodbye!', '[start] Até mais ver! [end]'),\n",
       " ('Goodbye!', '[start] Até o rever! [end]'),\n",
       " ('Hang on!', '[start] Aguarde. [end]'),\n",
       " ('Hang on!', '[start] Aguardem. [end]'),\n",
       " ('Hang on.', '[start] Aguarde. [end]'),\n",
       " ('Hang on.', '[start] Aguardem. [end]'),\n",
       " ('He came.', '[start] Ele veio. [end]'),\n",
       " ('He runs.', '[start] Ele corre. [end]'),\n",
       " ('Help me!', '[start] Me ajuda! [end]'),\n",
       " ('Help me!', '[start] Ajude-me! [end]'),\n",
       " ('Help me!', '[start] Ajudem-me! [end]'),\n",
       " ('Help me.', '[start] Me ajude. [end]'),\n",
       " ('Help me.', '[start] Ajude-me. [end]'),\n",
       " ('Help me.', '[start] Ajuda-me. [end]'),\n",
       " ('Help me.', '[start] Ajudem-me. [end]'),\n",
       " ('Help me.', '[start] Me ajuda! [end]'),\n",
       " ('Help me.', '[start] Ajude-me! [end]'),\n",
       " ('Help me.', '[start] Ajudem-me! [end]'),\n",
       " ('Help us.', '[start] Ajudem-nos. [end]'),\n",
       " ('Help us.', '[start] Nos ajude. [end]'),\n",
       " ('Help us.', '[start] Ajude a gente. [end]'),\n",
       " ('Help us.', '[start] Ajude-nos. [end]'),\n",
       " ('Hi, Tom.', '[start] Oi, Tom. [end]'),\n",
       " ('Hit Tom.', '[start] Acerta o Tom. [end]'),\n",
       " ('Hold it!', '[start] Segure isso! [end]'),\n",
       " ('Hold it!', '[start] Segura isso! [end]'),\n",
       " ('Hold on.', '[start] Espere. [end]'),\n",
       " ('Hold on.', '[start] Espera. [end]'),\n",
       " ('Hug Tom.', '[start] Abrace o Tom. [end]'),\n",
       " ('Hug Tom.', '[start] Abracem o Tom. [end]'),\n",
       " ('I agree.', '[start] Eu concordo. [end]'),\n",
       " ('I agree.', '[start] Estou de acordo. [end]'),\n",
       " ('I agree.', '[start] Eu estou de acordo. [end]'),\n",
       " ('I cried.', '[start] Eu chorei. [end]'),\n",
       " ('I drove.', '[start] Eu dirigi. [end]'),\n",
       " ('I moved.', '[start] Mudei-me. [end]'),\n",
       " ('I smoke.', '[start] Eu fumo. [end]'),\n",
       " ('I snore.', '[start] Eu ronco. [end]'),\n",
       " ('I stink.', '[start] Eu cheiro mal. [end]'),\n",
       " ('I tried.', '[start] Eu tentei. [end]'),\n",
       " ('I tried.', '[start] Tentei. [end]'),\n",
       " ('I waved.', '[start] Eu acenei. [end]'),\n",
       " (\"I'll go.\", '[start] Eu vou. [end]'),\n",
       " (\"I'll go.\", '[start] Eu vou! [end]'),\n",
       " (\"I'm fat.\", '[start] Estou gordo. [end]'),\n",
       " (\"I'm fat.\", '[start] Sou gordo. [end]'),\n",
       " (\"I'm fat.\", '[start] Eu sou gorda. [end]'),\n",
       " (\"I'm fat.\", '[start] Eu estou gorda. [end]'),\n",
       " (\"I'm fit.\", '[start] Eu estou em forma. [end]'),\n",
       " (\"I'm fit.\", '[start] Estou em forma. [end]'),\n",
       " (\"I'm hot.\", '[start] Tenho calor. [end]'),\n",
       " (\"I'm hot.\", '[start] Estou com calor. [end]'),\n",
       " (\"I'm mad.\", '[start] Estou bravo. [end]'),\n",
       " (\"I'm mad.\", '[start] Estou louco. [end]'),\n",
       " (\"I'm old.\", '[start] Eu sou velho. [end]'),\n",
       " (\"I'm old.\", '[start] Eu sou velha. [end]'),\n",
       " (\"I'm old.\", '[start] Eu estou velho. [end]'),\n",
       " (\"I'm old.\", '[start] Eu estou velha. [end]'),\n",
       " (\"I'm sad.\", '[start] Estou triste. [end]'),\n",
       " (\"I'm sad.\", '[start] Eu sou triste. [end]'),\n",
       " (\"I'm sad.\", '[start] Eu estou triste. [end]'),\n",
       " (\"I'm shy.\", '[start] Eu sou tímido. [end]'),\n",
       " (\"I'm wet.\", '[start] Estou molhado. [end]'),\n",
       " (\"I'm wet.\", '[start] Estou molhada. [end]'),\n",
       " (\"It's OK.\", '[start] Está tudo bem. [end]'),\n",
       " (\"It's OK.\", '[start] Tudo bem. [end]'),\n",
       " (\"It's OK.\", '[start] Está bem. [end]'),\n",
       " (\"It's OK.\", '[start] Tá bom. [end]'),\n",
       " (\"It's me!\", '[start] Sou eu. [end]'),\n",
       " (\"It's me.\", '[start] Sou eu. [end]'),\n",
       " ('Join us.', '[start] Junte-se. [end]'),\n",
       " ('Join us.', '[start] Junte-se a nós! [end]'),\n",
       " ('Keep it.', '[start] Fique com ele. [end]'),\n",
       " ('Keep it.', '[start] Fica com ele. [end]'),\n",
       " ('Keep it.', '[start] Fiquem com ele. [end]'),\n",
       " ('Keep it.', '[start] Fique com ela. [end]'),\n",
       " ('Keep it.', '[start] Fica com ela. [end]'),\n",
       " ('Keep it.', '[start] Fiquem com ela. [end]'),\n",
       " ('Kiss me.', '[start] Me beija. [end]'),\n",
       " ('Kiss me.', '[start] Dê-me um beijo. [end]'),\n",
       " ('Kiss me.', '[start] Beije-me. [end]'),\n",
       " ('Kiss me.', '[start] Me beije. [end]'),\n",
       " ('Look up.', '[start] Olhe para cima! [end]'),\n",
       " ('Look up.', '[start] Olha pra cima! [end]'),\n",
       " ('Look up.', '[start] Olhe pra cima! [end]'),\n",
       " ('Look up.', '[start] Olha para cima! [end]'),\n",
       " ('Me, too.', '[start] Eu também. [end]'),\n",
       " ('Open up.', '[start] Abra. [end]'),\n",
       " ('Perfect!', '[start] Perfeito! [end]'),\n",
       " ('See you!', '[start] Nos vemos! [end]'),\n",
       " ('See you.', '[start] Até mais. [end]'),\n",
       " ('See you.', '[start] Até logo. [end]'),\n",
       " ('See you.', '[start] Nos vemos. [end]'),\n",
       " ('See you.', '[start] A gente se vê. [end]'),\n",
       " ('Show me.', '[start] Mostre-me. [end]'),\n",
       " ('Show me.', '[start] Me mostre. [end]'),\n",
       " ('Show me.', '[start] Me mostra. [end]'),\n",
       " ('Shut up!', '[start] Cale a boca! [end]'),\n",
       " ('Shut up!', '[start] Cala a boca! [end]'),\n",
       " ('Shut up!', '[start] Cala-te! [end]'),\n",
       " ('Shut up!', '[start] Cale-se! [end]'),\n",
       " ('Shut up!', '[start] Calem a boca! [end]'),\n",
       " ('Shut up!', '[start] Calem-se! [end]'),\n",
       " ('Stop it.', '[start] Pare com isso! [end]'),\n",
       " ('Stop it.', '[start] Pare. [end]'),\n",
       " ('Stop it.', '[start] Para com isso. [end]'),\n",
       " ('Take it.', '[start] Pegue-o! [end]'),\n",
       " ('Take it.', '[start] Peguem-no! [end]'),\n",
       " ('Take it.', '[start] Pegue-o. [end]'),\n",
       " ('Take it.', '[start] Pega-o. [end]'),\n",
       " ('Take it.', '[start] Peguem-no. [end]'),\n",
       " ('Take it.', '[start] Pegue-a. [end]'),\n",
       " ('Take it.', '[start] Pega-a. [end]'),\n",
       " ('Take it.', '[start] Peguem-na. [end]'),\n",
       " ('Tell me.', '[start] Diga-me. [end]'),\n",
       " ('Tell me.', '[start] Digam-me. [end]'),\n",
       " ('Tell me.', '[start] Diz-me. [end]'),\n",
       " ('Tom ate.', '[start] O Tom comeu. [end]'),\n",
       " ('Tom ate.', '[start] Tom comeu. [end]'),\n",
       " ('Tom hid.', '[start] Tom se escondeu. [end]'),\n",
       " ('Tom ran.', '[start] Tom correu. [end]'),\n",
       " ('Tom ran.', '[start] O Tom correu. [end]'),\n",
       " ('Tom won.', '[start] Tom ganhou. [end]'),\n",
       " ('Wait up.', '[start] Esperem. [end]'),\n",
       " ('Wait up.', '[start] Espere. [end]'),\n",
       " ('Wake up!', '[start] Acorda! [end]'),\n",
       " ('Wake up!', '[start] Acorde! [end]'),\n",
       " ('Wake up!', '[start] Acordem! [end]'),\n",
       " ('Wake up.', '[start] Acorde. [end]'),\n",
       " ('Wake up.', '[start] Acordem. [end]'),\n",
       " ('Wash up.', '[start] Lava-te. [end]'),\n",
       " ('Wash up.', '[start] Lave-se. [end]'),\n",
       " ('Wash up.', '[start] Lavem-se. [end]'),\n",
       " ('Wash up.', '[start] Lave as mãos. [end]'),\n",
       " ('Wash up.', '[start] Lave o rosto. [end]'),\n",
       " ('We care.', '[start] Nós nos importamos. [end]'),\n",
       " ('We know.', '[start] Nós sabemos. [end]'),\n",
       " ('We lost.', '[start] Perdemos. [end]'),\n",
       " ('We lost.', '[start] Nós perdemos. [end]'),\n",
       " ('Welcome.', '[start] Seja bem-vindo! [end]'),\n",
       " ('Welcome.', '[start] Bem-vindo! [end]'),\n",
       " ('Welcome.', '[start] Seja bem-vinda! [end]'),\n",
       " ('Welcome.', '[start] Bem-vinda! [end]'),\n",
       " ('Who ate?', '[start] Quem comeu? [end]'),\n",
       " ('Who ran?', '[start] Quem correu? [end]'),\n",
       " ('Who won?', '[start] Quem ganhou? [end]'),\n",
       " ('Why not?', '[start] Por que não? [end]'),\n",
       " ('You run.', '[start] Você corre. [end]'),\n",
       " ('You run.', '[start] Corra. [end]'),\n",
       " ('You won.', '[start] Você ganhou. [end]'),\n",
       " ('You won.', '[start] Você venceu. [end]'),\n",
       " ('You won.', '[start] Vocês venceram. [end]'),\n",
       " ('Am I fat?', '[start] Estou gordo? [end]'),\n",
       " ('Am I fat?', '[start] Estou gorda? [end]'),\n",
       " ('Ask them.', '[start] Pergunte para eles. [end]'),\n",
       " ('Ask them.', '[start] Pergunte para elas. [end]'),\n",
       " ('Back off!', '[start] Para trás! [end]'),\n",
       " ('Back off.', '[start] Recue. [end]'),\n",
       " ('Back off.', '[start] Recuem. [end]'),\n",
       " ('Back off.', '[start] Recua. [end]'),\n",
       " ('Be brave.', '[start] Seja corajoso. [end]'),\n",
       " ('Be brave.', '[start] Seja valente. [end]'),\n",
       " ('Be brief.', '[start] Seja breve. [end]'),\n",
       " ('Be quick.', '[start] Seja rápido. [end]'),\n",
       " ('Be quiet.', '[start] Fique quieta. [end]'),\n",
       " ('Be quiet.', '[start] Fique quieto. [end]'),\n",
       " ('Be still.', '[start] Acalme-se. [end]'),\n",
       " ('Beats me.', '[start] Não faço ideia. [end]'),\n",
       " ('Call Tom.', '[start] Ligue para o Tom. [end]'),\n",
       " ('Can I go?', '[start] Posso ir? [end]'),\n",
       " ('Cheer up!', '[start] Ânimo! [end]'),\n",
       " ('Cool off!', '[start] Acalme-se. [end]'),\n",
       " ('Cool off!', '[start] Acalme-se! [end]'),\n",
       " ('Cuff him.', '[start] Algeme-o. [end]'),\n",
       " (\"Don't go.\", '[start] Não vá. [end]'),\n",
       " ('Drive on.', '[start] Dirija! [end]'),\n",
       " ('Find Tom.', '[start] Encontre o Tom. [end]'),\n",
       " ('Find Tom.', '[start] Encontrem o Tom. [end]'),\n",
       " ('Fix this.', '[start] Conserta isto. [end]'),\n",
       " ('Get away!', '[start] Cai fora! [end]'),\n",
       " ('Get away!', '[start] Fora! [end]'),\n",
       " ('Get away!', '[start] Se manda! [end]'),\n",
       " ('Get down!', '[start] Abaixe-se. [end]'),\n",
       " ('Get down!', '[start] Desça! [end]'),\n",
       " ('Get down.', '[start] Escreva. [end]'),\n",
       " ('Get down.', '[start] Abaixe-se. [end]'),\n",
       " ('Get down.', '[start] Deitem-se. [end]'),\n",
       " ('Get lost!', '[start] Se manda! [end]'),\n",
       " ('Get lost!', '[start] Vai-te embora! [end]'),\n",
       " ('Get lost!', '[start] Vá-se embora! [end]'),\n",
       " ('Get lost.', '[start] Se manda! [end]'),\n",
       " ('Get real!', '[start] Acorda! [end]'),\n",
       " ('Get real!', '[start] Cai na real! [end]'),\n",
       " ('Get real.', '[start] Acorda! [end]'),\n",
       " ('Get real.', '[start] Cai na real! [end]'),\n",
       " ('Go ahead!', '[start] Vá em frente! [end]'),\n",
       " ('Go ahead!', '[start] Continue! [end]'),\n",
       " ('Go ahead.', '[start] Continua. [end]'),\n",
       " ('Go ahead.', '[start] Vá em frente! [end]'),\n",
       " ('Go ahead.', '[start] Continue! [end]'),\n",
       " ('Go ahead.', '[start] Continuem. [end]'),\n",
       " ('Good job!', '[start] Bom trabalho! [end]'),\n",
       " ('Grab Tom.', '[start] Pegue o Tom. [end]'),\n",
       " ('Grab him.', '[start] Agarre-o. [end]'),\n",
       " ('Grab him.', '[start] Agarrem-no. [end]'),\n",
       " ('Have fun.', '[start] Diverte-te! [end]'),\n",
       " ('Have fun.', '[start] Divirtam-se! [end]'),\n",
       " ('Have fun.', '[start] Divirta-se! [end]'),\n",
       " ('He spoke.', '[start] Ele falou. [end]'),\n",
       " ('He tries.', '[start] Ele tenta. [end]'),\n",
       " (\"He's Tom.\", '[start] Ele é o Tom. [end]'),\n",
       " (\"He's Tom.\", '[start] Ele é Tom. [end]'),\n",
       " (\"He's wet.\", '[start] Ele está molhado. [end]'),\n",
       " ('Help Tom.', '[start] Ajude o Tom. [end]'),\n",
       " ('Help Tom.', '[start] Ajuda o Tom. [end]'),\n",
       " ('Help Tom.', '[start] Ajudem o Tom. [end]'),\n",
       " ('Hi, guys.', '[start] Olá, pessoal. [end]'),\n",
       " ('How cute!', '[start] Que bonitinho! [end]'),\n",
       " ('How cute!', '[start] Que fofinho! [end]'),\n",
       " ('How cute!', '[start] Que fofinha! [end]'),\n",
       " ('How cute!', '[start] Que bonitinha! [end]'),\n",
       " ('How deep?', '[start] Quão fundo? [end]'),\n",
       " ('How deep?', '[start] Qual a profundidade? [end]'),\n",
       " ('How deep?', '[start] Qual é a profundidade? [end]'),\n",
       " ('How nice!', '[start] Que legal! [end]'),\n",
       " ('How rude!', '[start] Que bruto! [end]'),\n",
       " ('How rude!', '[start] Que rude! [end]'),\n",
       " ('Hurry up.', '[start] Apresse-se. [end]'),\n",
       " ('Hurry up.', '[start] Depressa! [end]'),\n",
       " ('Hurry up.', '[start] Apressa-te! [end]'),\n",
       " ('Hurry up.', '[start] Se apressa! [end]'),\n",
       " ('I burped.', '[start] Eu arrotei. [end]'),\n",
       " ('I can go.', '[start] Eu posso ir. [end]'),\n",
       " ('I can go.', '[start] Eu consigo ir. [end]'),\n",
       " ('I danced.', '[start] Eu dancei. [end]'),\n",
       " ('I failed.', '[start] Eu fracassei. [end]'),\n",
       " ('I forgot.', '[start] Eu me esqueci. [end]'),\n",
       " ('I forgot.', '[start] Eu esqueci. [end]'),\n",
       " ('I forgot.', '[start] Esqueci. [end]'),\n",
       " ('I forgot.', '[start] Me esqueci. [end]'),\n",
       " ('I get it.', '[start] Eu entendo. [end]'),\n",
       " ('I got it.', '[start] Entendi. [end]'),\n",
       " ('I got it.', '[start] Eu entendi. [end]'),\n",
       " ('I got up.', '[start] Eu me levantei. [end]'),\n",
       " ('I got up.', '[start] Me levantei. [end]'),\n",
       " ('I helped.', '[start] Eu ajudei. [end]'),\n",
       " ('I jumped.', '[start] Eu pulei. [end]'),\n",
       " ('I looked.', '[start] Eu olhei. [end]'),\n",
       " ('I nodded.', '[start] Balancei a cabeça. [end]'),\n",
       " ('I nodded.', '[start] Eu balancei a cabeça. [end]'),\n",
       " ('I phoned.', '[start] Eu telefonei. [end]'),\n",
       " ('I prayed.', '[start] Eu orei. [end]'),\n",
       " ('I prayed.', '[start] Eu rezei. [end]'),\n",
       " ('I resign.', '[start] Eu me demito. [end]'),\n",
       " ('I rested.', '[start] Eu descansei. [end]'),\n",
       " ('I saw it.', '[start] Eu o vi. [end]'),\n",
       " ('I smiled.', '[start] Eu sorri. [end]'),\n",
       " ('I smiled.', '[start] Dei um sorriso. [end]'),\n",
       " ('I stayed.', '[start] Eu fiquei. [end]'),\n",
       " ('I talked.', '[start] Eu falei. [end]'),\n",
       " ('I waited.', '[start] Eu esperei. [end]'),\n",
       " ('I winked.', '[start] Eu pisquei. [end]'),\n",
       " ('I yawned.', '[start] Eu bocejei. [end]'),\n",
       " (\"I'll pay.\", '[start] Eu pagarei. [end]'),\n",
       " (\"I'll try.\", '[start] Tentarei. [end]'),\n",
       " (\"I'll win.\", '[start] Vou vencer. [end]'),\n",
       " (\"I'm back.\", '[start] Voltei. [end]'),\n",
       " (\"I'm bald.\", '[start] Sou careca. [end]'),\n",
       " (\"I'm bald.\", '[start] Eu sou careca. [end]'),\n",
       " (\"I'm bald.\", '[start] Estou careca. [end]'),\n",
       " (\"I'm busy.\", '[start] Estou ocupado. [end]'),\n",
       " (\"I'm busy.\", '[start] Estou ocupada. [end]'),\n",
       " (\"I'm cold.\", '[start] Estou com frio. [end]'),\n",
       " (\"I'm cool.\", '[start] Sou legal. [end]'),\n",
       " (\"I'm deaf.\", '[start] Sou surdo. [end]'),\n",
       " (\"I'm done.\", '[start] Já terminei. [end]'),\n",
       " (\"I'm fair.\", '[start] Eu sou justo. [end]'),\n",
       " (\"I'm fast.\", '[start] Sou rápido. [end]'),\n",
       " (\"I'm fine.\", '[start] Estou bem. [end]'),\n",
       " (\"I'm fine.\", '[start] Eu vou bem. [end]'),\n",
       " (\"I'm fine.\", '[start] Eu estou bem. [end]'),\n",
       " (\"I'm free.\", '[start] Eu sou livre. [end]'),\n",
       " (\"I'm free.\", '[start] Estou livre. [end]'),\n",
       " (\"I'm full.\", '[start] Estou cheio. [end]'),\n",
       " (\"I'm full.\", '[start] Estou cheia. [end]'),\n",
       " (\"I'm glad.\", '[start] Estou contente. [end]'),\n",
       " (\"I'm good.\", '[start] Eu vou bem. [end]'),\n",
       " (\"I'm here.\", '[start] Estou aqui. [end]'),\n",
       " (\"I'm here.\", '[start] Eu estou aqui. [end]'),\n",
       " (\"I'm home.\", '[start] Cheguei. [end]'),\n",
       " (\"I'm hurt.\", '[start] Estou machucado. [end]'),\n",
       " (\"I'm hurt.\", '[start] Estou ferido. [end]'),\n",
       " (\"I'm lost.\", '[start] Estou perdido. [end]'),\n",
       " (\"I'm next.\", '[start] Sou o próximo. [end]'),\n",
       " (\"I'm next.\", '[start] Sou a próxima. [end]'),\n",
       " (\"I'm okay.\", '[start] Estou bem. [end]'),\n",
       " (\"I'm okay.\", '[start] Eu estou bem. [end]'),\n",
       " (\"I'm poor.\", '[start] Sou pobre. [end]'),\n",
       " (\"I'm rich.\", '[start] Eu sou rico. [end]'),\n",
       " (\"I'm rich.\", '[start] Eu sou rica. [end]'),\n",
       " (\"I'm safe.\", '[start] Eu estou seguro. [end]'),\n",
       " (\"I'm sick.\", '[start] Estou doente. [end]'),\n",
       " (\"I'm sick.\", '[start] Estou doente! [end]'),\n",
       " (\"I'm sick.\", '[start] Estou enfermo. [end]'),\n",
       " (\"I'm sick.\", '[start] Estou enferma. [end]'),\n",
       " (\"I'm sick.\", '[start] Eu estou enfermo. [end]'),\n",
       " (\"I'm sure.\", '[start] Tenho certeza. [end]'),\n",
       " (\"I'm tall.\", '[start] Eu sou alto. [end]'),\n",
       " (\"I'm thin.\", '[start] Eu sou magro. [end]'),\n",
       " (\"I'm thin.\", '[start] Sou magro. [end]'),\n",
       " (\"I'm tidy.\", '[start] Estou arrumado. [end]'),\n",
       " (\"I'm tidy.\", '[start] Eu estou arrumado. [end]'),\n",
       " (\"I'm ugly.\", '[start] Sou feio. [end]'),\n",
       " (\"I'm ugly.\", '[start] Estou feio. [end]'),\n",
       " (\"I'm ugly.\", '[start] Sou feia. [end]'),\n",
       " (\"I'm ugly.\", '[start] Estou feia. [end]'),\n",
       " (\"I'm well.\", '[start] Estou bem. [end]'),\n",
       " (\"I'm wise.\", '[start] Eu sou sábio. [end]'),\n",
       " (\"I've won.\", '[start] Eu venci. [end]'),\n",
       " ('It helps.', '[start] Isso ajuda. [end]'),\n",
       " ('It hurts.', '[start] Isso machuca. [end]'),\n",
       " ('It works.', '[start] Funciona. [end]'),\n",
       " (\"It's Tom.\", '[start] É o Tom. [end]'),\n",
       " (\"It's fun.\", '[start] É divertido. [end]'),\n",
       " (\"It's his.\", '[start] É dele. [end]'),\n",
       " (\"It's hot.\", '[start] Está quente. [end]'),\n",
       " (\"It's new.\", '[start] Isso é novo. [end]'),\n",
       " (\"It's new.\", '[start] Isso está novo. [end]'),\n",
       " (\"It's old.\", '[start] É velho. [end]'),\n",
       " (\"It's red.\", '[start] É vermelho. [end]'),\n",
       " (\"It's red.\", '[start] Isso é vermelho. [end]'),\n",
       " (\"It's red.\", '[start] É vermelha. [end]'),\n",
       " (\"It's sad.\", '[start] É triste. [end]'),\n",
       " ('Keep out!', '[start] Não entre. [end]'),\n",
       " ('Keep out!', '[start] Afaste-se! [end]'),\n",
       " ('Keep out.', '[start] Não entre. [end]'),\n",
       " ('Keep out.', '[start] Afaste-se! [end]'),\n",
       " ('Kiss Tom.', '[start] Beije Tom. [end]'),\n",
       " ('Leave it.', '[start] Deixe para lá. [end]'),\n",
       " ('Leave me.', '[start] Me deixe. [end]'),\n",
       " ('Leave me.', '[start] Deixe-me. [end]'),\n",
       " ('Leave me.', '[start] Me deixa. [end]'),\n",
       " ('Leave us.', '[start] Deixe-nos. [end]'),\n",
       " ('Leave us.', '[start] Nos deixe. [end]'),\n",
       " (\"Let's go!\", '[start] Vamos! [end]'),\n",
       " (\"Let's go.\", '[start] Vamos! [end]'),\n",
       " ('Look out!', '[start] Preste atenção! [end]'),\n",
       " ('Look out!', '[start] Atenção! [end]'),\n",
       " ('Look out!', '[start] Cuidado! [end]'),\n",
       " ('Marry me.', '[start] Case comigo. [end]'),\n",
       " ('May I go?', '[start] Posso ir? [end]'),\n",
       " ('Save Tom.', '[start] Salve o Tom. [end]'),\n",
       " ('Save Tom.', '[start] Salve a Tom. [end]'),\n",
       " ('She came.', '[start] Ela veio. [end]'),\n",
       " ('She died.', '[start] Ela morreu. [end]'),\n",
       " ('She runs.', '[start] Ela corre. [end]'),\n",
       " ('Sit down!', '[start] Assente-se! [end]'),\n",
       " ('Sit down.', '[start] Sente-se. [end]'),\n",
       " ('Sit here.', '[start] Senta aqui. [end]'),\n",
       " ('Sit here.', '[start] Sente-se aqui. [end]'),\n",
       " ('Speak up!', '[start] Fale mais alto! [end]'),\n",
       " ('Speak up!', '[start] Falem mais alto! [end]'),\n",
       " ('Speak up!', '[start] Fala mais alto! [end]'),\n",
       " ('Stand by.', '[start] Aguarde. [end]'),\n",
       " ('Stand by.', '[start] Aguardem. [end]'),\n",
       " ('Stand up!', '[start] Levante-se! [end]'),\n",
       " ('Stand up!', '[start] De pé! [end]'),\n",
       " ('Stand up.', '[start] Levante-se. [end]'),\n",
       " ('Stand up.', '[start] Levantem-se. [end]'),\n",
       " ('Stop Tom.', '[start] Pare o Tom. [end]'),\n",
       " ('Stop Tom.', '[start] Parem o Tom. [end]'),\n",
       " ('Stop Tom.', '[start] Impeça o Tom. [end]'),\n",
       " ('Stop Tom.', '[start] Impeçam o Tom. [end]'),\n",
       " ('Stop Tom.', '[start] Interrompa o Tom. [end]'),\n",
       " ('Stop Tom.', '[start] Detenham Tom. [end]'),\n",
       " ('Take Tom.', '[start] Leve o Tom. [end]'),\n",
       " ('Take Tom.', '[start] Leva o Tom. [end]'),\n",
       " ('Tell Tom.', '[start] Conte ao Tom. [end]'),\n",
       " ('Tell Tom.', '[start] Contem ao Tom. [end]'),\n",
       " ('Tell Tom.', '[start] Conta para o Tom. [end]'),\n",
       " ('Terrific!', '[start] Fantástico! [end]'),\n",
       " ('Terrific!', '[start] Genial! [end]'),\n",
       " ('Terrific!', '[start] Excelente! [end]'),\n",
       " ('Terrific!', '[start] Magnífico! [end]'),\n",
       " ('They won.', '[start] Eles ganharam. [end]'),\n",
       " ('They won.', '[start] Elas ganharam. [end]'),\n",
       " ('They won.', '[start] Elas venceram. [end]'),\n",
       " ('They won.', '[start] Eles venceram. [end]'),\n",
       " ('Tom came.', '[start] Tom veio. [end]'),\n",
       " ('Tom died.', '[start] Tom morreu. [end]'),\n",
       " ('Tom died.', '[start] O Tom morreu. [end]'),\n",
       " ('Tom fell.', '[start] Tom caiu. [end]'),\n",
       " ('Tom knew.', '[start] Tom sabia. [end]'),\n",
       " ('Tom lied.', '[start] Tom mentiu. [end]'),\n",
       " ('Tom lies.', '[start] Tom mente. [end]'),\n",
       " ('Tom lost.', '[start] O Tom perdeu. [end]'),\n",
       " ('Tom paid.', '[start] O Tom pagou. [end]'),\n",
       " ('Tom quit.', '[start] Tom desistiu. [end]'),\n",
       " ('Tom swam.', '[start] O Tom nadou. [end]'),\n",
       " ('Tom swam.', '[start] Tom nadou. [end]'),\n",
       " ('Too late.', '[start] Tarde demais. [end]'),\n",
       " ('Trust me.', '[start] Confie em mim. [end]'),\n",
       " ('Try hard.', '[start] Esforce-se. [end]'),\n",
       " ('Try hard.', '[start] Se esforça. [end]'),\n",
       " ('Try some.', '[start] Experimenta. [end]'),\n",
       " ('Try some.', '[start] Experimente. [end]'),\n",
       " ('Try some.', '[start] Experimentem. [end]'),\n",
       " ('Try this.', '[start] Experimenta isto. [end]'),\n",
       " ('Try this.', '[start] Experimente isto. [end]'),\n",
       " ('Try this.', '[start] Experimentem isto. [end]'),\n",
       " ('Use this.', '[start] Use isso. [end]'),\n",
       " ('Warn Tom.', '[start] Avise ao Tom. [end]'),\n",
       " ('Warn Tom.', '[start] Avisem ao Tom. [end]'),\n",
       " ('Warn Tom.', '[start] Alerte o Tom. [end]'),\n",
       " ('Warn Tom.', '[start] Fale para o Tom. [end]'),\n",
       " ('Watch me.', '[start] Observe-me. [end]'),\n",
       " ('We agree.', '[start] Concordamos. [end]'),\n",
       " ('We agree.', '[start] Nós concordamos. [end]'),\n",
       " ('We stood.', '[start] Nós ficamos. [end]'),\n",
       " ('We stood.', '[start] Ficamos. [end]'),\n",
       " ('We tried.', '[start] Nós tentamos. [end]'),\n",
       " (\"We'll go.\", '[start] Nós iremos. [end]'),\n",
       " (\"We're #1.\", '[start] Nós somos o número 1. [end]'),\n",
       " (\"We're #1.\", '[start] Somos o número 1. [end]'),\n",
       " (\"We're OK.\", '[start] Estamos bem. [end]'),\n",
       " ('What for?', '[start] Para quê? [end]'),\n",
       " ('What fun!', '[start] Que divertido! [end]'),\n",
       " ('Who am I?', '[start] Quem sou eu? [end]'),\n",
       " ('Who came?', '[start] Quem veio? [end]'),\n",
       " ('Who died?', '[start] Quem morreu? [end]'),\n",
       " ('Who fell?', '[start] Quem caiu? [end]'),\n",
       " ('Who quit?', '[start] Quem desistiu? [end]'),\n",
       " ('Who swam?', '[start] Quem nadou? [end]'),\n",
       " ('Write me.', '[start] Escreva-me. [end]'),\n",
       " ('After you.', '[start] Você primeiro. [end]'),\n",
       " ('Aim. Fire!', '[start] Preparar, apontar, fogo! [end]'),\n",
       " ('Am I late?', '[start] Estou atrasado? [end]'),\n",
       " ('Answer me.', '[start] Responda-me. [end]'),\n",
       " ('Answer me.', '[start] Respondam-me. [end]'),\n",
       " ('Answer me.', '[start] Responda-me! [end]'),\n",
       " ('Answer me.', '[start] Respondam-me! [end]'),\n",
       " ('Answer me.', '[start] Me responde! [end]'),\n",
       " ('Answer me.', '[start] Me responda! [end]'),\n",
       " ('Be honest.', '[start] Seja sincera. [end]'),\n",
       " ('Be seated.', '[start] Sentai-vos. [end]'),\n",
       " ('Birds fly.', '[start] Pássaros voam. [end]'),\n",
       " ('Birds fly.', '[start] Os pássaros voam. [end]'),\n",
       " ('Bless you.', '[start] Saúde. [end]'),\n",
       " ('Call home!', '[start] Ligue para casa! [end]'),\n",
       " ('Calm down!', '[start] Acalme-se. [end]'),\n",
       " ('Calm down!', '[start] Acalme-se! [end]'),\n",
       " ('Calm down.', '[start] Acalme-se. [end]'),\n",
       " ('Calm down.', '[start] Acalma-te. [end]'),\n",
       " ('Calm down.', '[start] Acalmem-se. [end]'),\n",
       " ('Calm down.', '[start] Te acalma. [end]'),\n",
       " ('Calm down.', '[start] Aquieta o facho. [end]'),\n",
       " ('Can I eat?', '[start] Posso comer? [end]'),\n",
       " ('Can we go?', '[start] Podemos ir? [end]'),\n",
       " ('Catch Tom.', '[start] Pegue o Tom. [end]'),\n",
       " ('Catch Tom.', '[start] Peguem o Tom. [end]'),\n",
       " ('Catch him.', '[start] Pegue-o. [end]'),\n",
       " ('Catch him.', '[start] Peguem-no. [end]'),\n",
       " ('Come back.', '[start] Volte. [end]'),\n",
       " ('Come back.', '[start] Voltem. [end]'),\n",
       " ('Come here.', '[start] Venha aqui. [end]'),\n",
       " ('Come here.', '[start] Vem cá. [end]'),\n",
       " ('Come here.', '[start] Ande cá. [end]'),\n",
       " ('Come home.', '[start] Venha para casa. [end]'),\n",
       " ('Come over!', '[start] Vem! [end]'),\n",
       " ('Come over!', '[start] Venham! [end]'),\n",
       " ('Come over!', '[start] Venha. [end]'),\n",
       " ('Come over.', '[start] Venha. [end]'),\n",
       " ('Come soon.', '[start] Venha logo. [end]'),\n",
       " ('Cool down.', '[start] Acalme-se. [end]'),\n",
       " ('Cool down.', '[start] Acalme-se! [end]'),\n",
       " ('Cool down.', '[start] Acalmem-se. [end]'),\n",
       " ('Cool down.', '[start] Se acalme. [end]'),\n",
       " ('Did I win?', '[start] Eu ganhei? [end]'),\n",
       " ('Dogs bark.', '[start] Cães latem. [end]'),\n",
       " ('Dogs bark.', '[start] Cachorros latem. [end]'),\n",
       " (\"Don't ask.\", '[start] Não faça perguntas. [end]'),\n",
       " (\"Don't ask.\", '[start] Não pergunte! [end]'),\n",
       " (\"Don't ask.\", '[start] Não pergunte. [end]'),\n",
       " (\"Don't cry.\", '[start] Não chore. [end]'),\n",
       " (\"Don't cry.\", '[start] Não chorem. [end]'),\n",
       " (\"Don't die.\", '[start] Não morra! [end]'),\n",
       " (\"Don't die.\", '[start] Não morram! [end]'),\n",
       " (\"Don't lie.\", '[start] Não minta. [end]'),\n",
       " (\"Don't run.\", '[start] Não corra. [end]'),\n",
       " (\"Don't run.\", '[start] Não corram. [end]'),\n",
       " (\"Don't run.\", '[start] Não corras. [end]'),\n",
       " ('Excuse me.', '[start] Perdão. [end]'),\n",
       " ('Excuse me.', '[start] Com licença. [end]'),\n",
       " ('Excuse me.', '[start] Desculpa! [end]'),\n",
       " ('Excuse me.', '[start] Desculpe! [end]'),\n",
       " ('Excuse me?', '[start] O quê? [end]'),\n",
       " ('Excuse me?', '[start] O que você disse? [end]'),\n",
       " ('Excuse me?', '[start] Perdão? [end]'),\n",
       " ('Fantastic!', '[start] Fantástico! [end]'),\n",
       " ('Feel this.', '[start] Sinta isso. [end]'),\n",
       " ('Follow me.', '[start] Siga-me. [end]'),\n",
       " ('Follow me.', '[start] Sigam-me. [end]'),\n",
       " ('Follow me.', '[start] Me siga! [end]'),\n",
       " ('Follow me.', '[start] Me sigam! [end]'),\n",
       " ('Follow us.', '[start] Siga-nos. [end]'),\n",
       " ('Follow us.', '[start] Sigam-nos. [end]'),\n",
       " ('Forget it!', '[start] Esqueça! [end]'),\n",
       " ('Forget it!', '[start] Esquece! [end]'),\n",
       " ('Forget it.', '[start] Esqueça. [end]'),\n",
       " ('Forget it.', '[start] Esquece. [end]'),\n",
       " ('Forget it.', '[start] Esqueçam. [end]'),\n",
       " ('Forget it.', '[start] Esqueça! [end]'),\n",
       " ('Forget it.', '[start] Esquece! [end]'),\n",
       " ('Forget it.', '[start] Deixa. [end]'),\n",
       " ('Forget me.', '[start] Esqueça-me. [end]'),\n",
       " ('Get a job.', '[start] Arrume um emprego. [end]'),\n",
       " ('Go for it.', '[start] Vá em frente. [end]'),\n",
       " ('Go inside.', '[start] Entre. [end]'),\n",
       " ('Go inside.', '[start] Entra. [end]'),\n",
       " ('Go inside.', '[start] Vá para dentro. [end]'),\n",
       " ('Go inside.', '[start] Vai para dentro. [end]'),\n",
       " ('Goodnight.', '[start] Boa noite! [end]'),\n",
       " ('Hands off.', '[start] Tira as mãos. [end]'),\n",
       " ('Hands off.', '[start] Tire as mãos. [end]'),\n",
       " ('Hands off.', '[start] Tirem as mãos. [end]'),\n",
       " ('Have some.', '[start] Pega um pouco. [end]'),\n",
       " ('Have some.', '[start] Pegue um pouco. [end]'),\n",
       " ('Have some.', '[start] Peguem um pouco. [end]'),\n",
       " ('He is ill.', '[start] Ele está doente. [end]'),\n",
       " ('He is old.', '[start] Ele é velho. [end]'),\n",
       " ('He shaved.', '[start] Ele se barbeou. [end]'),\n",
       " ('He shaved.', '[start] Se barbeou. [end]'),\n",
       " ('He smiled.', '[start] Ele sorriu. [end]'),\n",
       " (\"He's a DJ.\", '[start] Ele é DJ. [end]'),\n",
       " (\"He's rich.\", '[start] Ele é rico. [end]'),\n",
       " ('Here I am.', '[start] Aqui estou. [end]'),\n",
       " (\"Here's $5.\", '[start] Aqui estão 5 dólares. [end]'),\n",
       " ('Hold this.', '[start] Segure isto. [end]'),\n",
       " ('How awful!', '[start] Que horror! [end]'),\n",
       " ('How awful!', '[start] Que terrível! [end]'),\n",
       " ('How awful!', '[start] Que ridículo! [end]'),\n",
       " ('How is it?', '[start] Como é isso? [end]'),\n",
       " ('How is it?', '[start] Como isso é? [end]'),\n",
       " (\"How's Tom?\", '[start] Como Tom está? [end]'),\n",
       " (\"How's Tom?\", '[start] Como está o Tom? [end]'),\n",
       " (\"How's Tom?\", '[start] Como vai o Tom? [end]'),\n",
       " ('I am full.', '[start] Estou cheio. [end]'),\n",
       " ('I am full.', '[start] Estou cheia. [end]'),\n",
       " ('I am lost.', '[start] Estou perdido. [end]'),\n",
       " ('I am okay.', '[start] Estou bem. [end]'),\n",
       " ('I am okay.', '[start] Eu vou bem. [end]'),\n",
       " ('I am okay.', '[start] Eu estou bem. [end]'),\n",
       " ('I am sick.', '[start] Estou doente. [end]'),\n",
       " ('I am sick.', '[start] Sou doente. [end]'),\n",
       " ('I am sure.', '[start] Tenho certeza. [end]'),\n",
       " ('I am tall.', '[start] Eu sou alto. [end]'),\n",
       " ('I am tall.', '[start] Sou alto. [end]'),\n",
       " ('I ate out.', '[start] Eu comi fora. [end]'),\n",
       " ('I blinked.', '[start] Eu pisquei. [end]'),\n",
       " ('I can fly.', '[start] Eu sei voar. [end]'),\n",
       " ('I can run.', '[start] Eu posso correr. [end]'),\n",
       " ('I can run.', '[start] Eu sei correr. [end]'),\n",
       " ('I can ski.', '[start] Eu sei esquiar. [end]'),\n",
       " ('I can win.', '[start] Eu posso ganhar. [end]'),\n",
       " ('I cheated.', '[start] Eu trapaceei. [end]'),\n",
       " ('I coughed.', '[start] Eu tossi. [end]'),\n",
       " ('I escaped.', '[start] Eu escapei. [end]'),\n",
       " ('I fainted.', '[start] Eu desmaiei. [end]'),\n",
       " ('I fainted.', '[start] Desmaiei. [end]'),\n",
       " ('I fear so.', '[start] Eu temo que sim. [end]'),\n",
       " ('I get you.', '[start] Eu te entendo. [end]'),\n",
       " ('I get you.', '[start] Te entendo. [end]'),\n",
       " ('I get you.', '[start] Eu entendo você. [end]'),\n",
       " ('I give in.', '[start] Eu me rendo. [end]'),\n",
       " ('I give up.', '[start] Eu desisto. [end]'),\n",
       " ('I got hit.', '[start] Fui atingido. [end]'),\n",
       " ('I got mad.', '[start] Fiquei furioso. [end]'),\n",
       " ('I got mad.', '[start] Eu fiquei furioso. [end]'),\n",
       " ('I grunted.', '[start] Eu resmunguei. [end]'),\n",
       " ('I had fun.', '[start] Eu me diverti. [end]'),\n",
       " ('I hope so.', '[start] Assim espero. [end]'),\n",
       " ('I hope so.', '[start] Espero que sim. [end]'),\n",
       " ('I inhaled.', '[start] Eu inalei. [end]'),\n",
       " ('I knew it.', '[start] Eu sabia. [end]'),\n",
       " ('I laughed.', '[start] Eu ri. [end]'),\n",
       " ('I laughed.', '[start] Ri. [end]'),\n",
       " ('I like it.', '[start] Eu gosto dele. [end]'),\n",
       " ('I lost it.', '[start] Eu o perdi. [end]'),\n",
       " ('I love it.', '[start] Eu amo isso. [end]'),\n",
       " ('I made it.', '[start] Consegui. [end]'),\n",
       " ('I may win.', '[start] Eu posso ganhar. [end]'),\n",
       " ('I mean it.', '[start] Estou falando sério. [end]'),\n",
       " ('I mean it.', '[start] Eu estou falando sério. [end]'),\n",
       " ('I miss it.', '[start] Saudades. [end]'),\n",
       " ('I miss it.', '[start] Saudades disso. [end]'),\n",
       " ('I need it.', '[start] Eu preciso disso. [end]'),\n",
       " ('I need it.', '[start] Preciso disso. [end]'),\n",
       " ('I need it.', '[start] Eu preciso. [end]'),\n",
       " ('I need it.', '[start] Preciso. [end]'),\n",
       " ('I noticed.', '[start] Eu notei. [end]'),\n",
       " ('I noticed.', '[start] Notei. [end]'),\n",
       " ('I promise.', '[start] Eu prometo. [end]'),\n",
       " ('I said no.', '[start] Eu disse que não. [end]'),\n",
       " ('I said no.', '[start] Disse que não. [end]'),\n",
       " ('I saw Tom.', '[start] Eu vi o Tom. [end]'),\n",
       " ('I saw Tom.', '[start] Vi o Tom. [end]'),\n",
       " ('I saw him.', '[start] Eu o vi. [end]'),\n",
       " ('I saw one.', '[start] Eu vi um. [end]'),\n",
       " ('I saw one.', '[start] Eu vi uma. [end]'),\n",
       " ('I see Tom.', '[start] Eu vejo o Tom. [end]'),\n",
       " ('I shouted.', '[start] Eu gritei. [end]'),\n",
       " ('I slipped.', '[start] Eu escorreguei. [end]'),\n",
       " ('I sneezed.', '[start] Eu espirrei. [end]'),\n",
       " ('I stopped.', '[start] Eu parei. [end]'),\n",
       " ('I studied.', '[start] Eu estudei. [end]'),\n",
       " ('I tripped.', '[start] Eu tropecei. [end]'),\n",
       " ('I tripped.', '[start] Tropecei. [end]'),\n",
       " ('I want it.', '[start] Quero. [end]'),\n",
       " ('I want it.', '[start] Eu quero isso. [end]'),\n",
       " ('I want it.', '[start] Quero isso. [end]'),\n",
       " ('I was sad.', '[start] Eu estava triste. [end]'),\n",
       " ('I was sad.', '[start] Eu fiquei triste. [end]'),\n",
       " ('I was wet.', '[start] Eu estava molhado. [end]'),\n",
       " ('I was wet.', '[start] Eu estava molhada. [end]'),\n",
       " ('I will go.', '[start] Eu vou. [end]'),\n",
       " ('I woke up.', '[start] Eu acordei. [end]'),\n",
       " (\"I'd do it.\", '[start] Eu faria isso. [end]'),\n",
       " (\"I'll call.\", '[start] Ligarei. [end]'),\n",
       " (\"I'll cook.\", '[start] Cozinharei. [end]'),\n",
       " (\"I'll live.\", '[start] Eu vou viver. [end]'),\n",
       " (\"I'll lose.\", '[start] Vou perder. [end]'),\n",
       " (\"I'll sing.\", '[start] Cantarei. [end]'),\n",
       " (\"I'll stay.\", '[start] Ficarei. [end]'),\n",
       " (\"I'll stay.\", '[start] Eu vou ficar. [end]'),\n",
       " (\"I'll stop.\", '[start] Eu vou parar. [end]'),\n",
       " (\"I'll swim.\", '[start] Eu vou nadar. [end]'),\n",
       " (\"I'll wait.\", '[start] Esperarei. [end]'),\n",
       " (\"I'll walk.\", '[start] Andarei. [end]'),\n",
       " (\"I'll work.\", '[start] Eu vou trabalhar. [end]'),\n",
       " (\"I'm a man.\", '[start] Eu sou homem. [end]'),\n",
       " (\"I'm a man.\", '[start] Sou um homem. [end]'),\n",
       " (\"I'm a pro.\", '[start] Sou um profissional. [end]'),\n",
       " (\"I'm alert.\", '[start] Estou alerta. [end]'),\n",
       " (\"I'm alive.\", '[start] Estou vivo. [end]'),\n",
       " (\"I'm alive.\", '[start] Estou viva. [end]'),\n",
       " (\"I'm alone.\", '[start] Estou sozinho. [end]'),\n",
       " (\"I'm angry.\", '[start] Estou com raiva. [end]'),\n",
       " (\"I'm awake.\", '[start] Estou acordado. [end]'),\n",
       " (\"I'm blind.\", '[start] Sou cega. [end]'),\n",
       " (\"I'm blind.\", '[start] Sou cego. [end]'),\n",
       " (\"I'm blind.\", '[start] Eu sou cega. [end]'),\n",
       " (\"I'm blind.\", '[start] Eu sou cego. [end]'),\n",
       " (\"I'm bored.\", '[start] Estou chateado. [end]'),\n",
       " (\"I'm bored.\", '[start] Eu estou entediado. [end]'),\n",
       " (\"I'm bossy.\", '[start] Eu sou mandona. [end]'),\n",
       " (\"I'm bossy.\", '[start] Eu sou mandão. [end]'),\n",
       " (\"I'm brave.\", '[start] Eu sou corajoso. [end]'),\n",
       " (\"I'm broke.\", '[start] Estou sem dinheiro. [end]'),\n",
       " (\"I'm broke.\", '[start] Eu estou duro. [end]'),\n",
       " (\"I'm broke.\", '[start] Estou duro. [end]'),\n",
       " (\"I'm broke.\", '[start] Estou falido. [end]'),\n",
       " (\"I'm broke.\", '[start] Estou quebrado. [end]'),\n",
       " (\"I'm crazy.\", '[start] Estou bravo. [end]'),\n",
       " (\"I'm crazy.\", '[start] Estou louco. [end]'),\n",
       " (\"I'm dizzy.\", '[start] Estou tonto. [end]'),\n",
       " (\"I'm drunk.\", '[start] Estou bêbado. [end]'),\n",
       " (\"I'm drunk.\", '[start] Estou embriagado. [end]'),\n",
       " (\"I'm drunk.\", '[start] Eu estou embriagado. [end]'),\n",
       " (\"I'm drunk.\", '[start] Estou bêbada. [end]'),\n",
       " (\"I'm early.\", '[start] Estou adiantado. [end]'),\n",
       " (\"I'm going.\", '[start] Eu vou. [end]'),\n",
       " (\"I'm going.\", '[start] Estou indo. [end]'),\n",
       " (\"I'm going.\", '[start] Já vou. [end]'),\n",
       " (\"I'm happy.\", '[start] Eu sou feliz. [end]'),\n",
       " (\"I'm happy.\", '[start] Eu estou feliz. [end]'),\n",
       " (\"I'm happy.\", '[start] Sou feliz. [end]'),\n",
       " (\"I'm happy.\", '[start] Estou feliz. [end]'),\n",
       " (\"I'm loved.\", '[start] Eu sou amado. [end]'),\n",
       " (\"I'm loved.\", '[start] Eu sou amada. [end]'),\n",
       " (\"I'm loved.\", '[start] Sou amado. [end]'),\n",
       " (\"I'm loyal.\", '[start] Eu sou leal. [end]'),\n",
       " (\"I'm lucky.\", '[start] Eu sou sortudo. [end]'),\n",
       " (\"I'm lying.\", '[start] Estou mentindo. [end]'),\n",
       " (\"I'm naive.\", '[start] Eu sou ingênua. [end]'),\n",
       " (\"I'm naive.\", '[start] Eu sou ingênuo. [end]'),\n",
       " (\"I'm naked.\", '[start] Estou pelado. [end]'),\n",
       " (\"I'm naked.\", '[start] Estou pelada. [end]'),\n",
       " (\"I'm obese.\", '[start] Estou obeso. [end]'),\n",
       " (\"I'm older.\", '[start] Sou mais velho. [end]'),\n",
       " (\"I'm older.\", '[start] Estou mais velho. [end]'),\n",
       " (\"I'm picky.\", '[start] Sou exigente. [end]'),\n",
       " (\"I'm ready!\", '[start] Estou pronto! [end]'),\n",
       " (\"I'm ready!\", '[start] Estou pronta! [end]'),\n",
       " (\"I'm ready.\", '[start] Estou pronto. [end]'),\n",
       " (\"I'm ready.\", '[start] Eu estou pronto. [end]'),\n",
       " (\"I'm right.\", '[start] Eu estou certo. [end]'),\n",
       " (\"I'm small.\", '[start] Eu sou pequena. [end]'),\n",
       " (\"I'm small.\", '[start] Eu sou pequeno. [end]'),\n",
       " (\"I'm sober.\", '[start] Estou sóbrio. [end]'),\n",
       " (\"I'm sorry.\", '[start] Desculpa! [end]'),\n",
       " (\"I'm sorry.\", '[start] Desculpe! [end]'),\n",
       " (\"I'm sorry.\", '[start] Lamento. [end]'),\n",
       " (\"I'm sorry.\", '[start] Me desculpe. [end]'),\n",
       " (\"I'm sorry.\", '[start] Sinto muito. [end]'),\n",
       " (\"I'm stuck.\", '[start] Estou preso. [end]'),\n",
       " (\"I'm stuck.\", '[start] Eu estou presa. [end]'),\n",
       " (\"I'm stuck.\", '[start] Eu estou preso. [end]'),\n",
       " (\"I'm stuck.\", '[start] Estou presa. [end]'),\n",
       " (\"I'm timid.\", '[start] Eu sou tímido. [end]'),\n",
       " (\"I'm tired.\", '[start] Estou cansado. [end]'),\n",
       " (\"I'm tired.\", '[start] Eu estou cansado! [end]'),\n",
       " (\"I'm tired.\", '[start] Estou cansado! [end]'),\n",
       " (\"I'm upset.\", '[start] Estou chateado. [end]'),\n",
       " (\"I'm upset.\", '[start] Estou chateada. [end]'),\n",
       " (\"I'm upset.\", '[start] Eu estou chateado. [end]'),\n",
       " (\"I'm weird.\", '[start] Sou estranho. [end]'),\n",
       " (\"I'm weird.\", '[start] Estou estranho. [end]'),\n",
       " (\"I'm wrong.\", '[start] Estou errado. [end]'),\n",
       " (\"I'm wrong.\", '[start] Estou errada. [end]'),\n",
       " (\"I'm young.\", '[start] Eu sou jovem. [end]'),\n",
       " (\"I'm young.\", '[start] Sou jovem. [end]'),\n",
       " (\"I'm yours.\", '[start] Sou seu. [end]'),\n",
       " (\"I'm yours.\", '[start] Eu sou seu. [end]'),\n",
       " (\"I'm yours.\", '[start] Eu sou sua. [end]'),\n",
       " (\"I'm yours.\", '[start] Sou sua. [end]'),\n",
       " ('Is Tom OK?', '[start] O Tom está bem? [end]'),\n",
       " ('Is Tom in?', '[start] O Tom está dentro? [end]'),\n",
       " ('Is he Tom?', '[start] Ele é o Tom? [end]'),\n",
       " ('Is he Tom?', '[start] É ele o Tom? [end]'),\n",
       " ('Is it bad?', '[start] É grave? [end]'),\n",
       " ('Is it big?', '[start] É grande? [end]'),\n",
       " ('Is it far?', '[start] É longe? [end]'),\n",
       " ('Is it hot?', '[start] Está calor? [end]'),\n",
       " ('Is it you?', '[start] É você? [end]'),\n",
       " ('Is it you?', '[start] És tu? [end]'),\n",
       " ('It burned.', '[start] Queimou. [end]'),\n",
       " ('It failed.', '[start] Falhou. [end]'),\n",
       " ('It rained.', '[start] Estava chovendo. [end]'),\n",
       " ('It snowed.', '[start] Nevou. [end]'),\n",
       " ('It stinks.', '[start] Isso fede. [end]'),\n",
       " ('It was OK.', '[start] Foi bonzinho. [end]'),\n",
       " ('It worked.', '[start] Funcionou! [end]'),\n",
       " (\"It's 3:30.\", '[start] São 3h30. [end]'),\n",
       " (\"It's 7:45.\", '[start] São 7h45. [end]'),\n",
       " (\"It's 8:30.\", '[start] São 8h30. [end]'),\n",
       " (\"It's 9:15.\", '[start] São 9h15. [end]'),\n",
       " (\"It's a TV.\", '[start] É uma TV. [end]'),\n",
       " (\"It's a TV.\", '[start] Isso é uma TV. [end]'),\n",
       " (\"It's a TV.\", '[start] Isto é uma TV. [end]'),\n",
       " (\"It's cool.\", '[start] É legal. [end]'),\n",
       " (\"It's done!\", '[start] Pronto, já está! [end]'),\n",
       " (\"It's done!\", '[start] Está pronto! [end]'),\n",
       " (\"It's done.\", '[start] Está pronto. [end]'),\n",
       " (\"It's done.\", '[start] Está pronta. [end]'),\n",
       " (\"It's easy.\", '[start] Isso é fácil. [end]'),\n",
       " (\"It's easy.\", '[start] É fácil. [end]'),\n",
       " (\"It's fair.\", '[start] É justo. [end]'),\n",
       " (\"It's fine.\", '[start] Está bem. [end]'),\n",
       " (\"It's food.\", '[start] É comida. [end]'),\n",
       " (\"It's free.\", '[start] É de graça. [end]'),\n",
       " (\"It's free.\", '[start] É gratuito. [end]'),\n",
       " (\"It's here.\", '[start] Está aqui. [end]'),\n",
       " (\"It's here.\", '[start] É aqui. [end]'),\n",
       " (\"It's hers.\", '[start] É dela. [end]'),\n",
       " (\"It's hers.\", '[start] Isso é dela. [end]'),\n",
       " (\"It's huge.\", '[start] É enorme. [end]'),\n",
       " (\"It's late.\", '[start] Está tarde. [end]'),\n",
       " (\"It's late.\", '[start] É tarde. [end]'),\n",
       " (\"It's mine.\", '[start] É meu. [end]'),\n",
       " (\"It's mine.\", '[start] É minha. [end]'),\n",
       " (\"It's okay.\", '[start] Está bem. [end]'),\n",
       " (\"It's open.\", '[start] Está aberto. [end]'),\n",
       " (\"It's ours.\", '[start] É nosso. [end]'),\n",
       " (\"It's over.\", '[start] Acabou. [end]'),\n",
       " (\"It's time.\", '[start] Está na hora. [end]'),\n",
       " (\"It's true!\", '[start] É verdade! [end]'),\n",
       " (\"It's true.\", '[start] É verdade. [end]'),\n",
       " (\"It's work.\", '[start] É trabalho. [end]'),\n",
       " ('Keep away.', '[start] Mantenha-se afastado. [end]'),\n",
       " ('Keep back.', '[start] Afaste-se. [end]'),\n",
       " ('Keep back.', '[start] Fique afastado. [end]'),\n",
       " ('Keep calm.', '[start] Acalme-se. [end]'),\n",
       " ('Keep cool.', '[start] Mantenha a calma. [end]'),\n",
       " ('Keep cool.', '[start] Te acalma. [end]'),\n",
       " ('Keep down.', '[start] Fique abaixado. [end]'),\n",
       " ('Keep down.', '[start] Fique abaixada. [end]'),\n",
       " ('Keep that.', '[start] Fique com isso. [end]'),\n",
       " ('Keep them.', '[start] Fique com eles. [end]'),\n",
       " ('Keep them.', '[start] Fique com elas. [end]'),\n",
       " ('Keep this.', '[start] Fique com isto. [end]'),\n",
       " ('Keep warm.', '[start] Mantenha aquecido. [end]'),\n",
       " ('Kill them.', '[start] Mate-os. [end]'),\n",
       " ('Kill them.', '[start] Mate-as. [end]'),\n",
       " ('Leave Tom.', '[start] Deixe o Tom. [end]'),\n",
       " ('Leave now!', '[start] Saia agora! [end]'),\n",
       " ('Leave now.', '[start] Vá embora agora. [end]'),\n",
       " ('Let me go!', '[start] Deixe-me ir embora! [end]'),\n",
       " ('Let me in.', '[start] Deixe-me entrar. [end]'),\n",
       " ('Let us in.', '[start] Deixe-nos entrar. [end]'),\n",
       " (\"Let's see.\", '[start] Vejamos. [end]'),\n",
       " ('Listen up.', '[start] Escute. [end]'),\n",
       " ('Listen up.', '[start] Escutem. [end]'),\n",
       " ('Look back!', '[start] Olhe para trás! [end]'),\n",
       " ('Look back!', '[start] Olhem para trás! [end]'),\n",
       " ('Look back!', '[start] Olha pra trás! [end]'),\n",
       " ('Look back.', '[start] Olhe para trás! [end]'),\n",
       " ('Look back.', '[start] Olhem para trás! [end]'),\n",
       " ('Look here.', '[start] Olhe aqui. [end]'),\n",
       " ...]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with open(\"data_text\\eng-por.txt\", encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "text_pairs = []\n",
    "for line in lines: \n",
    "    english, portuguese, _ = line.split(\"\\t\")\n",
    "    portuguese = \"[start] \" + portuguese + \" [end]\"\n",
    "    text_pairs.append((english, portuguese))\n",
    "display(text_pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, let's divide them into the customary training, validation, and test sets after shuffling them. We also need to prepare the samples of our target language with special tokes (`[start]` and `[end]`), to help the generative part of our models to understand _when a sentence starts and ends_.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The learned vocabulary for each language needs to be separated into two separate TextVectorization layers, which need to be created now (English and Portuguese). The \"`[start]`\" and \"`[end]`\" tokens that we have inserted must also be kept. The characters \"`[ ]`\" would normally be stripped, but we want to leave them in.**\n",
    "\n",
    "**Portuguese is a language that has a particular encoding (`iso-8859-1`). In the Portuguese language, \"está\" and \"esta\" are not the same word and have different meanings. So we need to preserve the accents as much as we can. TensorFlow 2.11 has a class for vectorization, `TextVectorization`, which allows you to select the desired encoding types (not available in 2.10). Unfortunately, Tensorflow 2.11 has no support for native windows + Cuda. However, we can just take the function we want to use and import it separately. In the `updated_TextVectorization.py` file, you find version 2.11 of the `tf.keras.layers.TextVectorization` class, available [here](https://github.com/keras-team/keras/blob/v2.11.0/keras/layers/preprocessing/text_vectorization.py#L50-L653). However, you won't find this script in our repository, due to the licenses of the TensorFlow library.**\n",
    "\n",
    "**So that we can reuse the vocab learned, we will save them as `txt` files. In these files are all the words learned by the `TextVectorization` class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from updated_TextVectorization import TextVectorization\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "vocab_size = 20000\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    "    encoding='iso-8859-1'\n",
    "    )\n",
    "\n",
    "target_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    "    encoding='iso-8859-1'\n",
    "    )\n",
    "\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_portuguese_texts = [pair[1] for pair in train_pairs]\n",
    "\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_portuguese_texts)\n",
    "\n",
    "portuguese_vocab = target_vectorization.get_vocabulary()\n",
    "\n",
    "with open(r'data_text\\\\portuguese_vocabulary.txt', 'w', encoding='iso-8859-1') as fp:\n",
    "    for word in portuguese_vocab:\n",
    "        fp.write(\"%s\\n\" % word)\n",
    "\n",
    "english_vocab = source_vectorization.get_vocabulary()\n",
    "\n",
    "with open(r'data_text\\\\english_vocabulary.txt', 'w', encoding='iso-8859-1') as fp:\n",
    "    for word in english_vocab:\n",
    "        fp.write(\"%s\\n\" % word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, we will create a `tf.data pipeline` using our data. The dataset will be transformed into a \"(inputs, target)\" where the target is the Portuguese sentence offset by one step ahead (the next token in the input sequence). Meanwhile, inputs are a dictionary with two keys, \"encoder inputs\" (the English sentence) and \"decoder inputs\" (the Portuguese sentence).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "def format_dataset(eng, por):\n",
    "    eng = source_vectorization(eng)\n",
    "    por = target_vectorization(por)\n",
    "    return ({\n",
    "    \"english\": eng,\n",
    "    \"portuguese\": por[:, :-1],\n",
    "    }, por[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, por_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    por_texts = list(por_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, por_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(42).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can train our first model, which will be an RNN. `Recurrent neural networks` dominated `sequence-to-sequence` learning from 2015–2017 before being overtaken by Transformer. They were the basis for many real-world machine-translation systems, like the [Google Translate circa 2017](https://en.wikipedia.org/wiki/Google_Neural_Machine_Translation), which was powered by a stack of eight large LSTM layers.**\n",
    "\n",
    "**But for this notebook, we will use [GRUs](https://en.wikipedia.org/wiki/Gated_recurrent_unit) instead of an [LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory). The GRU is like an LSTM with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate.**\n",
    "\n",
    "**If you wish, you can stack more GRU layers in the encoder and decoder parts. As it is, this model has already  + more than 40M parameters. Stacking recurrent layers will slow training down. Given the _sequence processing nature_ of RNNs, there are limits two how much we can parallelize.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " english (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " portuguese (InputLayer)        [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 256)    5120000     ['english[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 256)    5120000     ['portuguese[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 1024)         7876608     ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, None, 1024)   3938304     ['embedding_1[0][0]',            \n",
      "                                                                  'bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 1024)   0           ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 20000)  20500000    ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 42,554,912\n",
      "Trainable params: 42,554,912\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\") \n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source) \n",
    "encoded_source = layers.Bidirectional(layers.GRU(latent_dim), merge_mode=\"sum\")(x)\n",
    "\n",
    "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"portuguese\") \n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target) \n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source) \n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x) \n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)\n",
    "\n",
    "seq2seq_rnn.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "seq2seq_rnn.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now all that is left to do is to train our `seq2seq_rnn`. Be mindful that without a GPU, the training of this model could take many hours.**\n",
    "\n",
    "**If you wish to skip training and use the pre-trained model (`GRU-eng-pot.keras`), you can download it in this [link](https://drive.google.com/uc?export=download&id=1q2S7xdKdsRCg5PSthvxNnUTuzo8gYUnx).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.10.1\n",
      "Eager mode:  True\n",
      "GPU is available\n",
      "Epoch 1/15\n",
      "3695/3695 [==============================] - 1138s 306ms/step - loss: 1.3794 - accuracy: 0.4983 - val_loss: 1.1027 - val_accuracy: 0.5913\n",
      "Epoch 2/15\n",
      "3695/3695 [==============================] - 1125s 305ms/step - loss: 1.0935 - accuracy: 0.6008 - val_loss: 1.0387 - val_accuracy: 0.6307\n",
      "Epoch 3/15\n",
      "3695/3695 [==============================] - 1126s 305ms/step - loss: 1.0568 - accuracy: 0.6305 - val_loss: 1.0426 - val_accuracy: 0.6434\n",
      "Epoch 4/15\n",
      "3695/3695 [==============================] - 1125s 304ms/step - loss: 1.0522 - accuracy: 0.6450 - val_loss: 1.0484 - val_accuracy: 0.6501\n",
      "Epoch 5/15\n",
      "3695/3695 [==============================] - 1134s 307ms/step - loss: 1.0494 - accuracy: 0.6526 - val_loss: 1.0506 - val_accuracy: 0.6539\n",
      "Epoch 6/15\n",
      "3695/3695 [==============================] - 1135s 307ms/step - loss: 1.0478 - accuracy: 0.6566 - val_loss: 1.0528 - val_accuracy: 0.6552\n",
      "Epoch 7/15\n",
      "3695/3695 [==============================] - 1136s 307ms/step - loss: 1.0477 - accuracy: 0.6585 - val_loss: 1.0554 - val_accuracy: 0.6553\n",
      "Epoch 8/15\n",
      "3695/3695 [==============================] - 1136s 307ms/step - loss: 1.0495 - accuracy: 0.6585 - val_loss: 1.0583 - val_accuracy: 0.6540\n",
      "Epoch 9/15\n",
      "3695/3695 [==============================] - 1135s 307ms/step - loss: 1.0537 - accuracy: 0.6571 - val_loss: 1.0613 - val_accuracy: 0.6538\n",
      "Epoch 10/15\n",
      "3695/3695 [==============================] - 1136s 307ms/step - loss: 1.0591 - accuracy: 0.6553 - val_loss: 1.0673 - val_accuracy: 0.6517\n",
      "Epoch 11/15\n",
      "3695/3695 [==============================] - 1136s 307ms/step - loss: 1.0653 - accuracy: 0.6528 - val_loss: 1.0716 - val_accuracy: 0.6490\n",
      "Epoch 12/15\n",
      "3695/3695 [==============================] - 1136s 307ms/step - loss: 1.0727 - accuracy: 0.6493 - val_loss: 1.0793 - val_accuracy: 0.6469\n",
      "Epoch 13/15\n",
      "3695/3695 [==============================] - 1136s 307ms/step - loss: 1.0810 - accuracy: 0.6461 - val_loss: 1.0842 - val_accuracy: 0.6448\n",
      "Epoch 14/15\n",
      "3695/3695 [==============================] - 1136s 307ms/step - loss: 1.0902 - accuracy: 0.6425 - val_loss: 1.0925 - val_accuracy: 0.6414\n",
      "Epoch 15/15\n",
      "3695/3695 [==============================] - 1135s 307ms/step - loss: 1.1007 - accuracy: 0.6385 - val_loss: 1.0980 - val_accuracy: 0.6389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26eb7c350d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"models\\GRU-eng-pot.keras\",\n",
    "                                                save_best_only=True,\n",
    "                                                monitor=\"val_loss\",\n",
    "                                                patience=10, \n",
    "                                                restore_best_weights=True)]\n",
    "\n",
    "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy is a crude way to monitor validation-set performance during this kind of task. On average, this model predicts the next word in the Portuguese sentence correctly $65\\%$ of the time. However, in practice, next-token accuracy isn’t a great metric for machine translation models. During inference, you’re generating the target sentence from scratch, and you can’t rely on previously generated tokens (a.k.a. $100\\%$ correctness does not mean you have a good translator). In real-world machine translation applications, we would likely use “_BLEU scores_” to evaluate our models.**\n",
    "\n",
    "**BLEU (_bilingual evaluation understudy_) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine's output and that of a human: \"_the closer a machine translation is to a professional human translation, the better it is_\" – this is the central idea behind BLEU.**\n",
    "\n",
    "**To test our model, we can use sentences from our test set, or write some English sentences ourselves. Given an English sentence, we will feed the decoder block a `[start]` token, together with the encoded version of the English sentence, and the decoder will auto-regressively generate the next token, append to the decoded sequence ([`[start], [new token], ...`]), repeatedly, until the `[end]` token is generated, or the model reaches gets stomped by our maximum sentence length.** \n",
    "\n",
    "**Bellow, we import our trained model and vocabularies and test it on some sample text. The trained (or downloaded) model should be in the `models` folder.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:\n",
      "What is its name?\n",
      "Portuguese transalation:\n",
      "[start] o que é o seu nome [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "How old are you?\n",
      "Portuguese transalation:\n",
      "[start] quantos anos [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "I know you know where Mary is.\n",
      "Portuguese transalation:\n",
      "[start] eu sei que você sabe onde está [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "We will show Tom.\n",
      "Portuguese transalation:\n",
      "[start] nós vamos ao tom [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "What do you all do?\n",
      "Portuguese transalation:\n",
      "[start] o que vocês estão fazendo [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "Don't do it!\n",
      "Portuguese transalation:\n",
      "[start] não faça isso [end]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "import keras\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "seq2seq_rnn = tf.keras.models.load_model(\"data\\GRU-eng-pot.keras\")\n",
    "\n",
    "with open('data\\portuguese_vocabulary.txt', encoding='utf-8', errors='backslashreplace') as file_in:\n",
    "    portuguese_vocab = [line.strip() for line in file_in]\n",
    "\n",
    "with open('data\\english_vocabulary.txt', encoding='utf-8', errors='backslashreplace') as file_in:\n",
    "    english_vocab = [line.strip() for line in file_in]\n",
    "\n",
    "from updated_TextVectorization import TextVectorization\n",
    "\n",
    "target_vectorization = TextVectorization(max_tokens=20000,\n",
    "                                        output_mode=\"int\",\n",
    "                                        output_sequence_length=21,\n",
    "                                        standardize=custom_standardization,\n",
    "                                        vocabulary=portuguese_vocab,\n",
    "                                        encoding='iso-8859-1')\n",
    "\n",
    "source_vectorization = TextVectorization(max_tokens=20000,\n",
    "                                        output_mode=\"int\",\n",
    "                                        output_sequence_length=20,\n",
    "                                        vocabulary=english_vocab,\n",
    "                                        encoding='iso-8859-1')\n",
    "\n",
    "portuguese_index_lookup = dict(zip(range(len(portuguese_vocab)), portuguese_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = seq2seq_rnn.predict([tokenized_input_sentence, tokenized_target_sentence], verbose=0) \n",
    "        sampled_token_index = np.argmax(next_token_predictions[0, i, :]) \n",
    "        sampled_token = portuguese_index_lookup[sampled_token_index] \n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\": \n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "eng_sentences =[\"What is its name?\",\n",
    "                \"How old are you?\",\n",
    "                \"I know you know where Mary is.\",\n",
    "                \"We will show Tom.\",\n",
    "                \"What do you all do?\",\n",
    "                \"Don't do it!\"]\n",
    "\n",
    "for sentence in eng_sentences:\n",
    "    print(f\"English sentence:\\n{sentence}\")\n",
    "    print(f'Portuguese transalation:\\n{decode_sequence(sentence)}')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see, translations are far from perfect. To improve this model, we could:**\n",
    "\n",
    "- **Use a deep stack of recurrent layers for both the encoder and the decoder.** \n",
    "- **We could use an `LSTM` instead of a `GRU`.**\n",
    "\n",
    "**However, RNNs have limitations when it comes to sequence-to-sequence tasks and language modeling in general. For example,  due to their propensity to gradually forget the past, RNNs struggle to handle extremely long sequences. Consequently, RNN-based models are unable to retain long-term context, which is necessary for translating lengthy documents (which is one of the reasons why many online translation tools are limited by the size of the input).**\n",
    "\n",
    "**It is because of these limitations that the machine learning community has embraced the 'Transformer' architecture for sequence-to-sequence problems. And sequence-to-sequence learning is the task where Transformers excel the most.**\n",
    "\n",
    "**Bellow, you can see the full sequence-to-sequence Transformer.**\n",
    "\n",
    "![transformer-block](https://d2l.ai/_images/transformer.svg)\n",
    "\n",
    "_source_: [Dive into Deep Learning 11. Attention Mechanisms and Transformers - 11.7. The Transformer Architecture](https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html)\n",
    "\n",
    "**The transformer model consists of two main components: the encoder and the decoder.**\n",
    "\n",
    "**The encoder is responsible for converting the input sequence into a set of internal representations that capture the relevant information in the input. It does this by applying a series of self-attention and feedforward layers to the input sequence. The self-attention layers allow the model to dynamically weigh the input features and compute a weighted sum of the features, which allows the model to capture long-range dependencies in the input data.**\n",
    "\n",
    "**The decoder is responsible for generating the output sequence based on the internal representations produced by the encoder. It does this by applying a series of self-attention and feedforward layers to the internal representations and a set of additional \"context\" vectors that are computed from the encoder output. The decoder also uses an attention mechanism to weigh the context vectors and incorporate information from the encoder output into the generation of the output sequence (a type of residual connection shared by both transformer blocks).**\n",
    "\n",
    "**Together, the encoder and decoder allow the transformer model to effectively process and translate input sequences and capture long-range dependencies in the data.**\n",
    "\n",
    "**For an extremely _comprehensive_ and _ilustrated_ explanation of what is \"_attention_\" or how a \"_transformer works_\", we recommend the work of _Jay Alammar_:**\n",
    "\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/);\n",
    "- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/).\n",
    "\n",
    "**But the general \"gist\" of attention and self attention is this:**\n",
    "\n",
    "**In a transformer model, self-attention is calculated using three input matrices: the query matrix ($Q$), the key matrix ($K$), and the value matrix ($V$). The self-attention mechanism computes a weighted sum of the value matrix ($V$) based on the similarity between the queries ($Q$) and the keys ($K$).**\n",
    "\n",
    "**The query, key, and value matrices are typically derived from the input data by applying a linear transformation to the input features. The dimensions of the matrices depend on the size of the input and the number of attention \"heads\" used by the model.**\n",
    "\n",
    "**To compute self-attention, the model first computes the dot product of the query and key matrices, which produces a matrix of dot products. The dot products are then divided by the square root of the dimensionality of the key matrix to ensure that the self-attention weights are well-behaved. Finally, the dot products are passed through a softmax function to produce a matrix of attention weights that sum to $1$.**\n",
    "\n",
    "**The attention weights are then used to weight the value matrix ($V$) and compute a weighted sum of the values, which is then used as the output of the self-attention mechanism. This process is repeated for each attention head and the output of all the attention heads is concatenated to produce the final self-attention output.**\n",
    "\n",
    "**The self-attention mechanism allows the model to dynamically weigh the input features and compute a weighted sum of the features based on the similarity between the queries and the keys. This allows the model to capture long-range dependencies in the input data and effectively process input sequences of variable length.**\n",
    "\n",
    "**Here is the equation for self-attention calculation in transformer models:**\n",
    "\n",
    "$$ Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V $$\n",
    "\n",
    "**Where, $Q$ is the query matrix, $K$ is the key matrix, $V$ is the value matrix, and $d_k$ is the dimensionality of the key matrix.**\n",
    "\n",
    "## Transformer Encoder\n",
    "\n",
    "**Fundamentally, the encoder part is a \"text classification\" machine. It's a very generic module that takes in a sequence and learns how to transform it into a more useful representation.**\n",
    "\n",
    "**We used the decoder part to make a toxicity classifier in this [notebook](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/06db2a47b9868672df778cceb99c33610a8cc9f8/ML%20Intro%20Course/toxicity_detection.ipynb). And [Ai.ra](https://github.com/Nkluge-correa/Aira-EXPERT), AIRES closed domain chatbot, was built (in its last interaction), as a `decoder-only transformer`.**\n",
    "\n",
    "**The decoder block is made up of dense layers that process sequence tokens independently of one another, as well as an attention layer that examines the tokens as a group. You could rearrange the tokens in a sequence and still get the same pairwise attention scores.**\n",
    "\n",
    "**Self-attention is a set-processing mechanism that focuses on the relationships between pairs of sequence elements; unlike RNNs, it is not concerned with the order of the tokens.**\n",
    "\n",
    "**Below we implement the encoder block using the subclass features of the Keras API.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding\n",
    "\n",
    "**As said before, self-attention is an order-agnostic technique. However, as done by [Vaswaniet al.](https://arxiv.org/abs/1706.03762), we can manually inject order information in the representations through _positional encoding_ (i.e., give the model access to word-order information).**\n",
    "\n",
    "**The original “_Attention is all you need_” paper added to the word embeddings a vector containing values in the range [-1, 1] that varied cyclically depending on the position (a cosine function).**\n",
    "\n",
    "**We’ll simply use the `tf.keras.layers.Embedding` layer to create a positional encoding parallel to the normal `Embedding` layer. An embedding layer is a neural network layer that maps categorical variables, such as words in a vocabulary, into a continuous vector space. The goal of the embedding layer is to represent each word or categorical variable in a way that captures its relevant semantic and syntactic properties.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder\n",
    "\n",
    "**The Transformer decoder is the model's second half. It reads tokens $0....N$ in the target sequence and tries to predict token $N+1$, just like the RNN decoder. While doing so, it employs neural attention to determine which tokens in the encoded source sentence are most closely related to the target token it is currently attempting to predict.**\n",
    "\n",
    "**An extra bit of complication that the decoder brings is the idea of _causal padding_. Causal padding is critical for successfully training sequence-to-sequence Transformers. Causal padding is used to ensure that the network does not violate the temporal or causal order of the data. Unlike an RNN, which looks at its input one step at a time and thus only has access to steps $0....N$ to generate output step $N+1$, the TransformerDecoder can look at the entire target sequence at once (without casual padding). If it could use all of its input, it would simply learn to copy input step $N+1$. As a result, the model would achieve perfect training accuracy while learning nothing useful. And that is why we need causal padding. To \"_hide the future_\" from out model during training.**\n",
    "\n",
    "![causal-attention-padding](https://jalammar.github.io/images/gpt2/transformer-attention-mask.png)\n",
    "\n",
    "_Source:_ [The Illustrated GPT-2 (Visualizing Transformer Language Models)](https://jalammar.github.io/illustrated-gpt2/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When we put all of the puzzle pieces together, we get the complete end-to-end Transformer. It simply combines the components we've created thus far: the `PositionalEmbedding` layers, the `TransformerEncoder`, and the `TransformerDecoder`. Similarly to our GRU model, we could stack these decoder and encoder blocks one on top of the other to create a more robust model.**\n",
    "\n",
    "**However, let us keep it small, and use just one encoder-decoder block. The original Transformer model consists of 6 stacked encoder-decoder blocks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " english (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " portuguese (InputLayer)        [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_4 (Positi  (None, None, 256)   5125120     ['english[0][0]']                \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " positional_embedding_5 (Positi  (None, None, 256)   5125120     ['portuguese[0][0]']             \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " transformer_encoder_2 (Transfo  (None, None, 256)   3155456     ['positional_embedding_4[0][0]'] \n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " transformer_decoder_2 (Transfo  (None, None, 256)   5259520     ['positional_embedding_5[0][0]', \n",
      " rmerDecoder)                                                     'transformer_encoder_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, None, 256)    0           ['transformer_decoder_2[0][0]']  \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, None, 20000)  5140000     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,805,216\n",
      "Trainable params: 23,805,216\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "vocab_size = 20000\n",
    "sequence_length = 20\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"portuguese\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x) \n",
    "\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "transformer.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see, this model is almost half the size of our RNN, and will be a little bit faster to train it! However, if you wish to skip training and use the pre-trained model (`transformer-eng-pot.keras`), you can download it in this [link](https://drive.google.com/uc?export=download&id=1NTTDKZZ41Vw-3IVMB_jLstqlqPp77vqy).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.10.1\n",
      "Eager mode:  True\n",
      "GPU is available\n",
      "Epoch 1/30\n",
      "3695/3695 [==============================] - 598s 161ms/step - loss: 1.4658 - accuracy: 0.4758 - val_loss: 1.1789 - val_accuracy: 0.5651\n",
      "Epoch 2/30\n",
      "3695/3695 [==============================] - 601s 163ms/step - loss: 1.1499 - accuracy: 0.5810 - val_loss: 1.0782 - val_accuracy: 0.6121\n",
      "Epoch 3/30\n",
      "3695/3695 [==============================] - 601s 163ms/step - loss: 1.0884 - accuracy: 0.6186 - val_loss: 1.0531 - val_accuracy: 0.6363\n",
      "Epoch 4/30\n",
      "3695/3695 [==============================] - 602s 163ms/step - loss: 1.0657 - accuracy: 0.6380 - val_loss: 1.0455 - val_accuracy: 0.6476\n",
      "Epoch 5/30\n",
      "3695/3695 [==============================] - 600s 162ms/step - loss: 1.0456 - accuracy: 0.6511 - val_loss: 1.0332 - val_accuracy: 0.6563\n",
      "Epoch 6/30\n",
      "3695/3695 [==============================] - 599s 162ms/step - loss: 1.0271 - accuracy: 0.6607 - val_loss: 1.0182 - val_accuracy: 0.6613\n",
      "Epoch 7/30\n",
      "3695/3695 [==============================] - 608s 165ms/step - loss: 1.0088 - accuracy: 0.6686 - val_loss: 1.0074 - val_accuracy: 0.6678\n",
      "Epoch 8/30\n",
      "3695/3695 [==============================] - 596s 161ms/step - loss: 0.9927 - accuracy: 0.6753 - val_loss: 0.9923 - val_accuracy: 0.6709\n",
      "Epoch 9/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.9773 - accuracy: 0.6809 - val_loss: 0.9903 - val_accuracy: 0.6704\n",
      "Epoch 10/30\n",
      "3695/3695 [==============================] - 603s 163ms/step - loss: 0.9632 - accuracy: 0.6861 - val_loss: 0.9846 - val_accuracy: 0.6741\n",
      "Epoch 11/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.9503 - accuracy: 0.6905 - val_loss: 0.9839 - val_accuracy: 0.6766\n",
      "Epoch 12/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.9384 - accuracy: 0.6949 - val_loss: 0.9748 - val_accuracy: 0.6798\n",
      "Epoch 13/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.9299 - accuracy: 0.6983 - val_loss: 0.9774 - val_accuracy: 0.6802\n",
      "Epoch 14/30\n",
      "3695/3695 [==============================] - 597s 161ms/step - loss: 0.9191 - accuracy: 0.7013 - val_loss: 0.9741 - val_accuracy: 0.6788\n",
      "Epoch 15/30\n",
      "3695/3695 [==============================] - 597s 161ms/step - loss: 0.9111 - accuracy: 0.7045 - val_loss: 0.9777 - val_accuracy: 0.6821\n",
      "Epoch 16/30\n",
      "3695/3695 [==============================] - 597s 161ms/step - loss: 0.9035 - accuracy: 0.7067 - val_loss: 0.9646 - val_accuracy: 0.6836\n",
      "Epoch 17/30\n",
      "3695/3695 [==============================] - 596s 161ms/step - loss: 0.8960 - accuracy: 0.7097 - val_loss: 0.9716 - val_accuracy: 0.6836\n",
      "Epoch 18/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.8897 - accuracy: 0.7114 - val_loss: 0.9708 - val_accuracy: 0.6814\n",
      "Epoch 19/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.8861 - accuracy: 0.7131 - val_loss: 0.9671 - val_accuracy: 0.6813\n",
      "Epoch 20/30\n",
      "3695/3695 [==============================] - 595s 161ms/step - loss: 0.8798 - accuracy: 0.7150 - val_loss: 0.9699 - val_accuracy: 0.6832\n",
      "Epoch 21/30\n",
      "3695/3695 [==============================] - 594s 161ms/step - loss: 0.8784 - accuracy: 0.7157 - val_loss: 0.9652 - val_accuracy: 0.6861\n",
      "Epoch 22/30\n",
      "3695/3695 [==============================] - 602s 163ms/step - loss: 0.8739 - accuracy: 0.7172 - val_loss: 0.9705 - val_accuracy: 0.6832\n",
      "Epoch 23/30\n",
      "3695/3695 [==============================] - 603s 163ms/step - loss: 0.8726 - accuracy: 0.7178 - val_loss: 0.9710 - val_accuracy: 0.6860\n",
      "Epoch 24/30\n",
      "3695/3695 [==============================] - 601s 163ms/step - loss: 0.8711 - accuracy: 0.7186 - val_loss: 0.9754 - val_accuracy: 0.6825\n",
      "Epoch 25/30\n",
      "3695/3695 [==============================] - 601s 163ms/step - loss: 0.8682 - accuracy: 0.7189 - val_loss: 0.9844 - val_accuracy: 0.6790\n",
      "Epoch 26/30\n",
      "3695/3695 [==============================] - 603s 163ms/step - loss: 0.8677 - accuracy: 0.7201 - val_loss: 0.9720 - val_accuracy: 0.6858\n",
      "Epoch 27/30\n",
      "3695/3695 [==============================] - 605s 164ms/step - loss: 0.8633 - accuracy: 0.7214 - val_loss: 0.9698 - val_accuracy: 0.6825\n",
      "Epoch 28/30\n",
      "3695/3695 [==============================] - 605s 164ms/step - loss: 0.8590 - accuracy: 0.7226 - val_loss: 0.9703 - val_accuracy: 0.6831\n",
      "Epoch 29/30\n",
      "3695/3695 [==============================] - 605s 164ms/step - loss: 0.8587 - accuracy: 0.7234 - val_loss: 0.9784 - val_accuracy: 0.6831\n",
      "Epoch 30/30\n",
      "3695/3695 [==============================] - 604s 164ms/step - loss: 0.8543 - accuracy: 0.7246 - val_loss: 0.9876 - val_accuracy: 0.6805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f22ba29ee0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"models\\\\transformer-eng-pot.keras\",\n",
    "                                                save_best_only=True,\n",
    "                                                monitor=\"val_loss\",\n",
    "                                                patience=10, \n",
    "                                                restore_best_weights=True)]\n",
    "\n",
    "transformer.fit(train_ds, epochs=30, validation_data=val_ds, callbacks=callbacks)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bellow, we import our trained model and vocabularies and test it on some sample text. Since we are using Keras subclassess, we will need to load our models like this:**\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "transformer = keras.models.load_model(\"transformer-eng-pot.keras\", \n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder, \n",
    "        \"PositionalEmbedding\": PositionalEmbedding,\n",
    "        \"TransformerDecoder\": TransformerDecoder})\n",
    "                                                 \n",
    "```\n",
    "\n",
    "**The trained (or downloaded) model should be in the `models` folder.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:\n",
      "What is its name?\n",
      "Portuguese transalation:\n",
      "[start] qual é o nome dele [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "How old are you?\n",
      "Portuguese transalation:\n",
      "[start] quantos anos você tem [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "I know you know where Mary is.\n",
      "Portuguese transalation:\n",
      "[start] eu sei que você sabe onde maria está [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "We will show Tom.\n",
      "Portuguese transalation:\n",
      "[start] nós vamos mostrar ao tom [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "What do you all do?\n",
      "Portuguese transalation:\n",
      "[start] o que vocês faz tudo [end]\n",
      "--------------------------------------------------\n",
      "English sentence:\n",
      "Don't do it!\n",
      "Portuguese transalation:\n",
      "[start] não faça isso [end]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "import keras\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "transformer = keras.models.load_model(\"models\\\\transformer-eng-pot.keras\", \n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder, \n",
    "        \"PositionalEmbedding\": PositionalEmbedding,\n",
    "        \"TransformerDecoder\": TransformerDecoder})\n",
    "\n",
    "with open('data_text\\portuguese_vocabulary.txt', encoding='utf-8', errors='backslashreplace') as file_in:\n",
    "    portuguese_vocab = [line.strip() for line in file_in]\n",
    "\n",
    "with open('data_text\\english_vocabulary.txt', encoding='utf-8', errors='backslashreplace') as file_in:\n",
    "    english_vocab = [line.strip() for line in file_in]\n",
    "\n",
    "from updated_TextVectorization import TextVectorization\n",
    "\n",
    "target_vectorization = TextVectorization(max_tokens=20000,\n",
    "                                        output_mode=\"int\",\n",
    "                                        output_sequence_length=21,\n",
    "                                        standardize=custom_standardization,\n",
    "                                        vocabulary=portuguese_vocab,\n",
    "                                        encoding='iso-8859-1')\n",
    "\n",
    "source_vectorization = TextVectorization(max_tokens=20000,\n",
    "                                        output_mode=\"int\",\n",
    "                                        output_sequence_length=20,\n",
    "                                        vocabulary=english_vocab,\n",
    "                                        encoding='iso-8859-1')\n",
    "\n",
    "portuguese_index_lookup = dict(zip(range(len(portuguese_vocab)), portuguese_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = portuguese_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "eng_sentences =[\"What is its name?\",\n",
    "                \"How old are you?\",\n",
    "                \"I know you know where Mary is.\",\n",
    "                \"We will show Tom.\",\n",
    "                \"What do you all do?\",\n",
    "                \"Don't do it!\"]\n",
    "\n",
    "for sentence in eng_sentences:\n",
    "    print(f\"English sentence:\\n{sentence}\")\n",
    "    print(f'Portuguese transalation:\\n{decode_sequence(sentence)}')\n",
    "    print('-' * 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can now compare the output of both models, and the Transformer (being lighter and faster to train) shows a better performance than the RNN. However, there is a problem with the translation of the first sentence.**\n",
    "\n",
    "**While the RNN translates `What is its name?` to `[start] o que é o seu nome [end]`, the transformer model makes a gender assumption, even though the source sentence wasn’t gendered (`[start] qual é o nome dele [end]`). Errors like these are not uncommon in NLP, algorithmic bias being one of the great problems associated with the use of language models in real applications.**\n",
    "\n",
    "**In AI literature, we call this a \"_hallucination_.\" When a machine learning (ML) model \"hallucinates\" something, it means that the model is producing output that is not based on the input data and is not representative of the underlying distribution of the data. This can occur when the model is overfitting to the training data and has learned to memorize specific examples rather than generalizing to new data.**\n",
    "\n",
    "**Hallucinations can occur in a variety of forms, depending on the type of ML model and the task it is trained to perform. For example, in the case of image classification, a model might hallucinate an object or pattern that is not present in the input image. In natural language processing, a model might generate nonsensical or unrelated text.**\n",
    "\n",
    "**Hallucinations can be a sign of overfitting and can indicate that the model is not generalizing well to new data. To prevent hallucinations, it is important to use techniques such as regularization and early stopping to prevent overfitting and to train models with curated, high-quality, datasets to prevent the positioning of the model with unwanted biases.**\n",
    "\n",
    "**Congratulations! You built your first transformational model!** 🤖\n",
    "\n",
    "---\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
