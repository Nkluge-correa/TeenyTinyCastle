{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $n$-armed bandit (RL 101)\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n",
    "\n",
    "**In _probability theory_ and _machine learning_, the multi-armed bandit problem (sometimes called $n$-armed bandit problem) is a problem in which a fixed limited set of resources must be allocated between competing (alternative) choices in a way that maximizes their expected gain, when each choice's properties are only partially known at the time of allocation, and may become better understood as time passes or by allocating resources to the choice.**\n",
    "\n",
    "![image](https://miro.medium.com/max/1296/1*qh46a_qurQk30SCet6oWDg.gif)\n",
    "\n",
    "**This is a classic reinforcement learning problem that exemplifies the exploration–exploitation tradeoff dilemma.**\n",
    "\n",
    "- **exploration**: _execute a random action with an uncertain reward._\n",
    "- **exploitation**: _execute a known action with a known reward_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below we have a series of levers, each with a _value associated_ with it.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandits_lever = [2., 1., 0, -3]\n",
    "num_bandits = len(bandits_lever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below is a function that controls whether pulling a lever will reward you ($1$) or not ($-1$). This reward depends (_stochastically_) on the _random number generated_ and the _value of each bandit_ (some bandits have a higher chance of giving a reward).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value for the 1 pull is -0.21225560503673163\n",
      "Value for the 2 pull is -0.6410835849018122\n",
      "Value for the 3 pull is -0.45515288898808837\n",
      "Value for the 4 pull is 0.17768401778407147\n",
      "Value for the 5 pull is -0.988727195714711\n",
      "Value for the 6 pull is -0.7139332953504195\n",
      "Value for the 7 pull is 2.2838139900033383\n",
      "Value for the 8 pull is -2.4487209308777436\n",
      "Value for the 9 pull is 0.8393497460413712\n",
      "Value for the 10 pull is -1.2287423559770971\n"
     ]
    }
   ],
   "source": [
    "def pullBandit(bandit):\n",
    "    # Returns a sample from the “standard normal” distribution, mean 0, varaiance 1.\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        # return a positive reward.\n",
    "        return 1\n",
    "    else:\n",
    "        # return a negative reward.\n",
    "        return -1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'Value for the {i+1} pull is {np.random.randn(1)[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build a simple _feed-forward network_, with a number of input neurons and weights equal to the number of bandits, and one output neuron that _chooses which bandit to pull_.\n",
    "\n",
    "**The agent will, in `10,000 episodes`, explore-exploit all possible bandits, and in the end (if successful) _converge on using the bandit that suits him best_ (higher chance of receiving a reward).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward for the 4 bandit: [-1.  0.  0.  0.]\n",
      "Running reward for the 4 bandit: [ 0. -3. -1. 83.]\n",
      "Running reward for the 4 bandit: [ -3.  -6.  -2. 176.]\n",
      "Running reward for the 4 bandit: [ -4. -11.  -3. 267.]\n",
      "Running reward for the 4 bandit: [ -8. -14.  -2. 359.]\n",
      "Running reward for the 4 bandit: [ -9. -17.  -2. 453.]\n",
      "Running reward for the 4 bandit: [-11. -18.  -3. 541.]\n",
      "Running reward for the 4 bandit: [-14. -20.  -1. 634.]\n",
      "Running reward for the 4 bandit: [-17. -21.  -1. 730.]\n",
      "Running reward for the 4 bandit: [-18. -23.   1. 821.]\n",
      "Running reward for the 4 bandit: [-19. -25.   1. 916.]\n",
      "Running reward for the 4 bandit: [ -20.  -26.    4. 1009.]\n",
      "Running reward for the 4 bandit: [ -22.  -25.    6. 1102.]\n",
      "Running reward for the 4 bandit: [-2.300e+01 -2.800e+01  1.000e+00  1.193e+03]\n",
      "Running reward for the 4 bandit: [ -28.  -29.    3. 1283.]\n",
      "Running reward for the 4 bandit: [ -31.  -29.    5. 1372.]\n",
      "Running reward for the 4 bandit: [ -32.  -29.    3. 1467.]\n",
      "Running reward for the 4 bandit: [ -35.  -29.    2. 1559.]\n",
      "Running reward for the 4 bandit: [-3.700e+01 -2.900e+01  1.000e+00  1.654e+03]\n",
      "Running reward for the 4 bandit: [ -41.  -31.    0. 1743.]\n",
      "Running reward for the 4 bandit: [-4.30e+01 -3.10e+01 -1.00e+00  1.84e+03]\n",
      "Running reward for the 4 bandit: [ -47.  -32.   -4. 1928.]\n",
      "Running reward for the 4 bandit: [ -48.  -35.   -6. 2020.]\n",
      "Running reward for the 4 bandit: [ -52.  -39.   -6. 2112.]\n",
      "Running reward for the 4 bandit: [ -54.  -40.   -9. 2206.]\n",
      "Running reward for the 4 bandit: [ -57.  -44.   -9. 2299.]\n",
      "Running reward for the 4 bandit: [ -60.  -46.   -9. 2390.]\n",
      "Running reward for the 4 bandit: [ -62.  -47.   -9. 2479.]\n",
      "Running reward for the 4 bandit: [ -62.  -50.  -10. 2571.]\n",
      "Running reward for the 4 bandit: [ -67.  -52.  -11. 2661.]\n",
      "Running reward for the 4 bandit: [ -68.  -54.  -12. 2757.]\n",
      "Running reward for the 4 bandit: [ -70.  -54.  -12. 2849.]\n",
      "Running reward for the 4 bandit: [ -72.  -54.   -8. 2939.]\n",
      "Running reward for the 4 bandit: [ -73.  -56.  -10. 3034.]\n",
      "Running reward for the 4 bandit: [ -76.  -58.  -12. 3127.]\n",
      "Running reward for the 4 bandit: [ -80.  -60.  -12. 3219.]\n",
      "Running reward for the 4 bandit: [ -82.  -63.  -13. 3309.]\n",
      "Running reward for the 4 bandit: [ -84.  -65.  -16. 3398.]\n",
      "Running reward for the 4 bandit: [ -88.  -69.  -16. 3488.]\n",
      "Running reward for the 4 bandit: [ -93.  -73.  -17. 3572.]\n",
      "Running reward for the 4 bandit: [ -94.  -77.  -19. 3665.]\n",
      "Running reward for the 4 bandit: [ -97.  -81.  -18. 3755.]\n",
      "Running reward for the 4 bandit: [ -98.  -82.  -17. 3850.]\n",
      "Running reward for the 4 bandit: [-101.  -83.  -16. 3943.]\n",
      "Running reward for the 4 bandit: [-102.  -86.  -17. 4038.]\n",
      "Running reward for the 4 bandit: [-105.  -92.  -19. 4127.]\n",
      "Running reward for the 4 bandit: [-108.  -95.  -19. 4215.]\n",
      "Running reward for the 4 bandit: [-113.  -97.  -21. 4304.]\n",
      "Running reward for the 4 bandit: [-114. -101.  -21. 4397.]\n",
      "Running reward for the 4 bandit: [-114. -105.  -20. 4486.]\n",
      "Running reward for the 4 bandit: [-116. -105.  -20. 4580.]\n",
      "Running reward for the 4 bandit: [-117. -106.  -22. 4674.]\n",
      "Running reward for the 4 bandit: [-123. -108.  -20. 4760.]\n",
      "Running reward for the 4 bandit: [-125. -110.  -16. 4852.]\n",
      "Running reward for the 4 bandit: [-126. -111.  -15. 4947.]\n",
      "Running reward for the 4 bandit: [-128. -115.  -15. 5039.]\n",
      "Running reward for the 4 bandit: [-129. -114.  -14. 5136.]\n",
      "Running reward for the 4 bandit: [-131. -115.  -13. 5230.]\n",
      "Running reward for the 4 bandit: [-133. -115.  -14. 5321.]\n",
      "Running reward for the 4 bandit: [-136. -116.  -14. 5413.]\n",
      "Running reward for the 4 bandit: [-140. -118.  -14. 5505.]\n",
      "Running reward for the 4 bandit: [-140. -120.  -15. 5600.]\n",
      "Running reward for the 4 bandit: [-140. -120.  -14. 5699.]\n",
      "Running reward for the 4 bandit: [-146. -119.  -13. 5785.]\n",
      "Running reward for the 4 bandit: [-147. -121.   -8. 5875.]\n",
      "Running reward for the 4 bandit: [-147. -121.   -7. 5968.]\n",
      "Running reward for the 4 bandit: [-1.480e+02 -1.210e+02 -5.000e+00  6.063e+03]\n",
      "Running reward for the 4 bandit: [-1.490e+02 -1.200e+02 -5.000e+00  6.161e+03]\n",
      "Running reward for the 4 bandit: [-152. -118.   -8. 6251.]\n",
      "Running reward for the 4 bandit: [-153. -120.  -11. 6343.]\n",
      "Running reward for the 4 bandit: [-156. -123.  -11. 6437.]\n",
      "Running reward for the 4 bandit: [-157. -124.  -11. 6535.]\n",
      "Running reward for the 4 bandit: [-160. -124.  -11. 6626.]\n",
      "Running reward for the 4 bandit: [-161. -125.  -13. 6718.]\n",
      "Running reward for the 4 bandit: [-162. -126.  -15. 6812.]\n",
      "Running reward for the 4 bandit: [-163. -127.  -15. 6910.]\n",
      "Running reward for the 4 bandit: [-166. -129.  -13. 7003.]\n",
      "Running reward for the 4 bandit: [-168. -130.  -12. 7095.]\n",
      "Running reward for the 4 bandit: [-170. -133.  -11. 7187.]\n",
      "Running reward for the 4 bandit: [-172. -131.  -10. 7280.]\n",
      "Running reward for the 4 bandit: [-174. -131.   -8. 7374.]\n",
      "Running reward for the 4 bandit: [-174. -134.  -12. 7463.]\n",
      "Running reward for the 4 bandit: [-174. -134.  -14. 7557.]\n",
      "Running reward for the 4 bandit: [-175. -136.  -14. 7650.]\n",
      "Running reward for the 4 bandit: [-176. -137.  -14. 7748.]\n",
      "Running reward for the 4 bandit: [-181. -138.  -14. 7842.]\n",
      "Running reward for the 4 bandit: [-183. -139.  -14. 7939.]\n",
      "Running reward for the 4 bandit: [-189. -140.  -17. 8025.]\n",
      "Running reward for the 4 bandit: [-190. -143.  -15. 8117.]\n",
      "Running reward for the 4 bandit: [-194. -144.  -17. 8206.]\n",
      "Running reward for the 4 bandit: [-196. -143.  -18. 8302.]\n",
      "Running reward for the 4 bandit: [-199. -144.  -17. 8393.]\n",
      "Running reward for the 4 bandit: [-201. -146.  -18. 8482.]\n",
      "Running reward for the 4 bandit: [-202. -148.  -20. 8577.]\n",
      "Running reward for the 4 bandit: [-204. -149.  -20. 8672.]\n",
      "Running reward for the 4 bandit: [-205. -150.  -19. 8765.]\n",
      "Running reward for the 4 bandit: [-207. -150.  -18. 8858.]\n",
      "Running reward for the 4 bandit: [-210. -152.  -22. 8949.]\n",
      "Running reward for the 4 bandit: [-211. -155.  -22. 9043.]\n",
      "Running reward for the 4 bandit: [-214. -158.  -24. 9135.]\n",
      "The agent thinks bandit 4 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weights = tf.Variable(tf.ones([num_bandits]))\n",
    "chosen_action = tf.argmax(weights, 0)\n",
    "\n",
    "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "responsible_weight = tf.slice(weights, action_holder, [1])\n",
    "loss = -(tf.log(responsible_weight)*reward_holder)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "update = optimizer.minimize(loss)\n",
    "total_episodes = 10000\n",
    "total_reward = np.zeros(num_bandits)\n",
    "e = 0.1  # epsilon refers to the probability of choosing to explore\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sech:\n",
    "    sech.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:\n",
    "        if np.random.rand(1) < e:\n",
    "            # Explore\n",
    "            action = np.random.randint(num_bandits)\n",
    "        else:\n",
    "            # Exploit\n",
    "            action = sech.run(chosen_action)\n",
    "\n",
    "        # Get reward\n",
    "        reward = pullBandit(bandits_lever[action])\n",
    "\n",
    "        # Update the network.\n",
    "        _, resp, ww = sech.run([update, responsible_weight, weights], feed_dict={\n",
    "                               reward_holder: [reward], action_holder: [action]})\n",
    "\n",
    "        # Update the score board\n",
    "        total_reward[action] += reward\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f'Running reward for the {num_bandits} bandit: {np.rint(total_reward)}')\n",
    "        i += 1\n",
    "print(f'The agent thinks bandit {np.argmax(ww)+1} is the most promising....')\n",
    "\n",
    "if np.argmax(ww) == np.argmax(-np.array(bandits_lever)):\n",
    "    print('...and it was right!')\n",
    "else:\n",
    "    print('...and it was wrong!')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
