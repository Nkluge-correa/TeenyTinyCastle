{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with `Transformer` Models \n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n",
    "\n",
    "**Text classification is the process of categorizing text into predefined classes or categories based on its content. One common classification task in NLP is toxicity detection.**\n",
    "\n",
    "**This is a common application of text classification in natural language processing, where the objective is to identify whether a piece of text contains language that is offensive, harmful, or threatening. To accomplish this, machine learning models are trained on large datasets of labeled toxic and non-toxic text examples and then used to predict the toxicity of new, unlabeled text.**\n",
    "\n",
    "**We can define toxicity as:**\n",
    "\n",
    "> **\"_abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation._\"**\n",
    "\n",
    "**For an in-depth discussion on toxicity detection as a machine learning problem, we recommend \"_[Learning from the worst: Dynamically generated datasets to improve online hate detection](https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2012.15761&hl=pt-BR&sa=T&oi=gsb&ct=res&cd=0&d=7265559494033067667&ei=QUJYY6TJL4iKmgHXk5LgDQ&scisig=AAGBfm3gsyOD5eqcUPLFvWmVm8PlLcMr3g)_\".**\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*d4k-PRw-warACDpklCh1mw.png\" alt=\"toxic-image\" width=\"800\"/>\n",
    "\n",
    "**In this notebook, we will be using a dataset created from the [Toxic Comment Classification Challenge Dataset](https://github.com/tianqwang/Toxic-Comment-Classification-Challenge), created by the [Conversation AI](https://conversationai.github.io/) team, a research initiative founded by [Jigsaw](https://jigsaw.google.com/) and Google (both a part of Alphabet).**\n",
    "\n",
    "## ☣️ DISCLAIMER/WARNING ☣️ \n",
    "\n",
    "**_This dataset contains text that may be considered profane, vulgar, or offensive. The vocabulary of the trained model also contains this type of language._**\n",
    "\n",
    "**The original dataset contains an unequal distribution of “_hate_” and “_not hate_” samples for multi-classification. However, we created a smaller version of the original dataset (the `toxic_content_dataset.csv`, available for download in [this link](https://drive.google.com/uc?export=download&id=1ZvZtrsE1dAl7CiHt16Jstp2rhkDKnGlL)). The used dataset contains an equal amount of “_hate_” and “_not hate_” samples, summing up to $70157$ samples total.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aww matches background colour seemingly stuck ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70152</th>\n",
       "      <td>lol gay never know good feels fuck woman ass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70153</th>\n",
       "      <td>fuck pansy jew would whine nai brith beat pale...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70154</th>\n",
       "      <td>shalom semite get fuck kill son bitch leave wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70155</th>\n",
       "      <td>think gay fag</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70156</th>\n",
       "      <td>previous conversation fucking shit eating libe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70157 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comment_text  toxic\n",
       "0      explanation edits made username hardcore metal...      1\n",
       "1      aww matches background colour seemingly stuck ...      1\n",
       "2      hey man really trying edit war guy constantly ...      1\n",
       "3      make real suggestions improvement wondered sec...      1\n",
       "4                          sir hero chance remember page      1\n",
       "...                                                  ...    ...\n",
       "70152       lol gay never know good feels fuck woman ass      0\n",
       "70153  fuck pansy jew would whine nai brith beat pale...      0\n",
       "70154  shalom semite get fuck kill son bitch leave wi...      0\n",
       "70155                                      think gay fag      0\n",
       "70156  previous conversation fucking shit eating libe...      0\n",
       "\n",
       "[70157 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/toxic_content_dataset.csv\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The original dataset had a lot of symbols and emojis that may hinder the performance of a language model trained for text classification. However, we already performed the cleaning procedure. Below you can find the funciton that we used to function to `preprocess` text data, removing unwanted characters and so on.**\n",
    "\n",
    "```python\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    clean_text = input_data.lower().replace(\"<br />\", \" \")\n",
    "    clean_text = re.sub(r\"[-()\\\"#/@;:<>{}=~|.?,]\", ' ', clean_text)\n",
    "    clean_text = re.sub(' +', ' ', clean_text)\n",
    "    return unidecode(clean_text)\n",
    "\n",
    "```\n",
    "\n",
    "**Text is one of the most widespread forms of sequence data and discrete signals (as opposed to continuous signals, like _images_ or _audio_). These sequences can be sequences of characters, syllables, or words.**\n",
    "\n",
    "**Deep learning for NLP is pattern recognition applied to paragraphs, sentences, and words, just as computer vision is pattern recognition applied to videos, images, and pixels.**\n",
    "\n",
    "**Like all neural networks, language models based on deep-learning architectures don’t take as input raw text, i.e., you _can not multiply a word by a weight matrix, add a bias, and apply a ReLU function at the end_. Neural networks only work with numeric tensors. Thus, we need to _vectorize_ our text data, i.e., transform the text into numeric tensors.**\n",
    "\n",
    "**For a comprehensive guide on how to vectorize text data, we recommend Chapter 6: Deep learning for text and sequences, in [_Deep Learning with Python_](https://tanthiamhuat.files.wordpress.com/2018/03/deeplearningwithpython.pdf). Below we will be using the `TextVectorization` layer from the [Keras](https://keras.io/) library.** \n",
    "\n",
    "**In terms of preprocessing, you can also pass symbols you may want to filter, by using the `filters` argument.**\n",
    "\n",
    "**Finally, we will split our dataset for training/validation/testing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# number of words in our vocabulary\n",
    "vocab_size = 20000\n",
    "\n",
    "# sentences lengthier than 100 will be truncated\n",
    "sequence_length = 100\n",
    "\n",
    "vectorization_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length\n",
    "    )\n",
    "\n",
    "vectorization_layer.adapt(df.comment_text)\n",
    "vocabulary = vectorization_layer.get_vocabulary()\n",
    "\n",
    "\n",
    "# Here we save our vocabulary for later use\n",
    "# DISCLAIMER - THIS VOCABULARY CONTAINS TOXIC LANGUAGE\n",
    "with open(r'models/toxic_vocabulary.txt', 'w', encoding='utf-8') as fp:\n",
    "    for word in vocabulary:\n",
    "        fp.write(\"%s\\n\" % word)\n",
    "    fp.close()\n",
    "\n",
    "# `train_test_split` is a grate way to slip datasets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.comment_text, df.toxic, test_size=0.2, random_state=42)\n",
    "\n",
    "# vectorize senteces and turn label into `floats`\n",
    "x_train = vectorization_layer(x_train)\n",
    "y_train = np.array(y_train).astype(float)\n",
    "x_test = vectorization_layer(x_test)\n",
    "y_test = np.array(y_test).astype(float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_networks \"Recurrent neural networks\"), like `LSTM` and `GRU`, and convolutional networks, like `1D Convnets`, are great options for dealing with problems involving NLP. In [other notebooks](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/fa17764aa8800c388d0d298b750c686757e0861e/ML%20Explainability/NLP%20Interpreter/model_maker.ipynb) of our repository, you will see many examples of how to build these networks for tasks like sentiment analysis.**\n",
    "\n",
    "**However, in this notebook, we will be using a `transformer` model, an extremely versatile and scalable architecture proposed by Vaswani et al. in [Attention Is All You Need](https://arxiv.org/abs/1706.03762).**\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" alt=\"drawing\" height=\"450\"/>\n",
    "\n",
    "**A `transformer` is a  deep learning model that adopts the mechanism of [self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning)), differentially weighting the significance of each part of the input data. Like RNNs, transformers are designed to process sequential input data. However, unlike RNNs, transformers process the entire input all at once (_not sequencially_). The transformer does not have to process one word at a time. This allows for more parallelization, thus reducing training times, and also allowing the training on larger datasets.**\n",
    "\n",
    "**For an extremely _comprehensive_ and _ilustrated_ explanation of what is \"_attention_\" or how a \"_transformer works_\", we recommend the work of _Jay Alammar_:**\n",
    "\n",
    "- **[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/).**\n",
    "- **[The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/).**\n",
    "\n",
    "**Using only the _decoder_ component of the original transformer architecture, we will implement a small transformer model in this notebook (a.k.a. a decoder-only transformer). The original transformer architecture, which consists of both an encoder and a decoder transformer block, was (originally) designed for _sequence-to-sequence_ tasks like translation.**\n",
    "\n",
    "**If you are interested in learning about the transformer architecture, check our _[sequence-to-sequence](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/fa17764aa8800c388d0d298b750c686757e0861e/ML%20Intro%20Course/16_seuqnece_to_sequence.ipynb)_ tutorial.**\n",
    "\n",
    "**In general, text classification can be done using the encoder component. It's a very generic module that learns to transform a sequence into a more useful representation after ingesting it.**\n",
    "\n",
    "**This model only has 4 attention heads with a capacity of 100 tokens and 4 transformer blocks. Our embedding layer's size is also restricted to embeddings with 128 dimensions and a vocabulary of 20,000 tokens (where the dense word vectors will be created).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.10.1\n",
      "Eager mode:  True\n",
      "GPU is available\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 100, 128)     2560000     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 100, 128)    256         ['embedding[0][0]']              \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 100, 128)    527488      ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 100, 128)     0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 100, 128)    0           ['dropout[0][0]',                \n",
      " da)                                                              'embedding[0][0]']              \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 100, 128)    256         ['tf.__operators__.add[0][0]']   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 100, 4)       516         ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 100, 4)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 100, 128)     640         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 100, 128)    0           ['conv1d_1[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 100, 128)    256         ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 100, 128)    527488      ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 100, 128)     0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 100, 128)    0           ['dropout_2[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 100, 128)    256         ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 100, 4)       516         ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 100, 4)       0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 100, 128)     640         ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 100, 128)    0           ['conv1d_3[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 100, 128)    256         ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 100, 128)    527488      ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 100, 128)     0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 100, 128)    0           ['dropout_4[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 100, 128)    256         ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 100, 4)       516         ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 100, 4)       0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 100, 128)     640         ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 100, 128)    0           ['conv1d_5[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 100, 128)    256         ['tf.__operators__.add_5[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 100, 128)    527488      ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 100, 128)     0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 100, 128)    0           ['dropout_6[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 100, 128)    256         ['tf.__operators__.add_6[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 100, 4)       516         ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 100, 4)       0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 100, 128)     640         ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 100, 128)    0           ['conv1d_7[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 100)         0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          12928       ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,689,681\n",
      "Trainable params: 4,689,681\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "2807/2807 [==============================] - 270s 95ms/step - loss: 0.2700 - accuracy: 0.8937 - val_loss: 0.1918 - val_accuracy: 0.9303\n",
      "Epoch 2/20\n",
      "2807/2807 [==============================] - 271s 97ms/step - loss: 0.1881 - accuracy: 0.9314 - val_loss: 0.1797 - val_accuracy: 0.9337\n",
      "Epoch 3/20\n",
      "2807/2807 [==============================] - 272s 97ms/step - loss: 0.1639 - accuracy: 0.9419 - val_loss: 0.2098 - val_accuracy: 0.9273\n",
      "Epoch 4/20\n",
      "2807/2807 [==============================] - 270s 96ms/step - loss: 0.1550 - accuracy: 0.9439 - val_loss: 0.1926 - val_accuracy: 0.9345\n",
      "Epoch 5/20\n",
      "2807/2807 [==============================] - 267s 95ms/step - loss: 0.1525 - accuracy: 0.9461 - val_loss: 0.2172 - val_accuracy: 0.9305\n",
      "Epoch 6/20\n",
      "2807/2807 [==============================] - 267s 95ms/step - loss: 0.1537 - accuracy: 0.9451 - val_loss: 0.2201 - val_accuracy: 0.9188\n",
      "Epoch 7/20\n",
      "2806/2807 [============================>.] - ETA: 0s - loss: 0.1473 - accuracy: 0.9471Restoring model weights from the end of the best epoch: 2.\n",
      "2807/2807 [==============================] - 267s 95ms/step - loss: 0.1473 - accuracy: 0.9471 - val_loss: 0.2765 - val_accuracy: 0.9310\n",
      "Epoch 7: early stopping\n",
      "439/439 [==============================] - 25s 57ms/step - loss: 0.1906 - accuracy: 0.9316\n",
      "Final Loss: 0.19.\n",
      "Final Performance: 93.16 %.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0):\n",
    "\n",
    "    vocab_size = 20000\n",
    "    sequence_length = 100\n",
    "    embed_size = 128\n",
    "\n",
    "    inputs = tf.keras.Input(shape=input_shape, dtype=\"int32\")\n",
    "    x = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                              output_dim=embed_size,\n",
    "                              input_length=sequence_length)(inputs)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = build_model(\n",
    "    (x_train.shape[1]),\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "model.compile(loss=tf.losses.BinaryCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.ModelCheckpoint(\"models/toxicity_model.keras\",\n",
    "                                                save_best_only=True),\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                            patience=5,\n",
    "                                            verbose=1,\n",
    "                                            mode=\"auto\",\n",
    "                                            baseline=None,\n",
    "                                            restore_best_weights=True)]\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          validation_split = 0.2,\n",
    "          epochs=20,\n",
    "          batch_size=16,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "test_loss_score, test_acc_score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'Final Loss: {round(test_loss_score, 2)}.')\n",
    "print(f'Final Performance: {round(test_acc_score * 100, 2)} %.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bellow we can test our trained model. You can also download the trained model (+ learned vocabulary) in [this link](https://drive.google.com/uc?export=download&id=1slnxFW9cP6GwUksnCYhuqRfVAAjfiJG_).** 🙃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I think you should shut up your big mouth\n",
      "\n",
      "Toxic 🤬 95.73% | Not toxic 😊 4.27\n",
      "__________________________________________________\n",
      "\n",
      "I do not agree with you\n",
      "\n",
      "Toxic 🤬 0.99% | Not toxic 😊 99.01\n",
      "__________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('models/toxicity_model.keras')\n",
    "\n",
    "with open('models/toxic_vocabulary.txt', encoding='utf-8') as fp:\n",
    "    vocabulary = [line.strip() for line in fp]\n",
    "    fp.close()\n",
    "\n",
    "vectorization_layer = tf.keras.layers.TextVectorization(max_tokens=20000,\n",
    "                                        output_mode=\"int\",\n",
    "                                        output_sequence_length=100,\n",
    "                                        vocabulary=vocabulary)\n",
    "\n",
    "strings = [\n",
    "    'I think you should shut up your big mouth',\n",
    "    'I do not agree with you'\n",
    "]\n",
    "\n",
    "preds = model.predict(vectorization_layer(strings),verbose=0)\n",
    "\n",
    "for i, string in enumerate(strings):\n",
    "    print(f'{string}\\n')\n",
    "    print(f'Toxic 🤬 {round((1 - preds[i][0]) * 100, 2)}% | Not toxic 😊 {round(preds[i][0] * 100, 2)}\\n')\n",
    "    print(\"_\" * 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can try to repurpose this architecture for other applications and tasks, like multi-classification (changing the function of the last layer to a `softmax` with the intended number of unique classes to your problem) instead of binary classification.**\n",
    "\n",
    "---\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
