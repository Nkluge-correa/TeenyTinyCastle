{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Text Generation with `transformer` models \n","\n","Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n","\n","`Text generation` models are a type of machine learning model that can generate natural language text. These models have a wide range of applications, including `language translation`, `summarization`, and `content creation`. There are several different approaches to text generation, including `statistical models`, `rule-based systems, and, more recently, `neural network-based models`. These models can be trained on large amounts of input text data and use this information to generate new text that is coherent and reflects the patterns and structures found in the training data. The quality of the generated text can vary depending on the _complexity of the model and the amount and quality of the training data_. Overall, text-generation models have the potential to revolutionize many industries by automating the creation of written content and enabling more efficient communication.\n","\n","One of the biggest and most current advances in language modeling was made possible by the invention of the `transformer` architecture. A `transformer` model is a type of neural network architecture first described in the 2017 paper \"_[Attention Is All You Need](https://arxiv.org/abs/1706.03762)_\" by Vaswani et al. Transformers were originally used for machine translation, but this adaptable architecture is now used in a wide range of fields and problems.\n","\n","<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" alt=\"drawing\" height=\"450\"/>\n","\n","Before the `transformer`, most language models used recurrent neural networks ([`RNNs`](https://en.wikipedia.org/wiki/Recurrent_neural_network)) to process language data. While effective, `RNNs` can be slow and difficult to parallelize, limiting their ability to scale to large datasets. The `transformer` architecture was designed to overcome these limitations and efficiently process long data sequences. It does this using `self-attention` mechanisms, which allow the model to weigh different input elements differently and consider the relationships between all elements in the input sequence simultaneously. This allows the `transformer` to process input data in parallel rather than sequentially like an `RNN`.\n","\n","The `transformer` architecture has enabled the development of powerful language models such as [`BERT`](https://huggingface.co/docs/transformers/model_doc/bert) and [`GPT-3`](https://arxiv.org/abs/2005.14165), which have achieved state-of-the-art results on a wide range of natural language processing tasks. \n","\n","To learn more about the original `transformer` architecture, go to our [`sequence-to-sequence` machine translation](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/48d415094d30e0e5bc8dde32715bb57428a87d7d/ML%20Intro%20Course/16_sequence_to_sequence.ipynb) notebook.\n","\n","In this notebook, we will implement a `decoder-only transformer` for text generation. `Decoder-only transformers`, such as the `GPT` (_Generative Pre-training Transformer_) series, are models that consist only of a `decoder`, which takes an input sequence and generates an output sequence. `Decoder-only transformers` have generated coherent text, including entire articles and stories. One advantage of decoder-only transformers is that they are relatively simple and easy to train, as they do not require an `encoder` component. \n","\n","To start building our model, we first need _good-quality text_. Good-quality text is important for the training of language models for several reasons. For example, good-quality text is more likely to reflect the real-world patterns and structures of language, which is important for the model to learn. A model trained on poorly written or grammatically incorrect text may struggle to generate correct and coherent output. Also, a model trained on large amounts of high-quality text may be able to learn language patterns more quickly and with fewer resources than a model trained on low-quality text.\n","\n","We will create a text dataset using the articles from the [`Stanford Encyclopedia of Philosophy`](https://plato.stanford.edu/). Details on the created text corpus can be found on this [markdown file](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/fa17764aa8800c388d0d298b750c686757e0861e/ML%20Intro%20Course/data/Stanford_Encyclopedia_of_Philosophy_Text_Corpus_v0.1.md), and the full dataset can be downloaded in [this link](https://drive.google.com/uc?export=download&id=167nlUxmY0Qc9_wGMuyMqgh2pUwyJDrte).\n","\n","## Getting a Text Corpus\n","\n","Web scraping is the process of extracting information from websites. This can be done using a variety of programming languages and tools, such as Python and its libraries for web scraping, such as `BeautifulSoup` and `Scrapy`.\n","\n","Web scraping can be useful but can be done in an unethical way and even illegally, so it is important to be aware of the website's terms of use before scraping. Many websites have terms of service that prohibit scraping, so it's important to review the terms of service of a website before scraping it.\n","\n","If we check the [`robots.txt`](https://plato.stanford.edu/robots.txt) file of the SEP, we see what we are allowed to do, and the following code will only scrape permissible content of the SEP.\n","\n","In this example, we will use `BeautifulSoup` to get our text data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","import requests\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","\n","\n","# Set the URL to scrape\n","url = \"https://plato.stanford.edu/contents.html\"\n","\n","# Send a request to the URL and retrieve the webpage\n","page = requests.get(url)\n","\n","# Parse the webpage with BeautifulSoup\n","soup = BeautifulSoup(page.text, \"html.parser\")\n","\n","# parse the href addresses and anchors of the html page\n","definitions = soup.find_all(\"a\")\n","quoted = re.compile('\"[^\"]*\"')\n","\n","entries = []\n","for definition in definitions:\n","    definition = str(definition)\n","\n","    # get all the addresses of the links in the 'contents.html' page\n","    for value in quoted.findall(definition):\n","\n","        # get all pages, that have philosophical text\n","        if value[1:-1].startswith(\"entries\"):\n","            entries.append(value[1:-1])\n","\n","# list of all the texts\n","paragraphs = []\n","\n","# list of all the source pages\n","source = []"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now that we have all the entries, we can begin to extract the text from them (note that the entries are allowed according to the [`robots.txt`](https://plato.stanford.edu/robots.txt)). \n","\n","⚠️ ALWAYS BE SURE YOU ARE ALLOWED TO SCRAPE. ⚠️"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f'Number of content pages: {len(entries)}')\n","# loop over all the pages that have philosophical text\n","for i, entri in enumerate(entries):\n","    url = f\"https://plato.stanford.edu/{entri}\"\n","\n","    # Send a request to the URL and retrieve the webpage\n","    page = requests.get(url)\n","\n","    # Parse the webpage with BeautifulSoup\n","    soup = BeautifulSoup(page.text, \"html.parser\")\n","\n","    # Get all the <p> tags from the parsed html\n","    texts = soup.find_all(\"p\")\n","\n","    # Loop over all the <p> tags from the parsed html\n","    for text in texts:\n","\n","        # remove the html tags from the string\n","        clean_string = re.sub(r'<[^>]*>', '', str(text))\n","\n","        # replace the '\\n' with \" \"\n","        clean_string = clean_string.replace(\"\\n\", \" \")\n","\n","        # append the source and text elements\n","        paragraphs.append(clean_string)\n","        source.append(url)\n","\n","    print(f\"Page {i + 1}. Scrapped page '{url}'!\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, let us create a dataframe with all of our text corpus."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# create a pandas data frame with the data\n","df = pd.DataFrame({'URL': source, 'Text': paragraphs})\n","\n","# drop duplicate text\n","df = df.drop_duplicates()\n","\n","# Clean the URL to get the \"Category\" of the page\n","def clean_url(string):\n","    return string.split('entries/')[1][:-1]\n","\n","# Apply the function to the \"URL\" column\n","df['Category'] = df['URL'].apply(clean_url)\n","\n","# save as a csv file\n","df.to_csv('SEP/stanford_encyclopedia_philosophy.csv', index=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["And here is our text corpus. You can download it directly using [this link](https://drive.google.com/uc?export=download&id=167nlUxmY0Qc9_wGMuyMqgh2pUwyJDrte). Avoid scrapping websites that have already been scrapped!"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>URL</th>\n","      <th>Text</th>\n","      <th>Category</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>https://plato.stanford.edu/entries/abduction/</td>\n","      <td>In the philosophical literature, the term “ab...</td>\n","      <td>abduction</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>https://plato.stanford.edu/entries/abduction/</td>\n","      <td>This entry is exclusively concerned with abdu...</td>\n","      <td>abduction</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>https://plato.stanford.edu/entries/abduction/</td>\n","      <td>See also the entry on  scientific discovery, ...</td>\n","      <td>abduction</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>https://plato.stanford.edu/entries/abduction/</td>\n","      <td>Most philosophers agree that abduction (in th...</td>\n","      <td>abduction</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>https://plato.stanford.edu/entries/abduction/</td>\n","      <td>You happen to know that Tim and Harry have re...</td>\n","      <td>abduction</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>182526</th>\n","      <td>https://plato.stanford.edu/entries/zombies/</td>\n","      <td>Many thanks to David Chalmers and to Bill Fis...</td>\n","      <td>zombies</td>\n","    </tr>\n","    <tr>\n","      <th>182527</th>\n","      <td>https://plato.stanford.edu/entries/zombies/</td>\n","      <td>Copyright © 2019 by   Robert Kirk &amp;lt;Robert....</td>\n","      <td>zombies</td>\n","    </tr>\n","    <tr>\n","      <th>182528</th>\n","      <td>https://plato.stanford.edu/entries/zombies/</td>\n","      <td>View this site from another server:</td>\n","      <td>zombies</td>\n","    </tr>\n","    <tr>\n","      <th>182529</th>\n","      <td>https://plato.stanford.edu/entries/zombies/</td>\n","      <td>The Stanford Encyclopedia of Philosophy is cop...</td>\n","      <td>zombies</td>\n","    </tr>\n","    <tr>\n","      <th>182530</th>\n","      <td>https://plato.stanford.edu/entries/zombies/</td>\n","      <td>Library of Congress Catalog Data: ISSN 1095-5054</td>\n","      <td>zombies</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>182531 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                  URL  \\\n","0       https://plato.stanford.edu/entries/abduction/   \n","1       https://plato.stanford.edu/entries/abduction/   \n","2       https://plato.stanford.edu/entries/abduction/   \n","3       https://plato.stanford.edu/entries/abduction/   \n","4       https://plato.stanford.edu/entries/abduction/   \n","...                                               ...   \n","182526    https://plato.stanford.edu/entries/zombies/   \n","182527    https://plato.stanford.edu/entries/zombies/   \n","182528    https://plato.stanford.edu/entries/zombies/   \n","182529    https://plato.stanford.edu/entries/zombies/   \n","182530    https://plato.stanford.edu/entries/zombies/   \n","\n","                                                     Text   Category  \n","0        In the philosophical literature, the term “ab...  abduction  \n","1        This entry is exclusively concerned with abdu...  abduction  \n","2        See also the entry on  scientific discovery, ...  abduction  \n","3        Most philosophers agree that abduction (in th...  abduction  \n","4        You happen to know that Tim and Harry have re...  abduction  \n","...                                                   ...        ...  \n","182526   Many thanks to David Chalmers and to Bill Fis...    zombies  \n","182527   Copyright © 2019 by   Robert Kirk &lt;Robert....    zombies  \n","182528                View this site from another server:    zombies  \n","182529  The Stanford Encyclopedia of Philosophy is cop...    zombies  \n","182530   Library of Congress Catalog Data: ISSN 1095-5054    zombies  \n","\n","[182531 rows x 3 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd \n","\n","df = pd.read_csv('SEP/stanford_encyclopedia_philosophy.csv')\n","display(df)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The loop below will create a dataset folder. The folder will contain a folder for each topic in the SEP. If you downloaded the dataset, you can skip the next cell."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset Folder Created!\n"]}],"source":["import os\n","\n","for category in df.Category.unique():\n","    os.mkdir(f\"SEP/dataset/{category}\")\n","    dff = df[df['Category'] == category]\n","\n","    for i, sample in enumerate(list(dff.Text)):\n","        with open(f'SEP/dataset/{category}/{i}.txt', 'w', encoding='utf-8') as fp:\n","            fp.write(sample)\n","            fp.close()\n","\n","print('Dataset Folder Created!') "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Using the entire SEP Corpus to train a language model (if you don't have access to powerful GPUs) can take a long time. As a result, for demonstration purposes, we will train our language model on a subset of our corpus. The same can be said for the entire dataset (bigger the dataset, longer the epochs).\n","\n","Our mini-dataset contains only `aesthetics-18th-british`, `aesthetics-18th-french`, `aesthetics-18th-german`, and `aesthetics-19th-romantic`."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 659 files\n"]}],"source":["import os\n","\n","filenames = []\n","directories = [\"SEP/dataset/aesthetics-18th-british\",\n","                \"SEP/dataset/aesthetics-18th-french\",\n","                \"SEP/dataset/aesthetics-18th-german\",\n","                \"SEP/dataset/aesthetics-19th-romantic\"]\n","\n","for directory in directories:\n","    for folder in os.listdir(directory):\n","        filenames.append(os.path.join(directory, folder))\n","\n","print(f\"Found {len(filenames)} files\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["All the found files are `txt` with some text about the topics selected above.\n","\n","Now, let us shuffle the order of our samples and create a dataset using the `tf.data.TextLineDataset`, which loads text from text files and creates a dataset where each line of the files becomes an element of the dataset. We also selected a small `batch_size` to avoid OOM (OUT-OF-MEMORY) problems."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import random\n","import tensorflow as tf\n","\n","batch_size = 16\n","\n","random.shuffle(filenames)\n","\n","text_ds = tf.data.TextLineDataset(filenames)\n","text_ds = text_ds.shuffle(buffer_size=256)\n","text_ds = text_ds.batch(batch_size)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Because we are using a small dataset, we will have a small vocabulary (in this example, we end up with a vocabulary with 8600 unique tokens). This is a very small vocabulary compared to large language models like `GPT-3`.\n","\n","> **Note:** Most Large Language Models have tokenizers trained via [Byte-pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding). Check out our other tutorials to learn how.\n","\n","We create our vocabulary using the `tf.keras.layers.TextVectorization`, passing a `custom_standardization` function to lower strings and parse punctuations. Then we adapt the `TextVectorization` layer to our dataset and get our vocabulary out of it. We save the vocabulary in a `txt` file for later use. From this vocabulary, we can detokenize the sequences produced by our language model."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 8600 unique tokens!\n"]}],"source":["import string\n","\n","# Will cut sequences with more than 500 tokens\n","sequence_length = 500\n","\n","# Maximum vocabulary size\n","vocab_size = 8700 \n","\n","# Lower all strings and parse punctuation\n","def custom_standardization(input_string):\n","    lowercased = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(lowercased, f\"([{string.punctuation}])\", r\" \\1\")\n","\n","from keras.layers import TextVectorization\n","\n","# Create a vectorization layer and adapt it to the text\n","vectorize_layer = TextVectorization(\n","    standardize=custom_standardization,\n","    max_tokens=vocab_size - 1,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1,\n",")\n","\n","# Fit the TextVectorization layer to the dataset\n","vectorize_layer.adapt(text_ds)\n","\n","# Get words back from token indices\n","vocab = vectorize_layer.get_vocabulary()  \n","\n","print(f'Found {len(vocab)} unique tokens!')\n","\n","# Save the vocabulary as a text file\n","with open(f'vocabulary.txt', 'w', encoding='utf-8') as fp:\n","    for word in vocab:\n","        fp.write(\"%s\\n\" % word)\n","    fp.close()\n","\n","# Index to detokenize tokens\n","vocab_index = {}\n","for index, word in enumerate(vocab):\n","    vocab_index[word] = index"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["To prepare our dataset, we shift word sequences by $1$ position so that the target for the position ($i$) is a word at position ($i+1$). The model will use all words up till position ($i$) to predict the next word. Thus, our language model will generate text in an autoregressive fashion."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset ready!\n"]}],"source":["def prepare_lm_dataset(text):\n","    \"\"\"\n","    Prepares a language modeling dataset by tokenizing the input text.\n","\n","    Args:\n","        text (str or tf.Tensor): The input text to be tokenized.\n","\n","    Returns:\n","        tuple: A tuple containing two elements: the input sequences (x) and the target sequences (y).\n","    \"\"\"\n","    text = tf.expand_dims(text, -1)\n","    tokenized_sentences = vectorize_layer(text)\n","    x = tokenized_sentences[:, :-1]\n","    y = tokenized_sentences[:, 1:]\n","    return x, y\n","\n","\n","text_ds = text_ds.map(prepare_lm_dataset)\n","text_ds = text_ds.prefetch(tf.data.AUTOTUNE)\n","\n","print('Dataset ready!')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this notebook, we will create a `decoder-only transformer` based on the `GPT` architecture. Our model will use some of the same components we used in our [`sequence-to-sequence`](xxx), like the `PositionalEmbedding` layer (a way to inject word-order information into our model), and the `TransformerDecoder` block, how is in charge with the actual prediction of the next token in a given sequence.\n","\n","The combination of these simple blocks gives rise to our `mini-GPT`. If you wish to create a more robust model and give it more data, you could stack more `TransformerDecoder` blocks and create residual connections among them."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from tensorflow import keras\n","from keras import layers\n","\n","class PositionalEmbedding(layers.Layer):\n","    \"\"\"\n","    This class creates a positional embedding layer that adds positional information to the input embeddings.\n","    It takes in the sequence length, input dimension, and output dimension as arguments.\n","    The call method takes in the inputs and returns the sum of the token embeddings and positional embeddings.\n","    The compute_mask method returns a boolean mask tensor based on the inputs.\n","    The get_config method returns the configuration of the layer.\n","    \"\"\"\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=input_dim, output_dim=output_dim)\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=output_dim)\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","    \n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super(PositionalEmbedding, self).get_config()\n","        config.update({\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","            \"input_dim\": self.input_dim,\n","        })\n","        return config\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    \"\"\"\n","    TransformerDecoder is a class that implements the decoder block of the Transformer model.\n","    It takes in the input sequence, encoder outputs and an optional mask and returns the decoder output.\n","    \n","    Args:\n","        embed_dim (int): The dimensionality of the embedding space.\n","        dense_dim (int): The dimensionality of the dense layer.\n","        num_heads (int): The number of attention heads.\n","        \n","    Returns:\n","        The decoder output.\n","    \"\"\"\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","          num_heads=num_heads, key_dim=embed_dim)\n","        self.attention_2 = layers.MultiHeadAttention(\n","          num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm_2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm_3 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(0.1)\n","        self.dropout2 = layers.Dropout(0.1)\n","        self.dropout3 = layers.Dropout(0.1)\n","        self.supports_masking = True\n","\n","    def get_config(self):\n","        config = super(TransformerDecoder, self).get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config\n","\n","    def get_causal_attention_mask(self, inputs):\n","        input_shape = tf.shape(inputs)\n","        batch_size, sequence_length = input_shape[0], input_shape[1]\n","        i = tf.range(sequence_length)[:, tf.newaxis]\n","        j = tf.range(sequence_length)\n","        mask = tf.cast(i >= j, dtype=\"int32\")\n","        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n","        mult = tf.concat(\n","            [tf.expand_dims(batch_size, -1),\n","             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n","        return tf.tile(mask, mult)\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        causal_mask = self.get_causal_attention_mask(inputs)\n","        if mask is not None:\n","            padding_mask = tf.cast(\n","                mask[:, tf.newaxis, :], dtype=\"int32\")\n","            padding_mask = tf.minimum(padding_mask, causal_mask)\n","        attention_output_1 = self.attention_1(inputs, inputs, attention_mask=causal_mask)\n","        attention_output_1 = self.dropout1(attention_output_1)\n","        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n","        attention_output_2 = self.attention_2(query=attention_output_1,\n","                                                value=encoder_outputs,\n","                                                key=encoder_outputs,\n","                                                attention_mask=padding_mask)\n","        attention_output_2 = self.dropout2(attention_output_2)\n","        attention_output_2 = self.layernorm_2(attention_output_1 + attention_output_2)\n","        proj_output = self.dense_proj(attention_output_2)\n","        proj_output = self.dropout3(proj_output)\n","        return self.layernorm_3(attention_output_2 + proj_output)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We end up with a 6.6M parameters model, which is small for a modern-day, state-of-the-art language model. Keeping things simple, this model has only two attention heads, one decoder block, a vocabulary (_targets of the final dense network_) of 8600 tokens, embedding dimensions of 256 (`GPT-3-Davinci` has 12288 dimensions in its embedding space), and a dense-latent-dimension space of 2048."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, 500)]        0           []                               \n","                                                                                                  \n"," positional_embedding (Position  (None, 500, 256)    2355200     ['input_1[0][0]']                \n"," alEmbedding)                                                                                     \n","                                                                                                  \n"," transformer_decoder (Transform  (None, 500, 256)    2104576     ['positional_embedding[0][0]',   \n"," erDecoder)                                                       'positional_embedding[0][0]']   \n","                                                                                                  \n"," dense_2 (Dense)                (None, 500, 8600)    2210200     ['transformer_decoder[0][0]']    \n","                                                                                                  \n","==================================================================================================\n","Total params: 6,669,976\n","Trainable params: 6,669,976\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}],"source":["from tensorflow import keras\n","import keras_nlp\n","\n","sequence_length = 500\n","embed_dim = 256\n","latent_dim = 2048\n","num_heads = 2\n","\n","inputs = keras.Input(shape=(sequence_length,), dtype=tf.int32)\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n","outputs = layers.Dense(len(vocab), activation=\"softmax\")(x)\n","model = keras.Model(inputs, outputs=outputs)\n","\n","perplexity = keras_nlp.metrics.Perplexity(name=\"perplexity\")\n","\n","model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n","                optimizer=\"adam\", metrics=perplexity)\n","model.summary()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, the last thing we need is to train our model. We are using `keras.callbacks` to save the best model in 30 epochs (for such a small dataset, we don't need more). The callback will monitor the `perplexity` of the model, which is the metric we use to evaluate performance.\n","\n","`Perplexity` is a metric that measures how well a probabilistic model (such as a language model) predicts a sample. `Perplexity` is defined as 2 to the power of the cross-entropy, which measures the difference between the sample's predicted and true probability distribution. \n","\n","The formula for `perplexity` is simply the inverse of the exponentiation of the cross-entropy:\n","\n","$$Perplexity = 2^{-cross\\;entropy}$$\n","\n","$$Cross\\;Entropy = -\\frac{1}{n}\\sum_{i=1}^{n}\\log P(w_i)$$\n","\n","where\n","\n","- $n$ is the length of the sample.\n","- $P(w_i)$ is the predicted probability of the $i$-th word in the sample according to the language model (_sum is taken over all words in the sample_). \n","\n","`Perplexity` reveals how well a model predicts the next word in a sequence and, thus, how well it knows the language. In general, it is used as an evaluation metric in Language Modeling."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Version:  2.10.1\n","Eager mode:  True\n","GPU is available\n","Epoch 1/30\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\CWLINK\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\backend.py:5582: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n","  output, from_logits = _get_logits(\n"]},{"name":"stdout","output_type":"stream","text":["42/42 [==============================] - 26s 548ms/step - loss: 1.8061 - perplexity: 1073.2589\n","Epoch 2/30\n","42/42 [==============================] - 23s 552ms/step - loss: 1.5625 - perplexity: 441.3623\n","Epoch 3/30\n","42/42 [==============================] - 23s 555ms/step - loss: 1.3664 - perplexity: 204.2755\n","Epoch 4/30\n","42/42 [==============================] - 23s 554ms/step - loss: 1.2208 - perplexity: 116.2645\n","Epoch 5/30\n","42/42 [==============================] - 23s 553ms/step - loss: 1.1078 - perplexity: 75.1609\n","Epoch 6/30\n","42/42 [==============================] - 23s 556ms/step - loss: 1.0033 - perplexity: 49.9031\n","Epoch 7/30\n","42/42 [==============================] - 23s 554ms/step - loss: 0.8976 - perplexity: 33.0584\n","Epoch 8/30\n","42/42 [==============================] - 23s 553ms/step - loss: 0.7942 - perplexity: 22.1419\n","Epoch 9/30\n","42/42 [==============================] - 23s 552ms/step - loss: 0.7069 - perplexity: 15.7344\n","Epoch 10/30\n","42/42 [==============================] - 23s 553ms/step - loss: 0.6118 - perplexity: 10.9019\n","Epoch 11/30\n","42/42 [==============================] - 23s 553ms/step - loss: 0.5248 - perplexity: 7.7765\n","Epoch 12/30\n","42/42 [==============================] - 23s 553ms/step - loss: 0.4421 - perplexity: 5.6184\n","Epoch 13/30\n","42/42 [==============================] - 23s 555ms/step - loss: 0.3729 - perplexity: 4.2886\n","Epoch 14/30\n","42/42 [==============================] - 23s 552ms/step - loss: 0.3051 - perplexity: 3.2823\n","Epoch 15/30\n","42/42 [==============================] - 23s 554ms/step - loss: 0.2465 - perplexity: 2.6204\n","Epoch 16/30\n","42/42 [==============================] - 23s 553ms/step - loss: 0.1985 - perplexity: 2.1686\n","Epoch 17/30\n","42/42 [==============================] - 23s 553ms/step - loss: 0.1590 - perplexity: 1.8569\n","Epoch 18/30\n","42/42 [==============================] - 23s 552ms/step - loss: 0.1189 - perplexity: 1.5837\n","Epoch 19/30\n","42/42 [==============================] - 23s 551ms/step - loss: 0.0911 - perplexity: 1.4239\n","Epoch 20/30\n","42/42 [==============================] - 23s 554ms/step - loss: 0.0728 - perplexity: 1.3278\n","Epoch 21/30\n","42/42 [==============================] - 23s 554ms/step - loss: 0.0578 - perplexity: 1.2531\n","Epoch 22/30\n","42/42 [==============================] - 23s 553ms/step - loss: 0.0424 - perplexity: 1.1789\n","Epoch 23/30\n","42/42 [==============================] - 23s 554ms/step - loss: 0.0329 - perplexity: 1.1370\n","Epoch 24/30\n","42/42 [==============================] - 23s 554ms/step - loss: 0.0260 - perplexity: 1.1063\n","Epoch 25/30\n","42/42 [==============================] - 23s 554ms/step - loss: 0.0231 - perplexity: 1.0951\n","Epoch 26/30\n","42/42 [==============================] - 23s 555ms/step - loss: 0.0190 - perplexity: 1.0776\n","Epoch 27/30\n","42/42 [==============================] - 23s 554ms/step - loss: 0.0158 - perplexity: 1.0640\n","Epoch 28/30\n","42/42 [==============================] - 23s 555ms/step - loss: 0.0143 - perplexity: 1.0577\n","Epoch 29/30\n","42/42 [==============================] - 23s 553ms/step - loss: 0.0130 - perplexity: 1.0526\n","Epoch 30/30\n","42/42 [==============================] - 23s 553ms/step - loss: 0.0116 - perplexity: 1.0470\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x24fafe0b0d0>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Version: \", tf.__version__)\n","print(\"Eager mode: \", tf.executing_eagerly())\n","print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n","\n","callbacks = [keras.callbacks.ModelCheckpoint(\"text_gen.keras\",\n","                                                save_best_only=True,\n","                                                monitor=\"perplexity\",\n","                                                patience=3, \n","                                                restore_best_weights=True)]\n","\n","model.fit(text_ds, verbose=1, epochs=30, callbacks=callbacks)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Having a trained language model on texts related to aesthetic philosophy, we can now use it to generate some text.\n","\n","To load models created using subclass functions from the Keras API, you need to pass your classes as custom objects (after building the classes ...):\n","\n","```python\n","\n","model = keras.models.load_model(\"your_model.keras\",\n","    custom_objects={\"TransformerDecoder\": TransformerDecoder, \n","        \"PositionalEmbedding\": PositionalEmbedding,\n","        \"perplexity\": keras_nlp.metrics.Perplexity})\n","\n","```"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from keras.layers import TextVectorization\n","from tensorflow import keras\n","import tensorflow as tf\n","from keras import layers\n","import numpy as np\n","import keras_nlp\n","\n","TextGenerator = keras.models.load_model(\"text_gen.keras\", \n","    custom_objects={\"PositionalEmbedding\": PositionalEmbedding,\n","        \"TransformerDecoder\": TransformerDecoder,\n","        \"perplexity\": keras_nlp.metrics.Perplexity})"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We also load our vocabulary to create a vocabulary index to detokenize the outputs of our model. With this vocabulary, we create a TextVectorization layer to tokenize our prompt inputs, passing our vocabulary so we don't have to adapt it again."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["with open('vocabulary.txt', encoding='utf-8') as fp:\n","    vocab = [line.strip() for line in fp]\n","    fp.close()\n","\n","vocab_index = {}\n","for index, word in enumerate(vocab):\n","    vocab_index[word] = index\n","\n","text_vectorization = TextVectorization(max_tokens=len(vocab),\n","                                        output_mode=\"int\",\n","                                        output_sequence_length=500,\n","                                        vocabulary=vocab)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["\n","def sample_from(logits, chose_from):\n","    \"\"\"\n","    This function allows us to sample from the \n","    probability distribution output of our \n","    model with a \"chose_from = 1\", the model will \n","    always argmax. But with a value higher than 1, \n","    this function will sample a token, randomly from \n","    the top n (chose_from = n) of the distribution\n","    \"\"\"\n","\n","    logits, indices = tf.math.top_k(logits, k=chose_from, sorted=True)\n","    indices = np.asarray(indices).astype(\"int32\")\n","    preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n","    preds = np.asarray(preds).astype(\"float32\")\n","    return np.random.choice(indices, p=preds)\n","\n","\n","def detokenize(number):\n","    \"\"\"\n","    Function to turn tokens back into words ...\n","    \"\"\"\n","    return vocab[number]\n","\n","def generate_text(start_tokens, generate_tokens, vobab, chose_from):\n","    \"\"\"\n","    This function takes as input a sequence of \"start_tokens\" \n","    (a prompt), a number of tokens to be generated, a vocabulary, \n","    and the \"chose_from\" parameter. The function will output a \n","    sequence of words, generated by the model, using the sampling \n","    method established. You can change the number of the \"chose_from\" \n","    argument to make the output less repetitive and more random. \n","    \"\"\"\n","\n","    start_tokens = [_ for _ in start_tokens]\n","    num_tokens_generated = 0\n","    tokens_generated = []\n","\n","    while num_tokens_generated <= generate_tokens:\n","\n","        pad_len = sequence_length - len(start_tokens)\n","        sample_index = len(start_tokens) - 1\n","        if pad_len < 0:\n","            x = start_tokens[:sequence_length]\n","            sample_index = sequence_length - 1\n","        elif pad_len > 0:\n","            x = start_tokens + [0] * pad_len\n","        else:\n","            x = start_tokens\n","\n","        x = np.array([x])\n","        y = TextGenerator.predict(x, verbose=0)\n","        sample_token = sample_from(y[0][sample_index], chose_from)\n","        tokens_generated.append(sample_token)\n","        start_tokens.append(sample_token)\n","        num_tokens_generated = len(tokens_generated)\n","\n","    return \" \".join([detokenize(_) for _ in start_tokens + tokens_generated])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As expected, the model knows \"_something about 17th-century aesthetics_.\" However, a greedy sampling policy makes the output repetitive. Let us introduce some randomness."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["'debates about artistic matters were greatly influenced by the new spaces and means of communication that emerged in the seventeenth and eighteenth centuries . critics expressed their judgments in published treatises and in periodicals such as le mercure galant ; philosophical ideas were also developed in oral conversations between members of the newly founded royal were greatly influenced by the new spaces and means of communication that emerged in the seventeenth and eighteenth centuries . critics expressed their judgments in published treatises and in periodicals such as le mercure galant ; philosophical ideas were also developed in oral conversations between members of the newly founded royal'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["start_prompt = \"debates about artistic matters\"\n","start_tokens = [vocab_index.get(_, 1) for _ in start_prompt.split()]\n","generate_tokens = 50\n","\n","generate_text(start_tokens, generate_tokens, vocab, 1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As expected, the model knows \"_something about 17th-century aesthetics_.\" However, a greedy sampling policy makes the output repetitive. Let us introduce some randomness."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["'debates about artistic matters were greatly else that lessing’s work the aftermath rules . their conviction as in interpretation of the romantics never gave these arts during the ancien published treatises and the romantic texts . in 1719 ] atomistically like flirting finally a vital approach to the romantics aspired rather to cost him his were greatly else that lessing’s work the aftermath rules . their conviction as in interpretation of the romantics never gave these arts during the ancien published treatises and the romantic texts . in 1719 ] atomistically like flirting finally a vital approach to the romantics aspired rather to cost him his'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["start_prompt = \"debates about artistic matters\"\n","start_tokens = [vocab_index.get(_, 1) for _ in start_prompt.split()]\n","generate_tokens = 50\n","\n","generate_text(start_tokens, generate_tokens, vocab, 3)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The text appears to be meaningless at this point. This is to be expected, given that we're dealing with a small model trained on even less text.\n","\n","To summarize, developing a `language model` is a complex task that requires knowledge of `natural language processing`, machine learning, and deep learning techniques. In this notebook, we covered the fundamentals of language modeling with the `GPT` architecture and TensorFlow. Remember that developing a good language model requires a large amount of data and computational resources, so don't be discouraged if your first attempts don't yield cutting-edge results.\n","\n","---\n","\n","Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle)."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"}}},"nbformat":4,"nbformat_minor":2}
