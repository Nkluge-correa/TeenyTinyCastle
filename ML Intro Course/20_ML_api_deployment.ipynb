{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["# Basic `MLOps` - Deploying ML models as APIs\n", "\n", "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n", "\n", "Deploying ML models as APIs is an essential component of `MLOps` (Machine Learning Operations). `MLOps` is the practice of managing and automating the entire lifecycle of a machine learning model, including development, deployment, monitoring, and maintenance. \n", "\n", "Deploying machine learning models as APIs enables data scientists and engineers to share their work with other members of the team or the outside world, allowing them to easily use the model's capabilities in their own applications.\n", "\n", "By creating an API for the machine learning model, the model can be accessed remotely and integrated into various applications with ease. This helps to create a more efficient and streamlined process for using machine learning models in production, ensuring the model is up-to-date and available for use when required.\n", "\n", "In this notebook, we will be making requests to an API that can be launched from the `xgboost_api.py` file. In it. We created an API using `FastAPI` and `Uvicorn`.\n", "\n", "FastAPI is a modern, fast (high-performance) web framework for building APIs with Python. It is built on top of Starlette for the web parts and Pydantic for the data parts. FastAPI allows you to quickly build APIs using asynchronous operations, providing automatic validation of request parameters, data serialization, among other features.\n", "\n", "`Uvicorn` is a lightning-fast `ASGI` (_Asynchronous Server Gateway Interface_) server that allows for asynchronous processing in web applications built with frameworks like `FastAPI`. It is built on top of the `asyncio` framework and provides an easy-to-use interface for running ASGI applications in production.\n", "\n", "<img src=\"https://camo.githubusercontent.com/86d9ca3437f5034da052cf0fd398299292aab0e4479b58c20f2fc37dd8ccbe05/68747470733a2f2f666173746170692e7469616e676f6c6f2e636f6d2f696d672f6c6f676f2d6d617267696e2f6c6f676f2d7465616c2e706e67\" alt=\"drawing\" width=\"400\"/>\n", "\n", "We are using the same `xgboost` model we explored in [this notebook](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/fa17764aa8800c388d0d298b750c686757e0861e/ML%20Intro%20Course/14_time_series_forecasting.ipynb). The [`xgboost_api.py`](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/fa17764aa8800c388d0d298b750c686757e0861e/ML%20Intro%20Course/20_xgboost_api.py) file is basically an implementation of that notebook as an API that takes a series of dates and sales, trains an `XGBoost` model, and returns the predictions for the future by a fixed amount (plus some statistical information about your data distribution).\n", "\n", "Bellow, we are just making an HTTP request to this API, which is running in parallel whit this notebook.\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["{'chocolate': {'dates': ['2023-01-03',\n", "                         '2023-01-04',\n", "                         '2023-01-05',\n", "                         '2023-01-06',\n", "                         '2023-01-07',\n", "                         '2023-01-08',\n", "                         '2023-01-09',\n", "                         '2023-01-10',\n", "                         '2023-01-11',\n", "                         '2023-01-12',\n", "                         '2023-01-13',\n", "                         '2023-01-14',\n", "                         '2023-01-15',\n", "                         '2023-01-16',\n", "                         '2023-01-17'],\n", "               'sales': [123.2838363647461,\n", "                         100.26144409179688,\n", "                         109.02445220947266,\n", "                         120.41613006591797,\n", "                         125.43670654296875,\n", "                         124.01226806640625,\n", "                         105.70793151855469,\n", "                         129.5361785888672,\n", "                         105.73247528076172,\n", "                         112.9113540649414,\n", "                         110.62677764892578,\n", "                         123.35912322998047,\n", "                         129.91722106933594,\n", "                         127.9371337890625,\n", "                         123.13905334472656],\n", "               'statistics': {'maximum': 466.0,\n", "                              'mean': 103.3488160291439,\n", "                              'minimum': 0.0,\n", "                              'std': 54.5542256848513,\n", "                              'variance': 2976.16354007369}}}\n"]}], "source": ["import pprint\n", "import requests\n", "import pandas as pd\n", "\n", "# The data we are going to use\n", "df = pd.read_csv('data/time_series_data.csv')\n", "\n", "# The API endpoint URL\n", "url = \"http://127.0.0.1:8000/predict\"\n", "\n", "# Define the input data\n", "data = {\n", "    \"product\": df.product_id[0], # name of the product\n", "    \"dates\": list(df.dates), # the list of dates\n", "    \"sales\": list(df.sales), # the list of sales\n", "    \"ahead\": 15 # how many days ahead we want to look\n", "}\n", "\n", "# Send a POST request with the input data\n", "response = requests.post(url, json=data)\n", "\n", "pprint.pprint(response.json())"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["And it is done. We have our predictions for the future. In theory, we could send any sales history to this API, and it would generate the forecast for us. \ud83d\ude43\n", "\n", "Let us see our results."]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"data": {"text/markdown": ["# `chocolate` Report\n", "--------------------------------------------------\n", "\n", "| dates      |   sales |\n", "|:-----------|--------:|\n", "| 2023-01-03 | 123.284 |\n", "| 2023-01-04 | 100.261 |\n", "| 2023-01-05 | 109.024 |\n", "| 2023-01-06 | 120.416 |\n", "| 2023-01-07 | 125.437 |\n", "| 2023-01-08 | 124.012 |\n", "| 2023-01-09 | 105.708 |\n", "| 2023-01-10 | 129.536 |\n", "| 2023-01-11 | 105.732 |\n", "| 2023-01-12 | 112.911 |\n", "| 2023-01-13 | 110.627 |\n", "| 2023-01-14 | 123.359 |\n", "| 2023-01-15 | 129.917 |\n", "| 2023-01-16 | 127.937 |\n", "| 2023-01-17 | 123.139 |\n", "\n", "**Total Sales for the next 15 days: 1771.30 Kg.**\n", "\n", "## Statistics Report\n", "\n", "- **`Mean`:** 103.35 Kg.\n", "- **`Minimum`:** 0.00 Kg.\n", "- **`Maximum`:** 466.00 Kg.\n", "- **`Variance`:** 2976.16.\n", "- **`Standard Deviation`:** 54.55.\n", "\n"], "text/plain": ["<IPython.core.display.Markdown object>"]}, "metadata": {}, "output_type": "display_data"}], "source": ["from IPython.display import Markdown\n", "\n", "results_df = pd.DataFrame({\n", "    'dates': response.json()[next(iter(response.json()))]['dates'],\n", "    'sales': response.json()[next(iter(response.json()))]['sales']})\n", "\n", "display(Markdown(f\"\"\"# `{next(iter(response.json()))}` Report\\n{\"-\"*50}\n", "\n", "{results_df.set_index('dates').to_markdown()}\n", "\n", "**Total Sales for the next 15 days: {results_df.sales.sum():.2f} Kg.**\n", "\n", "## Statistics Report\n", "\n", "- **`Mean`:** {response.json()[next(iter(response.json()))]['statistics']['mean']:.2f} Kg.\n", "- **`Minimum`:** {response.json()[next(iter(response.json()))]['statistics']['minimum']:.2f} Kg.\n", "- **`Maximum`:** {response.json()[next(iter(response.json()))]['statistics']['maximum']:.2f} Kg.\n", "- **`Variance`:** {response.json()[next(iter(response.json()))]['statistics']['variance']:.2f}.\n", "- **`Standard Deviation`:** {response.json()[next(iter(response.json()))]['statistics']['std']:.2f}.\n", "\n", "\"\"\"))\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["Now, you could deploy this model as a service, using, for example, a Platform as a Service (`PaaS`) to host your `API` (like [`Render`](https://render.com/) or [`Heroku`](https://www.heroku.com)). You could also deploy an already trained ML models with a more complicated architectures or services. The general blueprint will be basically the same. However, keeping your model and `API` secure and up-to-date is one of the constant works he have in `MLOps`.\n", "\n", "Currently, this API is hosted on `Render`(`Render` has free plans), and you can make calls to it by using the [https://teeny-tiny-api.onrender.com/predict](https://teeny-tiny-api.onrender.com). However, this is a really simple model, and should not be used for real forecasting applications.\n", "\n", "---\n", "\n", "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle)."]}], "metadata": {"kernelspec": {"display_name": "Python 3.9.13 64-bit", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.13"}, "orig_nbformat": 4, "vscode": {"interpreter": {"hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"}}}, "nbformat": 4, "nbformat_minor": 2}