{"cells": [{"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["# Visualizing the performance of deep learning models with `Tensorboard`\n", "\n", "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n", "\n", "`TensorBoard` is a web-based visualization tool that comes bundled with `TensorFlow`. It allows developers and researchers to visualize and monitor their `TensorFlow` models and experiments.\n", "\n", "With `TensorBoard`, you can visualize a wide range of data, such as scalar values, images, and histograms. You can monitor the training progress of your model, including the loss and accuracy over time, and also visualize the gradients and weights of the model. This can help you identify potential issues with your model and optimize its performance.\n", "\n", "`TensorBoard` also supports interactive visualization, which allows you to explore your model and data in real-time. For example, you can visualize embeddings in high-dimensional space and interactively explore clusters of similar data points. In addition to its visualization capabilities, `TensorBoard` also allows you to compare and track multiple experiments, making it easy to compare the performance of different models and configurations.\n", "\n", "<img src=\"https://www.tensorflow.org/tensorboard/images/tensorboard.gif\" width=400 />\n", "\n", "In this notebook, we will show you how to set up your `TensorBoard`, which for many developers is the primary control board of ML experiments.\n", "\n", "If you don't have tensorboard installed, you can quickly get this package by:\n", "\n", "- `pip install tensorboard` in your terminal/CLI.\n", "- `%pip install tensorboard` in a code cell of your jupyter notebook.\n", "- `!pip install tensorboard` in a code cell of your colab notebook.\n", "\n", "Now, you just need to load the tensorboard into your Notebook and set a log directory (_this directory is where tensorboard will store all the logs from your ML experiments_).\n", "\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["%load_ext tensorboard\n", "log_folder = 'logs'"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["If you want to reload the tensorboard from before:\n", "\n", "- `%reload_ext tensorboard`.\n", "\n", "If you want to clear the current logs to write new ones:\n", "\n", "- `rm -rf logs`.\n", "\n", "If you want to add a timestamp to your logs:\n", "\n", "- `log_folder = 'logs/fit/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")`.\n"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["In this example, we will use the `IMDB sentiment analysis dataset.\n", "\n", "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/69/IMDB_Logo_2016.svg\" width=400 />\n", "\n", "`IMDB sentiment analysis` is a popular natural language processing task that involves analyzing movie reviews to determine whether they are positive or negative (a binary classification problem). The dataset used for this task consists of thousands of movie reviews from the Internet Movie Database (`IMDB`), labeled as either positive or negative. \n", "\n", "The goal of sentiment analysis is to build a model that can accurately classify these reviews based on their sentiment. This task is important because it has many practical applications, such as helping companies monitor customer feedback and reviews.\n", "\n", "This will be one of our first implementations of a `NLP` model. `NLP`, or Natural Language Processing, is a subfield of artificial intelligence and computational linguistics that focuses on enabling computers to understand, interpret, and generate human language. `NLP` involves building models and algorithms that can analyze and process human language in various forms, such as text, speech, and images. Some of the most common applications of `NLP` include `sentiment analysis`, `machine translation`, `speech recognition`, `text classification`, `text generation`, and chatbots.\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Model: \"sequential\"\n", "_________________________________________________________________\n", " Layer (type)                Output Shape              Param #   \n", "=================================================================\n", " embed (Embedding)           (None, 500, 128)          256000    \n", "                                                                 \n", " conv1d (Conv1D)             (None, 494, 32)           28704     \n", "                                                                 \n", " max_pooling1d (MaxPooling1D  (None, 98, 32)           0         \n", " )                                                               \n", "                                                                 \n", " conv1d_1 (Conv1D)           (None, 92, 32)            7200      \n", "                                                                 \n", " global_max_pooling1d (Globa  (None, 32)               0         \n", " lMaxPooling1D)                                                  \n", "                                                                 \n", " dense (Dense)               (None, 1)                 33        \n", "                                                                 \n", "=================================================================\n", "Total params: 291,937\n", "Trainable params: 291,937\n", "Non-trainable params: 0\n", "_________________________________________________________________\n"]}], "source": ["import tensorflow as tf\n", "from keras import layers\n", "from keras.datasets import imdb\n", "from keras_preprocessing.sequence import pad_sequences\n", "\n", "\n", "vocabulary = 2000 # Number of words to consider as features.\n", "max_len = 500 # Cuts off texts after this number of words.\n", "\n", "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocabulary)\n", "\n", "x_train = pad_sequences(x_train, maxlen=max_len)\n", "x_test = pad_sequences(x_test, maxlen=max_len)\n", "\n", "model = tf.keras.models.Sequential()\n", "\n", "model.add(layers.Embedding(vocabulary, 128,\n", "                            input_length=max_len,\n", "                            name='embed'))\n", "model.add(layers.Conv1D(32, 7, activation='relu'))\n", "model.add(layers.MaxPooling1D(5))\n", "model.add(layers.Conv1D(32, 7, activation='relu'))\n", "model.add(layers.GlobalMaxPooling1D())\n", "model.add(layers.Dense(1))\n", "\n", "model.compile(optimizer='rmsprop',\n", "            loss='binary_crossentropy',\n", "            metrics=['acc'])\n", "\n", "model.summary()"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["Now that you have a model to track, you need to set your `Tensorboard` callback during the model\u2019s fit method (this is the method responsible for logging events such as activation histograms, metrics plots, training graph visualizations).\n", "\n", "Arguments that the `TensorBoard callback` takes are:\n", "\n", "- `log_dir`: where to store the logs.\n", "- `histogram_freq`: the frequency at which to compute activation and weight histograms for layers of the model (_0 means that histograms will not be computed_).\n", "- `write_graph`: `True` will make the graph vizualization in TensorBoard.\n", "- `write_images`: `True` will make model weights be visualized as an image in TensorBoard.\n", "- `update_freq`: determines how losses and metrics are written to TensorBoard (_when set to `epoch` they are written after every epoch_).\n", "- `profile_batch`: determines which batches will be profiled (_by default, the second batch is profiled. Setting profile_batch to 0 disables profiling_).\n", "- `embeddings_freq`: the frequency at which the embedding layers will be visualized (_setting to 0 means that the embeddings will not be visualized_).\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Version:  2.10.1\n", "Eager mode:  True\n", "GPU is available\n", "Epoch 1/20\n", "157/157 [==============================] - 9s 32ms/step - loss: 0.5866 - acc: 0.6872 - val_loss: 0.4315 - val_acc: 0.8344\n", "Epoch 2/20\n", "157/157 [==============================] - 4s 28ms/step - loss: 0.4242 - acc: 0.8540 - val_loss: 0.4342 - val_acc: 0.8602\n", "Epoch 3/20\n", "157/157 [==============================] - 4s 28ms/step - loss: 0.3546 - acc: 0.8838 - val_loss: 0.5057 - val_acc: 0.8522\n", "Epoch 4/20\n", "157/157 [==============================] - 4s 28ms/step - loss: 0.3240 - acc: 0.9036 - val_loss: 0.5703 - val_acc: 0.8570\n", "Epoch 5/20\n", "157/157 [==============================] - 4s 28ms/step - loss: 0.3021 - acc: 0.9126 - val_loss: 0.5214 - val_acc: 0.8630\n", "Epoch 6/20\n", "157/157 [==============================] - 4s 28ms/step - loss: 0.2555 - acc: 0.9359 - val_loss: 0.7787 - val_acc: 0.8476\n", "Epoch 7/20\n", "157/157 [==============================] - 4s 28ms/step - loss: 0.2043 - acc: 0.9521 - val_loss: 0.7406 - val_acc: 0.8584\n", "Epoch 8/20\n", "157/157 [==============================] - 4s 28ms/step - loss: 0.1734 - acc: 0.9699 - val_loss: 0.7334 - val_acc: 0.8690\n", "Epoch 9/20\n", "157/157 [==============================] - 4s 29ms/step - loss: 0.1434 - acc: 0.9797 - val_loss: 0.8381 - val_acc: 0.8700\n", "Epoch 10/20\n", "157/157 [==============================] - 4s 28ms/step - loss: 0.1268 - acc: 0.9845 - val_loss: 0.9284 - val_acc: 0.8702\n", "Epoch 11/20\n", "157/157 [==============================] - 4s 28ms/step - loss: 0.1097 - acc: 0.9883 - val_loss: 0.9830 - val_acc: 0.8678\n", "Epoch 12/20\n", "157/157 [==============================] - 4s 29ms/step - loss: 0.1045 - acc: 0.9898 - val_loss: 1.1435 - val_acc: 0.8626\n", "Epoch 13/20\n", "157/157 [==============================] - 4s 28ms/step - loss: 0.0991 - acc: 0.9901 - val_loss: 1.0950 - val_acc: 0.8694\n", "Epoch 14/20\n", "157/157 [==============================] - 4s 29ms/step - loss: 0.1074 - acc: 0.9901 - val_loss: 1.2699 - val_acc: 0.8560\n", "Epoch 15/20\n", "157/157 [==============================] - 4s 29ms/step - loss: 0.0968 - acc: 0.9922 - val_loss: 1.1760 - val_acc: 0.8644\n", "Epoch 16/20\n", "157/157 [==============================] - 5s 29ms/step - loss: 0.0984 - acc: 0.9908 - val_loss: 1.1797 - val_acc: 0.8674\n", "Epoch 17/20\n", "157/157 [==============================] - 4s 29ms/step - loss: 0.1020 - acc: 0.9912 - val_loss: 1.2068 - val_acc: 0.8664\n", "Epoch 18/20\n", "157/157 [==============================] - 4s 28ms/step - loss: 0.0962 - acc: 0.9919 - val_loss: 1.2424 - val_acc: 0.8660\n", "Epoch 19/20\n", "157/157 [==============================] - 5s 29ms/step - loss: 0.0943 - acc: 0.9922 - val_loss: 1.2335 - val_acc: 0.8668\n", "Epoch 20/20\n", "157/157 [==============================] - 4s 29ms/step - loss: 0.0954 - acc: 0.9916 - val_loss: 1.2502 - val_acc: 0.8656\n", "782/782 [==============================] - 3s 4ms/step - loss: 1.2741 - acc: 0.8623\n", "Final Loss: 1.27.\n", "Final Performance: 86.23 %.\n"]}], "source": ["\n", "callbacks = [tf.keras.callbacks.TensorBoard(log_dir=log_folder,\n", "                                           histogram_freq=1,\n", "                                           write_graph=True,\n", "                                           write_images=True,\n", "                                           update_freq='epoch',\n", "                                           profile_batch=2,\n", "                                           embeddings_freq=1)]\n", "\n", "print(\"Version: \", tf.__version__)\n", "print(\"Eager mode: \", tf.executing_eagerly())\n", "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n", "\n", "history = model.fit(x_train, y_train,\n", "                    epochs=20,\n", "                    batch_size=128,\n", "                    validation_split= 0.2,\n", "                    callbacks=callbacks,\n", "                    verbose=1)\n", "\n", "test_loss_score, test_acc_score = model.evaluate(x_test, y_test)\n", "\n", "print(f'Final Loss: {round(test_loss_score, 2)}.')\n", "print(f'Final Performance: {round(test_acc_score * 100, 2)} %.')"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["Here is a quick guide on how to navigate your `Tensorboard` dash:\n", "\n", "- The Scalars tab shows changes in the loss and metrics over the epochs. It can be used to track other scalar values such as learning rate and training speed.\n", "- The images tab shows the weights. Adjusting the slider displays the weights at various epochs.\n", "- The graph tab shows your model\u2019s layers. You can use this to check if the architecture of the model looks as intended.\n", "- The distribution tab shows the distribution of tensors (_the distribution of the weights and biases over each epoch_).\n", "- The histograms tab show the distribution of tensors over time.\n", "\n", "If you installed TensorBoard via pip, you can launch it via the command line:\n", "\n", "> `tensorboard --logdir=logs`\n", "\n", "On a Notebook, you can launch it using the cell below.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from tensorboard import notebook\n", "\n", "notebook.display(port=6006, height=1000)"]}, {"attachments": {}, "cell_type": "markdown", "metadata": {}, "source": ["---\n", "\n", "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.13"}, "orig_nbformat": 4, "vscode": {"interpreter": {"hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"}}}, "nbformat": 4, "nbformat_minor": 2}