{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxicity detection with `transformer` models \n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle).\n",
    "\n",
    "**Toxicity detection is a tool commonly used in discussion forums and other social media applications since moderation is crucial to promoting healthy online discussions.** \n",
    "\n",
    "**We can define toxicity as:**\n",
    "\n",
    "- **_“abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation.”_**\n",
    "\n",
    "**For an in-depth discussion on toxicity detection as a machine learning problem, we recommend \"_[Learning from the worst: Dynamically generated datasets to improve online hate detection](https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2012.15761&hl=pt-BR&sa=T&oi=gsb&ct=res&cd=0&d=7265559494033067667&ei=QUJYY6TJL4iKmgHXk5LgDQ&scisig=AAGBfm3gsyOD5eqcUPLFvWmVm8PlLcMr3g)_\".**\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*d4k-PRw-warACDpklCh1mw.png\" alt=\"toxic-image\" width=\"800\"/>\n",
    "\n",
    "\n",
    "**In this notebook, we will be using a dataset created from the [Toxic Comment Classification Challenge Dataset](https://github.com/tianqwang/Toxic-Comment-Classification-Challenge), created by the [Conversation AI](https://conversationai.github.io/) team, a research initiative founded by [Jigsaw](https://jigsaw.google.com/) and Google (both a part of Alphabet).**\n",
    "\n",
    "## ☣️ DISCLAIMER/WARNING ☣️ \n",
    "\n",
    "### _This dataset for this competition contains text that may be considered profane, vulgar, or offensive._\n",
    "\n",
    "**The original dataset contains an unequal distribution of “_hate_” and “_not hate_” samples for multi-classification. However, we created a smaller version of the original dataset (the `toxic_content_dataset.csv`, available for download in [this](https://drive.google.com/uc?export=download&id=1ZvZtrsE1dAl7CiHt16Jstp2rhkDKnGlL) link). The used dataset contains an equal amount of “_hate_” and “_not hate_” samples, summing up to $70157$ samples total.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>aww matches background colour seemingly stuck ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70152</th>\n",
       "      <td>70191</td>\n",
       "      <td>lol gay never know good feels fuck woman ass</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70153</th>\n",
       "      <td>70192</td>\n",
       "      <td>fuck pansy jew would whine nai brith beat pale...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70154</th>\n",
       "      <td>70193</td>\n",
       "      <td>shalom semite get fuck kill son bitch leave wi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70155</th>\n",
       "      <td>70194</td>\n",
       "      <td>think gay fag</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70156</th>\n",
       "      <td>70195</td>\n",
       "      <td>previous conversation fucking shit eating libe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70157 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                       comment_text  toxic\n",
       "0               0  explanation edits made username hardcore metal...      1\n",
       "1               1  aww matches background colour seemingly stuck ...      1\n",
       "2               2  hey man really trying edit war guy constantly ...      1\n",
       "3               3  make real suggestions improvement wondered sec...      1\n",
       "4               4                      sir hero chance remember page      1\n",
       "...           ...                                                ...    ...\n",
       "70152       70191       lol gay never know good feels fuck woman ass      0\n",
       "70153       70192  fuck pansy jew would whine nai brith beat pale...      0\n",
       "70154       70193  shalom semite get fuck kill son bitch leave wi...      0\n",
       "70155       70194                                      think gay fag      0\n",
       "70156       70195  previous conversation fucking shit eating libe...      0\n",
       "\n",
       "[70157 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    'https://drive.google.com/uc?export=download&id=1ZvZtrsE1dAl7CiHt16Jstp2rhkDKnGlL', \n",
    "    'toxic_content_dataset.csv'\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\"toxic_content_dataset.csv\")\n",
    "display(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The original dataset had a lot of symbols and emojis that may hinder the performance of a language model trained for text classification. However, we already performed the cleaning procedure. Below you can find the funciton that we used to function to `preprocess` text data, removing unwanted characters and so on.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
    "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "df['comment_text']= df['comment_text'].map(lambda s:preprocess(s)) \n",
    "\n",
    "x = list(df.comment_text)\n",
    "y = list(df.toxic)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "y_train = np.array(y_train).astype(float)\n",
    "y_test = np.array(y_test).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text is one of the most widespread forms of sequence data and discrete signals (as opposed to continuous signals, like _images_ or _audio_). These sequences can be sequences of characters, syllables, or words.**\n",
    "\n",
    "**Deep learning for NLP is pattern recognition applied to paragraphs, sentences, and words, just as computer vision is pattern recognition applied to videos, images, and pixels.**\n",
    "\n",
    "**Like all neural networks, language models based on deep-learning architectures don’t take as input raw text, i.e., you _can not multiply a word by a weight matrix, add a bias, and apply a ReLU function at the end_. Neural networks only work with numeric tensors. Thus, we need to _vectorize_ our text data, i.e., transform the text into numeric tensors.**\n",
    "\n",
    "**For a comprehensive guide on how to vectorize text data, we recommend Chapter 6: Deep learning for text and sequences, in [_Deep Learning with Python_](https://tanthiamhuat.files.wordpress.com/2018/03/deeplearningwithpython.pdf). Below we will be using the `Tokenizer` class from the [Keras](https://keras.io/) library.** \n",
    "\n",
    "**In terms of preprocessing, you can also pass symbols you may want to filter, by using the `filters` argument.**\n",
    "\n",
    "**_For simplicity, we are creating a tokenizer with a maximum of 3000 words. If you explore the `JSON` file where we saved our tokenizer, you can see the values attribute to each work in our data_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "\n",
    "\n",
    "vocab_size = 3000\n",
    "embed_size = 50\n",
    "max_len = 256\n",
    "tokenizer = Tokenizer(num_words=vocab_size,\n",
    "                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                      lower=True,\n",
    "                      split=\" \",\n",
    "                      oov_token=\"<OOV>\")\n",
    "\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "training_sequences = tokenizer.texts_to_sequences(x_train)\n",
    "training_padded = pad_sequences(\n",
    "    training_sequences, maxlen=max_len, truncating='post')\n",
    "\n",
    "\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with io.open('models\\\\tokenizer_toxic_detection.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Recurrent neural networks](https://en.wikipedia.org/wiki/Recurrent_neural_networks \"Recurrent neural networks\"), like `LSTM` and `GRU`, and convolutional networks, like `1D Convnets`, are great options for dealing with problems involving NLP. In [other notebooks](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Explainability/NLP%20Interpreter%20(en)/model_maker_en.ipynb) of our repository, you will see many examples of how to build these networks for tasks like sentiment analysis.**\n",
    "\n",
    "**However, in this notebook, we will be using a `transformer` model, an extremely versatile and scalable architecture proposed by Vaswani et al. in [Attention Is All You Need](https://arxiv.org/abs/1706.03762).**\n",
    "\n",
    "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" alt=\"drawing\" height=\"450\"/>\n",
    "\n",
    "**A `transformer`  is a  deep learning model that adopts the mechanism of [self-attention](https://en.wikipedia.org/wiki/Attention_(machine_learning) \"Attention (machine learning)\"), differentially weighting the significance of each part of the input data. Like RNNs, transformers are designed to process sequential input data. However, unlike RNNs, transformers process the entire input all at once (_not sequencially_). The transformer does not have to process one word at a time. This allows for more [parallelization](https://en.wikipedia.org/wiki/Parallel_computing \"Parallel computing\"), thus reducing training times, and also allowing the training on larger datasets.**\n",
    "\n",
    "**For an extremely _comprehensive_ and _ilustrated_ explanation of what is \"_attention_\" or how a \"_transformer works_\", we recommend the work of _Jay Alammar_:**\n",
    "\n",
    "- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/);\n",
    "- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/).\n",
    "\n",
    "**Using only the _decoder_ component of the original transformer architecture, we will implement a small transformer model in this notebook (a.k.a. a decoder-only transformer). The original transformer architecture, which consists of both an encoder and a decoder transformer block, was (originally) designed for _sequence-to-sequence_ tasks like translation.**\n",
    "\n",
    "**If you are interested in learning about the transformer architecture, check our _[sequence-to-sequence](https://github.com/Nkluge-correa/teeny-tiny_castle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML%20Intro%20Course/seuqnece-to-sequence.ipynb)_ tutorial.**\n",
    "\n",
    "**In general, text classification can be done using the encoder component. It's a very generic module that learns to transform a sequence into a more useful representation after ingesting it.**\n",
    "\n",
    "**This model only has 4 attention heads with a capacity of 256 tokens and 4 transformer blocks. Our embedding layer's size is also restricted to embeddings with 50 dimensions and a vocabulary of 3000 tokens (where the dense word vectors will be created).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  2.10.0\n",
      "Eager mode:  True\n",
      "GPU is available\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 256, 50)      150000      ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_8 (LayerNo  (None, 256, 50)     100         ['embedding_2[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_4 (MultiH  (None, 256, 50)     207922      ['layer_normalization_8[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 256, 50)      0           ['multi_head_attention_4[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_8 (TFOpLa  (None, 256, 50)     0           ['dropout_9[0][0]',              \n",
      " mbda)                                                            'embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_9 (LayerNo  (None, 256, 50)     100         ['tf.__operators__.add_8[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 256, 4)       204         ['layer_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 256, 4)       0           ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 256, 50)      250         ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_9 (TFOpLa  (None, 256, 50)     0           ['conv1d_9[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_8[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_10 (LayerN  (None, 256, 50)     100         ['tf.__operators__.add_9[0][0]'] \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_5 (MultiH  (None, 256, 50)     207922      ['layer_normalization_10[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 256, 50)      0           ['multi_head_attention_5[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, 256, 50)     0           ['dropout_11[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_9[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_11 (LayerN  (None, 256, 50)     100         ['tf.__operators__.add_10[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 256, 4)       204         ['layer_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 256, 4)       0           ['conv1d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 256, 50)      250         ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_11 (TFOpL  (None, 256, 50)     0           ['conv1d_11[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_10[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_12 (LayerN  (None, 256, 50)     100         ['tf.__operators__.add_11[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_6 (MultiH  (None, 256, 50)     207922      ['layer_normalization_12[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 256, 50)      0           ['multi_head_attention_6[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_12 (TFOpL  (None, 256, 50)     0           ['dropout_13[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_11[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_13 (LayerN  (None, 256, 50)     100         ['tf.__operators__.add_12[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 256, 4)       204         ['layer_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 256, 4)       0           ['conv1d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)             (None, 256, 50)      250         ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_13 (TFOpL  (None, 256, 50)     0           ['conv1d_13[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_12[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_14 (LayerN  (None, 256, 50)     100         ['tf.__operators__.add_13[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " multi_head_attention_7 (MultiH  (None, 256, 50)     207922      ['layer_normalization_14[0][0]', \n",
      " eadAttention)                                                    'layer_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 256, 50)      0           ['multi_head_attention_7[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_14 (TFOpL  (None, 256, 50)     0           ['dropout_15[0][0]',             \n",
      " ambda)                                                           'tf.__operators__.add_13[0][0]']\n",
      "                                                                                                  \n",
      " layer_normalization_15 (LayerN  (None, 256, 50)     100         ['tf.__operators__.add_14[0][0]']\n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)             (None, 256, 4)       204         ['layer_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 256, 4)       0           ['conv1d_14[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)             (None, 256, 50)      250         ['dropout_16[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_15 (TFOpL  (None, 256, 50)     0           ['conv1d_15[0][0]',              \n",
      " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 256)         0           ['tf.__operators__.add_15[0][0]']\n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          32896       ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 128)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            129         ['dropout_17[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,017,329\n",
      "Trainable params: 1,017,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "2808/2808 [==============================] - 637s 226ms/step - loss: 0.2597 - accuracy: 0.8940 - val_loss: 0.2171 - val_accuracy: 0.9227\n",
      "Epoch 2/20\n",
      "2808/2808 [==============================] - 633s 226ms/step - loss: 0.1815 - accuracy: 0.9302 - val_loss: 0.2027 - val_accuracy: 0.9199\n",
      "Epoch 3/20\n",
      "2808/2808 [==============================] - 632s 225ms/step - loss: 0.1654 - accuracy: 0.9358 - val_loss: 0.1840 - val_accuracy: 0.9267\n",
      "Epoch 4/20\n",
      "2808/2808 [==============================] - 631s 225ms/step - loss: 0.1527 - accuracy: 0.9411 - val_loss: 0.1875 - val_accuracy: 0.9251\n",
      "Epoch 5/20\n",
      "2808/2808 [==============================] - 632s 225ms/step - loss: 0.1417 - accuracy: 0.9443 - val_loss: 0.2205 - val_accuracy: 0.9305\n",
      "Epoch 6/20\n",
      "2808/2808 [==============================] - 633s 225ms/step - loss: 0.1336 - accuracy: 0.9487 - val_loss: 0.1843 - val_accuracy: 0.9283\n",
      "Epoch 7/20\n",
      "2808/2808 [==============================] - 632s 225ms/step - loss: 0.1256 - accuracy: 0.9509 - val_loss: 0.2084 - val_accuracy: 0.9302\n",
      "Epoch 8/20\n",
      "2808/2808 [==============================] - 632s 225ms/step - loss: 0.1168 - accuracy: 0.9536 - val_loss: 0.2076 - val_accuracy: 0.9198\n",
      "Epoch 9/20\n",
      "2808/2808 [==============================] - 634s 226ms/step - loss: 0.1101 - accuracy: 0.9584 - val_loss: 0.1997 - val_accuracy: 0.9298\n",
      "Epoch 10/20\n",
      "2808/2808 [==============================] - 634s 226ms/step - loss: 0.1044 - accuracy: 0.9597 - val_loss: 0.2450 - val_accuracy: 0.9305\n",
      "Epoch 11/20\n",
      "2808/2808 [==============================] - 629s 224ms/step - loss: 0.0972 - accuracy: 0.9626 - val_loss: 0.2417 - val_accuracy: 0.9261\n",
      "Epoch 12/20\n",
      "2808/2808 [==============================] - 631s 225ms/step - loss: 0.0894 - accuracy: 0.9658 - val_loss: 0.3002 - val_accuracy: 0.9297\n",
      "Epoch 13/20\n",
      "2808/2808 [==============================] - 633s 225ms/step - loss: 0.0852 - accuracy: 0.9672 - val_loss: 0.2509 - val_accuracy: 0.9256\n",
      "439/439 [==============================] - 67s 150ms/step - loss: 0.1727 - accuracy: 0.9291\n",
      "Final Loss: 0.17.\n",
      "Final Performance: 92.91 %.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    vocab_size = 3000\n",
    "    embed_size = 50\n",
    "    max_len = 256\n",
    "    inputs = tf.keras.Input(shape=input_shape, dtype=\"int32\")\n",
    "    x = tf.keras.layers.Embedding(input_dim=vocab_size,\n",
    "                              output_dim=embed_size,\n",
    "                              input_length=max_len)(inputs)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "input_shape = (training_padded.shape[1])\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "model.compile(loss=tf.losses.BinaryCrossentropy(),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "model.summary()\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                            patience=10, \n",
    "                                            restore_best_weights=True)]\n",
    "model.fit(training_padded,\n",
    "          y_train,\n",
    "          validation_split = 0.2,\n",
    "          epochs=20,\n",
    "          batch_size=16,\n",
    "          verbose=1,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(x_test)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=256, truncating='post')\n",
    "\n",
    "test_loss_score, test_acc_score = model.evaluate(test_padded, y_test)\n",
    "\n",
    "print(f'Final Loss: {round(test_loss_score, 2)}.')\n",
    "print(f'Final Performance: {round(test_acc_score * 100, 2)} %.')\n",
    "model.save(\"models\\\\toxic_detection_transformer.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bellow we can test our pre-trained model.** 🙃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think you should shut up your big mouth\n",
      "\n",
      "Text has toxic content 😔 98.89% | Text has no toxic content 😊 1.11\n",
      "\n",
      "**************************************************\n",
      "I do not agree with you\n",
      "\n",
      "Text has toxic content 😔 7.25% | Text has no toxic content 😊 92.75\n",
      "\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "\n",
    "model = keras.models.load_model('models\\\\toxic_detection_transformer.h5')\n",
    "\n",
    "with open('models\\\\tokenizer_toxic_detection.json') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(data)\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "strings = [\n",
    "    'I think you should shut up your big mouth',\n",
    "    'I do not agree with you'\n",
    "]\n",
    "\n",
    "preds = model.predict(\n",
    "        keras.preprocessing.sequence.pad_sequences(\n",
    "                                                    tokenizer.texts_to_sequences(strings),\n",
    "                                                    maxlen=256,\n",
    "                                                    truncating='post'\n",
    "                                                ),\n",
    "    verbose=0)\n",
    "\n",
    "for i, string in enumerate(strings):\n",
    "    print(f'{string}\\n')\n",
    "    print(f'Text has toxic content 😔 {round((1 - preds[i][0]) * 100, 2)}% | Text has no toxic content 😊 {round(preds[i][0] * 100, 2)}\\n\\n{\"*\" * 50}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can try to repurpose this architecture for other applications and tasks, like multi-classification instead of binary classification.**\n",
    "\n",
    "---\n",
    "\n",
    "Return to the [castle](https://github.com/Nkluge-correa/teeny-tiny_castle)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
