{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# _Cloning_ Language Models with Data Augmentation via Textattack\n","\n","Return to the [castle](https://github.com/Nkluge-correa/TeenyTinyCastle).\n","\n","Adversarial machine learning (`AML`) is a subfield of machine learning that focuses on developing algorithms and techniques that can withstand and respond to adversarial attacks. \n","\n","Adversarial attacks are a type of cyber attack where an attacker deliberately manipulates data inputs to ML models to cause them to produce incorrect outputs. \n","\n","`AML` aims to improve the robustness and security of ML models by identifying vulnerabilities and developing countermeasures to mitigate the impact of adversarial attacks. A range of techniques has been developed for `AML`, including `adversarial training` (_training models on adversarial examples_), and `defensive distillation` (_creating a distilled version of a model that is resistant to adversarial attacks_).\n","\n","`AML` is an active area of research, as ML models continue to be deployed in a wide range of applications where they may be vulnerable to attack.\n","\n","In this notebook, we will explore a type of attack called `model extraction` (_cloning_). But _what is model extraction?_\n","\n","![extraction](https://vitalab.github.io/article/images/stealml/fig1.jpeg)\n","\n","[Source](https://vitalab.github.io/article/2019/11/21/stealml.html)\n","\n","`Model extraction` is a type of cyber attack involving an attacker attempting to extract the details of a machine learning model trained by someone else. This can allow the attacker to create a copy of the model or use its insights to develop their ML model. \n","\n","These attacks typically involve a process of reverse engineering the model, which can be achieved through techniques such as querying the model with specific inputs or observing its responses to a set of test data. \n","\n","`Model extraction` attacks can be particularly damaging when the ML model is used to process sensitive or confidential data, such as personal information or financial data, as the attacker may be able to use the extracted model to gain unauthorized access to this data. \n","\n","If you do not want to train the models, you can load the trained versions in the cell below. But first, you need to download them (instructions in the `models` folder.)\n","\n","Let us begin our exposition with _an Unprotected Model ..._"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import json\n","import tensorflow as tf\n","from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n","\n","model_api = tf.keras.models.load_model('models/model_api.h5')\n","\n","with open('models/tokenizer_model_api.json') as fp:\n","    data = json.load(fp)\n","    tokenizer = tokenizer_from_json(data)\n","    fp.close()\n","\n","\n","def api_call(string):\n","    \"\"\"\n","    Sends a POST request to a model API with the input string, \n","    and returns a dictionary of predicted scores for each class.\n","\n","    Parameters:\n","    string (str): Input text to be sent to the model API.\n","\n","    Returns:\n","    dict: A dictionary containing the HTTP method used, the request string, \n","    and a nested dictionary with keys for each sentiment class and their \n","    corresponding predicted score.\n","\n","    \"\"\"\n","    pediction = model_api.predict(\n","        tf.keras.preprocessing.sequence.pad_sequences(\n","            tokenizer.texts_to_sequences([string]),\n","            maxlen=250,\n","            truncating='post'\n","        ),\n","    verbose=0)\n","    return {\n","    'method' : 'POST',\n","    'request' : f'{string}',\n","    'response': {\n","        'negative_score': f'{pediction[0][0]}',\n","        'neutral_score': f'{pediction[0][1]}',\n","        'positive_score': f'{pediction[0][2]}',\n","        }\n","    }\n"]},{"cell_type":"markdown","metadata":{},"source":["Let us assume that our victim has a model we can call via an API. This particular model is a _sentiment classifier_ (a.k.a. a language model) that we would like to clone. As an attacker, _we do not have a large budget_ (i.e., we must limit the number of calls we make to the model/API), and _we do not have a database of hundreds of thousands of labeled examples_ (if we did, we probably wouldn't need to be cloning this model)."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'method': 'POST',\n"," 'request': 'i did not like this tutorial 2/10',\n"," 'response': {'negative_score': '0.971930980682373',\n","              'neutral_score': '0.0237028319388628',\n","              'positive_score': '0.004366150591522455'}}\n"]}],"source":["import pprint\n","\n","# 'this explanation is really bad'\n","# 'i did not like this tutorial 2/10'\n","# 'this tutorial is garbage i wont my money back'\n","# 'is nice to see philosophers doing machine learning'\n","# 'this is a great and wonderful example of nlp'\n","# 'this tutorial is great one of the best tutorials ever made'\n","\n","request = 'i did not like this tutorial 2/10'\n","\n","api_response = api_call(request)\n","\n","pprint.pprint(api_response)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The model looks good, and we want to clone it.\n","\n","_How should an attacker proceed?_\n","\n","First, we need some data if we don't want to write all our initial samples by hand. Via [web scrapping](https://github.com/Nkluge-correa/TeenyTinyCastle/blob/bbe9c0a77499fa68de7c6d53bf5ef7e0b43a25e0/ML-Explainability/NLP%20(en)/scrape_en.ipynb) or through public data repositories (e.g., [Kaggle](https://www.kaggle.com/)) we were able to assemble an initial database containing 3000 unlabeled samples (_not enough to train a good sentiment classifier_).\n","\n","This is our `proto_dataset.csv`.\n","\n","This is a _black-box attack_, which means that we have no access to the model's _parameters/gradient/architecture_ (to us, it is just something that produces outputs after receiving inputs). However, we can use these outputs to classify our `proto_dataset`. Thus, information about the target model will (indirectly) be passed to our samples. We are stealing this model's predictive power, to try to replicate later.\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>proba</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>when modi promised “minimum government maximum...</td>\n","      <td>[0.9994592070579529, 0.0004880616324953735, 5....</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>vote such party and leadershipwho can take fas...</td>\n","      <td>[0.027490884065628052, 0.17690037190914154, 0....</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>didn’ write chowkidar does mean ’ anti modi tr...</td>\n","      <td>[0.044851239770650864, 0.9484665393829346, 0.0...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>with firm belief the leadership shri narendra ...</td>\n","      <td>[0.05731959268450737, 0.9352818131446838, 0.00...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>sultanpur uttar pradesh loksabha candidate sel...</td>\n","      <td>[0.9963728189468384, 0.0029497554060071707, 0....</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2994</th>\n","      <td>thats not true many are big supporters bjp and...</td>\n","      <td>[0.9744218587875366, 0.02039860561490059, 0.00...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2995</th>\n","      <td>all the name advertising then telling voters w...</td>\n","      <td>[0.9355488419532776, 0.05623412877321243, 0.00...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2996</th>\n","      <td>11was congress making defence strong ask urslf...</td>\n","      <td>[0.9995306730270386, 0.0002866439172066748, 0....</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2997</th>\n","      <td>when will show the real then can gain votes wo...</td>\n","      <td>[0.020042387768626213, 0.9782055020332336, 0.0...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2998</th>\n","      <td>kamre modi brand remove first letter from and ...</td>\n","      <td>[0.9502983689308167, 0.042559292167425156, 0.0...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2999 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                   text  \\\n","0     when modi promised “minimum government maximum...   \n","1     vote such party and leadershipwho can take fas...   \n","2     didn’ write chowkidar does mean ’ anti modi tr...   \n","3     with firm belief the leadership shri narendra ...   \n","4     sultanpur uttar pradesh loksabha candidate sel...   \n","...                                                 ...   \n","2994  thats not true many are big supporters bjp and...   \n","2995  all the name advertising then telling voters w...   \n","2996  11was congress making defence strong ask urslf...   \n","2997  when will show the real then can gain votes wo...   \n","2998  kamre modi brand remove first letter from and ...   \n","\n","                                                  proba  sentiment  \n","0     [0.9994592070579529, 0.0004880616324953735, 5....          0  \n","1     [0.027490884065628052, 0.17690037190914154, 0....          2  \n","2     [0.044851239770650864, 0.9484665393829346, 0.0...          1  \n","3     [0.05731959268450737, 0.9352818131446838, 0.00...          1  \n","4     [0.9963728189468384, 0.0029497554060071707, 0....          0  \n","...                                                 ...        ...  \n","2994  [0.9744218587875366, 0.02039860561490059, 0.00...          0  \n","2995  [0.9355488419532776, 0.05623412877321243, 0.00...          0  \n","2996  [0.9995306730270386, 0.0002866439172066748, 0....          0  \n","2997  [0.020042387768626213, 0.9782055020332336, 0.0...          1  \n","2998  [0.9502983689308167, 0.042559292167425156, 0.0...          0  \n","\n","[2999 rows x 3 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["import pandas as pd\n","import numpy as np\n","\n","df = pd.read_csv('data/proto_dataset.csv')\n","\n","def api_inference_call(string):\n","    \"\"\"\n","    Calls an API to classify the sentiment of a given string, \n","    and returns the result as a numpy array.\n","\n","    Parameters:\n","    -----------\n","    string : str\n","        The input string to be classified.\n","\n","    Returns:\n","    --------\n","    numpy.ndarray\n","        An array of shape (3,) containing the scores for negative, \n","        neutral, and positive sentiment, in that order.\n","    \"\"\"\n","    api_response = api_call(string)\n","    return np.array([float(item) for item in list(api_response['response'].values())]) \n","\n","df['proba'] = df.text.apply(api_inference_call)\n","df['sentiment'] = df.proba.apply(np.argmax)\n","\n","display(df)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Classifying our `proto_dataset` will vary according to the constraints imposed by our victim API (e.g., _cost per call, the limit of calls per minute, etc._). We now have a (_small_) dataset labeled by the target model. And if this model is indeed good (_why else would we want to clone it_), our samples have been accurately classified.\n","\n","Now we need to \"multiply our data\". We are assuming that the attacker does not have a large initial database, and it is not feasible to classify 30000 samples using the API of the target model (either by price or other restrictions).\n","\n","`Data augmentation` is a machine learning technique to increase a dataset's size and diversity by applying transformations or modifications to existing data samples. Data augmentation aims to improve the robustness and generalization ability of ML models by increasing the amount of training data available to them.\n","\n","This methodology can be particularly useful in applications where data is limited or expensive to collect, such as computer vision and natural language processing. Examples of data augmentation techniques include image cropping, rotation, and flipping in computer vision, text paraphrasing, word substitution, and spelling correction in natural language processing.\n","\n","[TextAttack](https://github.com/QData/TextAttack) is a Python framework for adversarial attacks, training, and NLP data augmentation.\n","\n","The part of TextAttack that interests us right now is its _data augmentation part_. Below, we list some of the many ready-made augmentation classes from this library:\n","\n","> Transforation tools $\\rightarrow$ text transformations implemented (e.g., _swaping words, like names and places_) used to create an `Augmenter` object.\n","\n","- `CompositeTransformation`: used to combine multiple transformations.\n","- `WordInsertionRandomSynonym`: inserts synonyms of words already in the sequence.\n","- `WordInsertionMaskedLM`: generate potential insertion for a word using a masked language model.\n","- `WordSwapHowNet`: transforms an input by replacing its words with synonyms in the stored synonyms bank generated by the OpenHowNet (needs a python version > 3.8.1).\n","- `WordSwapEmbedding`: transforms an input by replacing its words with synonyms in the word embedding space.\n","- `WordSwapHomoglyphSwap`: transforms an input by replacing its words with visually similar words using homoglyph swaps.\n","- `WordSwapQWERTY`: common misspellings related to the QWERTY keyboard style.\n","- `WordSwapContract`: transforms an input by performing contraction on recognized combinations.\n","- `WordSwapChangeLocation`: changes a location described in the text (e.g., Brazil -> Argentina).\n","- `WordSwapChangeNumber`: changes a number mentioned in the text (e.g., 7 -> 13).\n","- `WordSwapChangeName`: changes a name mentioned in the text (e.g., Alice -> Bob).\n","- `WordSwapInflections`: transforms an input by replacing its words with their inflections.\n","- `WordSwapMaskedLM` generates potential replacements for a word using a masked language model.\n","- `WordSwapRandomCharacterDeletion`: transforms an input by deleting its characters (`random_one=True, skip_first_char=True, skip_last_char=True` works well!).\n","- `WordSwapRandomCharacterInsertion`: transforms an input by inserting a random character (`random_one=True, skip_first_char=True, skip_last_char=True` works well!).\n","- `WordSwapRandomCharacterSubstitution` transforms an input by replacing one character in a word with a random new character.\n","\n","> Constraints $\\rightarrow$ constraints determine whether or not a given augmentation is valid, consequently enhancing the quality of the augmentations.\n","\n","- `RepeatModification`: a constraint disallowing the modification of previously modified words.\n","- `StopwordModification`: a constraint disallowing the modification of stopwords.\n","\n","> Augmentation parameters $\\rightarrow$ control parameters of the augmenting object.\n","\n","- `pct_words_to_swap`: percentage of words to swap per augmented example. The default is set to 0.1 (10%).\n","- `transformations_per_example`: maximum number of augmentations per input. The default is set to 1 (one augmented sentence given one original input)\n","\n","> Ready Recipes $\\rightarrow$ in addition to creating your own augmenter, you could also use pre-built augmentation recipes. These [recipes are implemented from published papers](https://textattack.readthedocs.io/en/latest/3recipes/augmenter_recipes.html) and are very convenient to use.\n","\n","- `CheckListAugmenter`: augments words by using the transformation methods provided by CheckList INV testing, which combines Name Replacement, Location Replacement, Number Alteration, and Contraction/Extension.\n","- `WordNetAugmenter`: another pre-made augmentation recipe (`high_yield=True, enable_advanced_metrics=True` works well!)."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Augmented Sample 1: also modi teli obc because his toilet upbringing and have mindset brain can chowkidar had there been his mentality kids besides they too would have become chowkidar gatekeeper\n","\n","Augmented Sample 2: modi equal teli obc fry because his upbringing lav and mindset can chowkidar there had there been his kids they too would have become minor chowkidar thither gatekeeper\n","\n","Augmented Sample 3: modi josh teli obc because nipper his upbringing and mindset can chowkidar thither had fry there been his kids they constitute too would bear have become chowkidar gatekeeper\n","\n","Augmented Sample 4: modi pot teli thither obc raising because his upbringing and mindset can chowkidar dope had there been his also kids they too would have there become chowkidar gatekeeper\n","\n","Augmented Sample 5: modi teli Kid obc because minor his upbringing and mindset can chowkidar had there been his kids they Kyd too would have suffer become go chowkidar canful gatekeeper\n","\n","Augmented Sample 6: modi teli nurture obc porter because his upbringing and mindset can chowkidar had there consume been his kids devour they outlook too birth would have become chowkidar gatekeeper\n","\n","Augmented Sample 7: modi teli obc because get his upbringing and mindset can chowkidar had there doorman been his kids excessively they too go would have josh become chowkidar equal gatekeeper\n","\n","Augmented Sample 8: modi teli obc because kid his upbringing and mindset be can doorman chowkidar had thither there seat been his kids they too would have become exist chowkidar gatekeeper\n","\n","Augmented Sample 9: modi teli rearing obc because his upbringing and outlook mindset can chowkidar suit had there been doorman his kids they too outlook would have become likewise chowkidar gatekeeper\n","\n","Augmented Sample 10: receive modi prospect teli obc because nestling his upbringing and mindset can outlook chowkidar had there been his kids they fosterage too would have possess become chowkidar gatekeeper\n","\n"]}],"source":["from textattack.augmentation import Augmenter\n","from textattack.transformations import CompositeTransformation, WordInsertionRandomSynonym, WordSwapContract\n","from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n","\n","transformation = CompositeTransformation(\n","    [WordInsertionRandomSynonym(), WordSwapContract()])\n","constraints = [RepeatModification(), StopwordModification()]\n","\n","aug = Augmenter(transformation=transformation,\n","                constraints=constraints,\n","                pct_words_to_swap=0.5,\n","                transformations_per_example=10)\n","\n","request = df.text[1058]\n","aug_request = aug.augment(request)\n","for i, generated_data in enumerate(aug_request):\n","    print(f'Augmented Sample {i+1}: {generated_data}\\n')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For each labeled sample in our `proto_dataset`, we will generate $10$ augmented copies.\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","0 samples augmented ...\n","250 samples augmented ...\n","500 samples augmented ...\n","750 samples augmented ...\n","1000 samples augmented ...\n","1250 samples augmented ...\n","1500 samples augmented ...\n","1750 samples augmented ...\n","2000 samples augmented ...\n","2250 samples augmented ...\n","2500 samples augmented ...\n","2750 samples augmented ...\n","\n"]}],"source":["labels = []\n","generated_sentences = []\n","\n","for i in range(len(df)):\n","    if i % 250 == 0:\n","        print(f'{i} samples augmented ...')\n","    if i % len(df) == 0 and i != 0:\n","        print(f'{i} samples augmented. Augmentation Complete.')\n","    request = df.text[i]\n","    label = df.sentiment[i]\n","    aug_request = aug.augment(request)\n","    for generated_data in aug_request:\n","        generated_sentences.append(generated_data)\n","        labels.append(label)\n","\n","data = {'text': generated_sentences,\n","        'sentiment': labels}\n","\n","generated_data = pd.DataFrame(data)\n","generated_data.to_csv('data/final_dataset.csv', idenx=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We repeat this process twice, wherein the second time, we increase the percentage of words to be changed in each sentence (`pct_words_to_swap=0.8`), including the `WordSwapQWERTY` transformation, to simulate common typing errors. We eliminate duplicates by arriving at a `dataset_final` with $59258$ samples. Any imbalance in the distribution of samples across classes is just a _mirror image of the biases of the original model_ (e.g., most of the samples classified in the `proto_dataset` have the label _\"negative sentiment\"_). The total time for creating this dataset was $5$ hours.\n","\n","Also, given the way that the API delivers model outputs (it gives us the _probability distribution_ of the victim's model `softmax` function), more information can be extracted. For example, we could [recover the model's logits from its probability predictions to approximate gradients](https://arxiv.org/abs/2011.14779). However, in this notebook/toy example, we will limit ourselves to the vanilla version of this attack."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>State when sodi Dromised “start minimur goveJn...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>behave wBen need modo promised “whC minimum as...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>tabernacle non when modi promised “minimum gov...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>when mMdi promiseO “minimum gBvernment maximum...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>when mbdi promised “minimum government maximum...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>59253</th>\n","      <td>kamre modi brand stiff remove marque first let...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59254</th>\n","      <td>kamre modi stay brand remove corpse first miss...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59255</th>\n","      <td>missive kamre modi brand remove offset first l...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59256</th>\n","      <td>take kamre modi brand remove first absent lett...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>59257</th>\n","      <td>take kamre stigmatize modi brand remove first ...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>59258 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                    text  sentiment\n","0      State when sodi Dromised “start minimur goveJn...          0\n","1      behave wBen need modo promised “whC minimum as...          0\n","2      tabernacle non when modi promised “minimum gov...          0\n","3      when mMdi promiseO “minimum gBvernment maximum...          0\n","4      when mbdi promised “minimum government maximum...          0\n","...                                                  ...        ...\n","59253  kamre modi brand stiff remove marque first let...          0\n","59254  kamre modi stay brand remove corpse first miss...          0\n","59255  missive kamre modi brand remove offset first l...          0\n","59256  take kamre modi brand remove first absent lett...          0\n","59257  take kamre stigmatize modi brand remove first ...          0\n","\n","[59258 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["df = pd.read_csv('data/final_dataset.csv')\n","display(df)\n"]},{"cell_type":"markdown","metadata":{},"source":["Now we can train our surrogate model the _old fashion_.\n","\n","- Load & Split the `dataset`;\n","- Build & Save the `tokenizer`;\n","- Train the `surrogate_model`.\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Version:  2.10.1\n","Eager mode:  True\n","GPU is available\n","Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," embedding (Embedding)       (None, None, 128)         640000    \n","                                                                 \n"," bidirectional (Bidirectiona  (None, None, 128)        98816     \n"," l)                                                              \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 128)              98816     \n"," nal)                                                            \n","                                                                 \n"," dense (Dense)               (None, 3)                 387       \n","                                                                 \n","=================================================================\n","Total params: 838,019\n","Trainable params: 838,019\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/20\n","1186/1186 [==============================] - 87s 70ms/step - loss: 0.3162 - accuracy: 0.8799 - val_loss: 0.1321 - val_accuracy: 0.9529\n","Epoch 2/20\n","1186/1186 [==============================] - 89s 75ms/step - loss: 0.0827 - accuracy: 0.9708 - val_loss: 0.1320 - val_accuracy: 0.9520\n","Epoch 3/20\n","1186/1186 [==============================] - 88s 74ms/step - loss: 0.0501 - accuracy: 0.9831 - val_loss: 0.1164 - val_accuracy: 0.9621\n","Epoch 4/20\n","1186/1186 [==============================] - 93s 79ms/step - loss: 0.0375 - accuracy: 0.9872 - val_loss: 0.1313 - val_accuracy: 0.9623\n","Epoch 5/20\n","1186/1186 [==============================] - 96s 81ms/step - loss: 0.0272 - accuracy: 0.9909 - val_loss: 0.1263 - val_accuracy: 0.9622\n","Epoch 6/20\n","1186/1186 [==============================] - ETA: 0s - loss: 0.0223 - accuracy: 0.9926Restoring model weights from the end of the best epoch: 3.\n","1186/1186 [==============================] - 100s 84ms/step - loss: 0.0223 - accuracy: 0.9926 - val_loss: 0.1387 - val_accuracy: 0.9638\n","Epoch 6: early stopping\n","371/371 [==============================] - 13s 36ms/step - loss: 0.1285 - accuracy: 0.9617\n","Final Loss: 0.13.\n","Final Performance: 96.17 %.\n"]}],"source":["import io\n","import json\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n","\n","\n","df = pd.read_csv('data/final_dataset.csv')\n","\n","vocab_size = 5000\n","embed_size = 128\n","sequence_length = 250\n","\n","tokenizer = Tokenizer(num_words=vocab_size,\n","                      filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n","                      lower=True,\n","                      split=\" \",\n","                      oov_token=\"<OOV>\")\n","\n","tokenizer.fit_on_texts(df.text)\n","tokenizer_json = tokenizer.to_json()\n","\n","with io.open('models/tokenizer_surrogate_model.json', 'w', encoding='utf-8') as fp:\n","    fp.write(json.dumps(tokenizer_json, ensure_ascii=False))\n","    fp.close()\n","\n","\n","x_train, x_test, y_train, y_test = train_test_split(\n","    df.text, df.sentiment, test_size=0.2, random_state=42)\n","\n","x_train = pad_sequences(\n","    tokenizer.texts_to_sequences(x_train), \n","    maxlen=sequence_length, \n","    truncating='post')\n","x_test = pad_sequences(\n","    tokenizer.texts_to_sequences(x_test), \n","    maxlen=sequence_length, \n","    truncating='post')\n","y_train = np.array(y_train).astype(float)\n","y_test = np.array(y_test).astype(float)\n","\n","\n","inputs = tf.keras.Input(shape=(None,), dtype=\"int32\")\n","x = tf.keras.layers.Embedding(input_dim=vocab_size,\n","                              output_dim=embed_size,\n","                              input_length=sequence_length)(inputs)\n","x = tf.keras.layers.Bidirectional(\n","    tf.keras.layers.LSTM(64, return_sequences=True))(x)\n","x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x)\n","outputs = tf.keras.layers.Dense(3, activation=\"softmax\")(x)\n","model = tf.keras.Model(inputs, outputs)\n","\n","\n","model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","print(\"Version: \", tf.__version__)\n","print(\"Eager mode: \", tf.executing_eagerly())\n","print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n","model.summary()\n","\n","callbacks = [tf.keras.callbacks.ModelCheckpoint(\"models/surrogate_model.h5\",\n","                                                save_best_only=True),\n","            tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n","                                            patience=3,\n","                                            verbose=1,\n","                                            mode=\"auto\",\n","                                            baseline=None,\n","                                            restore_best_weights=True)]\n","\n","model.fit(x_train,\n","          y_train,\n","          epochs=20,\n","          validation_split=0.2,\n","          callbacks=callbacks,\n","          verbose=1)\n","\n","test_loss_score, test_acc_score = model.evaluate(x_test, y_test)\n","\n","print(f'Final Loss: {round(test_loss_score, 2)}.')\n","print(f'Final Performance: {round(test_acc_score * 100, 2)} %.')\n"]},{"cell_type":"markdown","metadata":{},"source":["In real-life situations, we could not do a comparison test between the original model and our clone. But since this is just a toy example, we can! 🙃\n","\n","For this, we are using a test database not seen by both models.\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["46/46 [==============================] - 2s 31ms/step - loss: 0.6821 - accuracy: 0.7937\n","\n","Accuracy of the API MODEL: 79.37 %.\n","\n","46/46 [==============================] - 3s 31ms/step - loss: 1.4773 - accuracy: 0.6503\n","\n","Accuracy of the SURROGATE MODEL: 65.03 %.\n","\n"]}],"source":["\n","test_dataset = pd.read_csv('data/compare_models_dataset.csv')\n","\n","model_api = tf.keras.models.load_model('models/model_api.h5')\n","\n","with open('models/tokenizer_model_api.json') as fp:\n","    data = json.load(fp)\n","    tokenizer_api = tokenizer_from_json(data)\n","    fp.close()\n","\n","surrogate_model = tf.keras.models.load_model('models/surrogate_model.h5')\n","\n","with open('models/tokenizer_surrogate_model.json') as fp:\n","    data = json.load(fp)\n","    tokenizer_surrogate = tokenizer_from_json(data)\n","    word_index_surrogate = tokenizer_surrogate.word_index\n","    fp.close()\n","\n","x = pad_sequences(\n","    tokenizer_api.texts_to_sequences(test_dataset.text), \n","    maxlen=250, truncating='post')\n","y = np.array(test_dataset.sentiment).astype(float)\n","\n","_, test_acc_score = model_api.evaluate(x, y)\n","\n","print(f'\\nAccuracy of the API MODEL: {round(test_acc_score * 100, 2)} %.\\n')\n","\n","x = pad_sequences(\n","    tokenizer_surrogate.texts_to_sequences(test_dataset.text), \n","    maxlen=280, truncating='post')\n","\n","_, test_acc_score = surrogate_model.evaluate(x, y)\n","\n","print(\n","    f'\\nAccuracy of the SURROGATE MODEL: {round(test_acc_score * 100, 2)} %.\\n')\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["~ $14\\%$ is less accurate than this benchmark's original model but still valid. Architecture changes and database augmentation can improve the performance of our `surrogate_model`. We now have our own language model for sentiment classification, and we have spent not even $10%$ of what was invested in creating the original model (_supposedly_).\n","\n","Now, let us put our `surrogate_model` into production.\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'method': 'POST',\n"," 'request': 'i did not like this tutorial 2/10',\n"," 'response': {'negative_score': '0.8380430340766907',\n","              'neutral_score': '0.12354790419340134',\n","              'positive_score': '0.03840905427932739'}}\n"]}],"source":["import pprint\n","\n","def surrogate_api_call(string):\n","    pediction = surrogate_model.predict(\n","        tf.keras.preprocessing.sequence.pad_sequences(\n","            tokenizer.texts_to_sequences([string]),\n","            maxlen=250,\n","            truncating='post'\n","        ),\n","    verbose=0)\n","    return {\n","    'method' : 'POST',\n","    'request' : f'{string}',\n","    'response': {\n","        'negative_score': f'{pediction[0][0]}',\n","        'neutral_score': f'{pediction[0][1]}',\n","        'positive_score': f'{pediction[0][2]}',\n","        }\n","    }\n","\n","request = 'i did not like this tutorial 2/10'\n","\n","api_response = api_call(request)\n","\n","pprint.pprint(api_response)\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Model extraction attacks pose a threat to intellectual property and privacy. The availability of a model in the cloud, whether as a service or API, must be carefully architected by developers if they do not want to fall victim to this kind of attack. 🐱‍💻\n","\n","To mitigate the risk of model extraction attacks, it is important to implement robust security measures, such as data encryption, access controls, and tamper-proofing techniques, to protect the confidentiality and integrity of ML models.\n","\n","For more information on the subject, check the literature listed below:\n","\n","- [A Framework for Understanding Model Extraction Attack and Defense](https://arxiv.org/abs/2206.11480).\n","- [Increasing the Cost of Model Extraction with Calibrated Proof of Work](https://arxiv.org/abs/2201.09243).\n","- [Data-Free Model Extraction](https://arxiv.org/abs/2011.14779).\n","- [MEGEX: Data-Free Model Extraction Attack against Gradient-Based Explainable AI](https://arxiv.org/abs/2107.08909).\n","- [DeepSteal: Advanced Model Extractions Leveraging Efficient Weight Stealing in Memories](https://arxiv.org/abs/2111.04625).\n","- [Model Extraction and Defenses on Generative Adversarial Networks](https://arxiv.org/abs/2101.02069).\n","\n","---\n","\n","Return to the [castle](https://github.com/Nkluge-correa/TeenyTinyCastle)."]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"}}},"nbformat":4,"nbformat_minor":2}
