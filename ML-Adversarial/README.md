# Adversarial Machine Learning

You can find lessons on Adversarial ML in this folder.

Within these tutorials, we navigate the intricate landscape of thwarting adversarial attacks and understanding their nuances. Explore the dark arts of exploiting pickle serialization, create adversarial examples with SecML and Textattack, and apply the fast sign gradient method against convolutional neural networks.

| Tutorial                                            | GitHub                                                                                                                                        | Colab                                                                                                                                                                                              |
|-----------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Exploiting pickle serialization                     | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Adversarial/the_pickle_exploit.ipynb" target="_blank">LINK</a>       | <a href="https://colab.research.google.com/drive/1-xZDB44n_kgOaOqT3EcSPoPPsj_1BPbI" target="_blank"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a> |
| Creating adversarial examples with `SecML`          | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Adversarial/evasion_attacks.ipynb" target="_blank">LINK</a>          | <a href="https://colab.research.google.com/drive/1Axu4W7HWS_UOjYXJSOa9N58bsJrUGqmp" target="_blank"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a> |
| Applying the fast sign gradient method against CNNs | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Adversarial/evasion_attacks_FGSM.ipynb" target="_blank">LINK</a>     | <a href="https://colab.research.google.com/drive/17ZqlKCgnigCqDcxOM0sF2jQsCLt61bOO" target="_blank"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a> |
| Creating adverarial examples with `textattack`      | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Adversarial/adversarial_text_attack.ipynb" target="_blank">LINK</a>  | <a href="https://colab.research.google.com/drive/1ngqwaiCE405NvlmLlJJNEdJFqBC0CbXT" target="_blank"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a> |
| Extraction attacks via model clonning               | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Adversarial/model_extraction_nlp.ipynb" target="_blank">LINK</a>     | <a href="https://colab.research.google.com/drive/1QB12Bb5jcl8nkd0vLAYKFP6AHzUal0F_" target="_blank"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a> |
| Demonstrating poisoning attacks                     | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Adversarial/data_poisoning_attacks.ipynb" target="_blank">LINK</a>   | <a href="https://colab.research.google.com/drive/1i5oGM5PP-rn4QQxea4v2uUKEgQYtdfbn" target="_blank"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a> |
| Adversarial training for computer vision models     | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Adversarial/adversarial_training_cv.ipynb" target="_blank">LINK</a>  | <a href="https://colab.research.google.com/drive/1RL1A5R7gxcsovVFbejU-Ih81yODaBfL6" target="_blank"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a> |
| Adversarial training for language models            | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Adversarial/adversarial_training_nlp.ipynb" target="_blank">LINK</a> | <a href="https://colab.research.google.com/drive/1SrE5fYmWf0fjokgyTd-RSge8H_uMZ1I_" target="_blank"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a> |

---

Return to the [castle](https://github.com/Nkluge-correa/TeenyTinyCastle).
