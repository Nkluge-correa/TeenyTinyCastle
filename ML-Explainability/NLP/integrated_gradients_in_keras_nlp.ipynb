{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9qykZotTy4s"
      },
      "source": [
        "# Applying integrated gradients to Language Models\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1rTVXeecVJ4gLxkeAzMmrlj9ZG31QrXXq\" target=\"_blank\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\">\n",
        "</a>\n",
        "\n",
        "Return to the [castle](https://github.com/Nkluge-correa/TeenyTinyCastle).\n",
        "\n",
        "Language models are an excellent example of how we can use integrated gradient techniques to explain a neural network's behavioral processes. In short, integrated gradients are an interpretability method proposed in \"[Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365)\". This methodology uses the calculated gradient of an ML model to determine what influence the individual parts of an input (like tokens in a sentence or pixels in an image) have on the model's output.\n",
        "\n",
        "![image](https://raw.githubusercontent.com/garygsw/smooth-taylor/master/method-comparison.png)\n",
        "\n",
        "[Source](https://github.com/garygsw/smooth-taylor).\n",
        "\n",
        "Integrated gradients are closely related to saliency maps, something we covered in [this](https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/CV/CNN_attribution_maps.ipynb) (Grad-Cam) and [this](https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/NLP/lime_for_NLP.ipynb) (LIME) notebook. However, we will use the `alibi` library to explore this method in this tutorial.\n",
        "\n",
        "> **[Alibi](https://docs.seldon.io/projects/alibi) is an open-source Python library aimed at machine learning model inspection and interpretation.**\n",
        "\n",
        "As the object of our analysis, we will be using one of the models we trained on [this notebook](https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/NLP/model_maker.ipynb) (a Bidirectional LSTM), which can be found on the Hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeiJqkDMkbs8",
        "outputId": "7c7b0e81-d7c8-4813-e140-dbac9337631b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Git LFS initialized.\n",
            "Cloning into 'BiLSTM-sentiment-classifier'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 14 (delta 3), reused 0 (delta 0), pack-reused 4\u001b[K\n",
            "Unpacking objects: 100% (14/14), 4.55 KiB | 932.00 KiB/s, done.\n",
            "Filtering content: 100% (2/2), 25.87 MiB | 17.48 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "# First, we need to clone the repository of our model, which contains both the model and its tokenizer\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/AiresPucrs/BiLSTM-sentiment-classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNngEuG9D8oT"
      },
      "source": [
        "Let us quickly load and test our model to see if everything works as it should."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvlC5u55Ty4w",
        "outputId": "f7c095f4-e58f-4982-ed28-8b5144ba8026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review: \"this explanation is really bad\"\n",
            "(Negative 😔 95% | Positive 😊       5%)\n",
            "\n",
            "Review: \"i did not like this tutorial 2/10\"\n",
            "(Negative 😔 88% | Positive 😊       12%)\n",
            "\n",
            "Review: \"this tutorial is garbage i wont my money back\"\n",
            "(Negative 😔 89% | Positive 😊       11%)\n",
            "\n",
            "Review: \"is nice to see philosophers doing machine learning\"\n",
            "(Negative 😔 4% | Positive 😊       96%)\n",
            "\n",
            "Review: \"this is a great and wonderful example of nlp\"\n",
            "(Negative 😔 0% | Positive 😊       100%)\n",
            "\n",
            "Review: \"this tutorial is great one of the best tutorials ever made\"\n",
            "(Negative 😔 0% | Positive 😊       100%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "model_path = '/content/BiLSTM-sentiment-classifier/BiLSTM-sentiment-classifier.h5'\n",
        "tokenizer_path = '/content/BiLSTM-sentiment-classifier/tokenizer-BiLSTM-sentiment-classifier.json'\n",
        "\n",
        "# Load the model\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Load the tokenizer\n",
        "with open(tokenizer_path) as fp:\n",
        "    data = json.load(fp)\n",
        "    tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)\n",
        "    word_index = tokenizer.word_index\n",
        "    fp.close()\n",
        "\n",
        "# Define samples to test the model\n",
        "strings = [\n",
        "    'this explanation is really bad',\n",
        "    'i did not like this tutorial 2/10',\n",
        "    'this tutorial is garbage i wont my money back',\n",
        "    'is nice to see philosophers doing machine learning',\n",
        "    'this is a great and wonderful example of nlp',\n",
        "    'this tutorial is great one of the best tutorials ever made'\n",
        "]\n",
        "\n",
        "# Get some redictions\n",
        "preds = model.predict(\n",
        "    tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenizer.texts_to_sequences(strings),\n",
        "        maxlen=250,\n",
        "        truncating='post'\n",
        "    ), verbose=0)\n",
        "\n",
        "for i, string in enumerate(strings):\n",
        "    print(f'Review: \"{string}\"\\n(Negative 😔 {round((preds[i][0]) * 100)}% | Positive 😊 \\\n",
        "      {round(preds[i][1] * 100)}%)\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMYiCcaPTy4y"
      },
      "source": [
        "In language models, like text classification models, integrated gradients define an attribution value for each token in the input sentence. The attributions are calculated considering the integral of the model gradients with respect to the word embedding layer along a straight path from a baseline instance x' to the input instance x.\n",
        "\n",
        "Thus, we can say the attribution given to an input is equal to the difference between the model output at the instance x and the model output at the baseline x':\n",
        "\n",
        "$$A(x, x') = F(x) -  F(x')$$\n",
        "\n",
        "Now, to utilize the [`IntegratedGradients`](https://docs.seldon.io/projects/alibi/en/latest/api/alibi.explainers.html#alibi.explainers.IntegratedGradients) class from `alibi`, we need to set some arguments first:\n",
        "\n",
        "-   `model`: a tensorflow model.\n",
        "-   `layer`: a layer or a function having as parameter the model and returning a layer with respect to which the gradients are calculated. In the case of our language model, this is the `Embedding` layer.\n",
        "-   `target_fn`: A scalar function applied to the model's predictions (like ` np.argmax(predictions, axis=1)`).\n",
        "-   `method`: Method for the integral approximation (`riemann_left`,  `riemann_right`,  `riemann_middle`,  `riemann_trapezoid`,  `gausslegendre`).\n",
        "-   `n_steps`: Number of steps in the path integral approximation from the baseline to the input instance.  \n",
        "-   `internal_batch_size`: Batch size for the internal batching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htfwUjEuTy4y",
        "outputId": "73a69790-5786-4122-8e2d-a9643db158ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m522.1/522.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.5/98.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install alibi -q\n",
        "\n",
        "import alibi\n",
        "\n",
        "n_steps = 100 #128\n",
        "internal_batch_size = 500 #250\n",
        "\n",
        "ig  = alibi.explainers.IntegratedGradients(\n",
        "    model,\n",
        "    target_fn=None,\n",
        "    layer=lambda model: model.layers[1], # Embedding layer\n",
        "    n_steps=100, #128\n",
        "    method=\"gausslegendre\",\n",
        "    internal_batch_size = 500 #250\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_bvK7s5Ty4z"
      },
      "source": [
        "The integrated gradient attributions are calculated concerning the embedding layer for the number of samples we defined in our x_test_sample list. This could also be a partition of your testing set. With these samples, we use our model to generate a prediction array. Meanwhile, `ig.explain` (the alibi explainer object) requires a list of elements (predicted_classes) of the model's output to compute the gradients. We can achieve this by argmaxing the prediction array or passing the `preds.argmax(axis=1)` function as the `target` parameter. Since our model has a `softmax` output, we can set the `target` parameter as `preds.argmax(axis=1)`.\n",
        "\n",
        "> **Note: If you are using a model with a `sigmoid` output, some basic list comprehension (`[0 if preds[i][0] < 0.5 else 1 for i in range(len(preds))]`) can give you a list of predicted classes for your samples.**\n",
        "\n",
        "Here, we use the default baseline (`None`), which equates to a sequence of zeros (this corresponds to a sequence of padding characters, a.k.a. no input)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CL1dIcM6Ty4z"
      },
      "outputs": [],
      "source": [
        "# Sample text\n",
        "x_test_sample = [\n",
        "    'One of the weakest entries in the J-horror remake sweepstakes, One Missed Call is undone by bland performances and shopworn shocks.'\n",
        "]\n",
        "\n",
        "# Prediction\n",
        "preds = model.predict(\n",
        "    tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tokenizer.texts_to_sequences(x_test_sample),\n",
        "        maxlen=250,\n",
        "        truncating='post'\n",
        "    ), verbose=0\n",
        ")\n",
        "\n",
        "# Target fuction that gives you the class\n",
        "target_function =  preds.argmax(axis=1)\n",
        "\n",
        "explanation = ig.explain(tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                            tokenizer.texts_to_sequences(x_test_sample),\n",
        "                            maxlen=250,\n",
        "                            truncating='post'),\n",
        "                         baselines=None,\n",
        "                         target=target_function,\n",
        "                         attribute_to_layer_inputs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hstFfgBZTy4z"
      },
      "source": [
        "From this explanation object, we can recover our attributions. To retrieve them, we call the `explanation.attributions`, which, at index 0 (since we only passed a single sample), retrieves us an array of shape (1, 250, 128), i.e., one sample (batch dimension), of length size 250 (context length), and dimensionality 128 (the embedding dimension). Summing all values across the embedding dimension (axis 2), we get the attributions for our 250 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib-eQwIJTy4z",
        "outputId": "aae5d93c-7a75-4f89-8f0a-e6b66d8ce829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attributions shape: (1, 250)\n"
          ]
        }
      ],
      "source": [
        "attrs = explanation.attributions[0]\n",
        "attrs = attrs.sum(axis=2)\n",
        "print('Attributions shape:', attrs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L1VtHyuTy40"
      },
      "source": [
        "Now, we get rid of the padding tokens and take only the attributions that relate to the words in our input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbQELaYJTy40",
        "outputId": "820590f5-d0ef-48ba-ef81-dee89df81349"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(21, 21)"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get how many tokens our text sample has\n",
        "number_of_tokens = len(x_test_sample[0].split())\n",
        "# Get all the attributions that are not froma pad token\n",
        "atributions = attrs[0][- number_of_tokens:]\n",
        "# Get a list of corresponding words for every token\n",
        "words = x_test_sample[0].split()\n",
        "\n",
        "len(words), len(atributions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fa47085Ty40"
      },
      "source": [
        "To create a visually intuitive way of interpreting this model's output, we will assign a color to each attribution score, taking the `max` and `min` values to set a range of predefined colors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDhVLqvYTy40",
        "outputId": "1efc92b1-2e9b-4bb2-f2af-dcf840c9c793"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['#fff4b0',\n",
              " '#fff5b5',\n",
              " '#fff4b0',\n",
              " '#fff4b0',\n",
              " '#fff5b3',\n",
              " '#fff5b5',\n",
              " '#ffefa4',\n",
              " '#fff3ae',\n",
              " '#fee38b',\n",
              " '#fff0a8',\n",
              " '#fff1ab',\n",
              " '#ffeda1',\n",
              " '#fee085',\n",
              " '#fff4b0',\n",
              " '#fff8bb',\n",
              " '#fff2ac',\n",
              " '#800026',\n",
              " '#ffffcc',\n",
              " '#fff7b7',\n",
              " '#fee288',\n",
              " '#ffea99']"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import matplotlib\n",
        "\n",
        "# Get the max and min atributions to estipulate a range\n",
        "minima = min(atributions)\n",
        "maxima = max(atributions)\n",
        "\n",
        "# Normalize and map this range to a color scheme (we chose `YlOrRd`)\n",
        "norm = matplotlib.colors.Normalize(vmin=minima, vmax=maxima, clip=True)\n",
        "mapper = matplotlib.cm.ScalarMappable(norm=norm, cmap=matplotlib.cm.YlOrRd)\n",
        "\n",
        "# Create a list of HEX colors\n",
        "colors = [matplotlib.colors.to_hex(mapper.to_rgba(v)) for v in atributions]\n",
        "\n",
        "colors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N4GX-dgTy41"
      },
      "source": [
        "Now, with attributions and colors, we can create a function that maps the generated colors to each token in the text sample, yielding a colorful HTML representation of the attributions given to each word. Since we use the \"YlOrRd\" color scale, words with **high positive attribution** are colored in shades of **red**. Words with **middling attributions** are colored **orange**, while **low attributions** receive a **pale yellow**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "biycoHG8Ty41",
        "outputId": "62b345c5-39d9-4ba0-862f-73ec6b35669d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Sample: One of the weakest entries in the J-horror remake sweepstakes, One Missed Call is undone by bland performances and shopworn shocks.\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Prediction: (Negative 😔 80% | Positive 😊 20%)\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Attributions: <span style=\"color:#fff4b0\"><b>One</b></span> <span style=\"color:#fff5b5\"><b>of</b></span> <span style=\"color:#fff4b0\"><b>the</b></span> <span style=\"color:#fff4b0\"><b>weakest</b></span> <span style=\"color:#fff5b3\"><b>entries</b></span> <span style=\"color:#fff5b5\"><b>in</b></span> <span style=\"color:#ffefa4\"><b>the</b></span> <span style=\"color:#fff3ae\"><b>J-horror</b></span> <span style=\"color:#fee38b\"><b>remake</b></span> <span style=\"color:#fff0a8\"><b>sweepstakes,</b></span> <span style=\"color:#fff1ab\"><b>One</b></span> <span style=\"color:#ffeda1\"><b>Missed</b></span> <span style=\"color:#fee085\"><b>Call</b></span> <span style=\"color:#fff4b0\"><b>is</b></span> <span style=\"color:#fff8bb\"><b>undone</b></span> <span style=\"color:#fff2ac\"><b>by</b></span> <span style=\"color:#800026\"><b>bland</b></span> <span style=\"color:#ffffcc\"><b>performances</b></span> <span style=\"color:#fff7b7\"><b>and</b></span> <span style=\"color:#fee288\"><b>shopworn</b></span> <span style=\"color:#ffea99\"><b>shocks.</b></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"c9e817b7-1b10-4b65-b964-23aab9096179\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c9e817b7-1b10-4b65-b964-23aab9096179\")) {                    Plotly.newPlot(                        \"c9e817b7-1b10-4b65-b964-23aab9096179\",                        [{\"marker\":{\"color\":[\"#fff4b0\",\"#fff5b5\",\"#fff4b0\",\"#fff4b0\",\"#fff5b3\",\"#fff5b5\",\"#ffefa4\",\"#fff3ae\",\"#fee38b\",\"#fff0a8\",\"#fff1ab\",\"#ffeda1\",\"#fee085\",\"#fff4b0\",\"#fff8bb\",\"#fff2ac\",\"#800026\",\"#ffffcc\",\"#fff7b7\",\"#fee288\",\"#ffea99\"]},\"orientation\":\"h\",\"x\":[0.0023452159948647022,-0.0013024469371885061,0.0022560586221516132,0.0030170385725796223,-0.0005319124320521951,-0.001617498230189085,0.018236394971609116,0.005913562141358852,0.05089776962995529,0.012457971461117268,0.009964767843484879,0.020850658416748047,0.05638141185045242,0.0032986244186758995,-0.010012251324951649,0.008837256580591202,0.40113043785095215,-0.03163793310523033,-0.004952103830873966,0.05439959093928337,0.03222872316837311],\"y\":[\"One\",\"of\",\"the\",\"weakest\",\"entries\",\"in\",\"the\",\"J-horror\",\"remake\",\"sweepstakes,\",\"One\",\"Missed\",\"Call\",\"is\",\"undone\",\"by\",\"bland\",\"performances\",\"and\",\"shopworn\",\"shocks.\"],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#f2f5fa\"},\"error_y\":{\"color\":\"#f2f5fa\"},\"marker\":{\"line\":{\"color\":\"rgb(17,17,17)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"baxis\":{\"endlinecolor\":\"#A2B1C6\",\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"minorgridcolor\":\"#506784\",\"startlinecolor\":\"#A2B1C6\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"marker\":{\"line\":{\"color\":\"#283442\"}},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#506784\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"header\":{\"fill\":{\"color\":\"#2a3f5f\"},\"line\":{\"color\":\"rgb(17,17,17)\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#f2f5fa\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#f2f5fa\"},\"geo\":{\"bgcolor\":\"rgb(17,17,17)\",\"lakecolor\":\"rgb(17,17,17)\",\"landcolor\":\"rgb(17,17,17)\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#506784\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"dark\"},\"paper_bgcolor\":\"rgb(17,17,17)\",\"plot_bgcolor\":\"rgb(17,17,17)\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"radialaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"yaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"},\"zaxis\":{\"backgroundcolor\":\"rgb(17,17,17)\",\"gridcolor\":\"#506784\",\"gridwidth\":2,\"linecolor\":\"#506784\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#C8D4E3\"}},\"shapedefaults\":{\"line\":{\"color\":\"#f2f5fa\"}},\"sliderdefaults\":{\"bgcolor\":\"#C8D4E3\",\"bordercolor\":\"rgb(17,17,17)\",\"borderwidth\":1,\"tickwidth\":0},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"},\"bgcolor\":\"rgb(17,17,17)\",\"caxis\":{\"gridcolor\":\"#506784\",\"linecolor\":\"#506784\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"updatemenudefaults\":{\"bgcolor\":\"#506784\",\"borderwidth\":0},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#283442\",\"linecolor\":\"#506784\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#283442\",\"zerolinewidth\":2}}},\"xaxis\":{\"ticksuffix\":\"\",\"griddash\":\"dash\"},\"title\":{\"text\":\"Atributions and Words\"},\"paper_bgcolor\":\"rgba(0, 0, 0, 0)\",\"plot_bgcolor\":\"rgba(0, 0, 0, 0)\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c9e817b7-1b10-4b65-b964-23aab9096179');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "text_with_attributions = ' '.join([f'''<span style=\"color:{colors[i]}\"><b>{words[i]}</b></span>''' for i in range(len(words))])\n",
        "\n",
        "display(HTML(f'Sample: {x_test_sample[0]}\\n'))\n",
        "\n",
        "for i in range(len(preds)):\n",
        "    display(HTML(f'Prediction: (Negative 😔 {round((preds[i][0]) * 100)}% | Positive 😊 {round(preds[i][1] * 100)}%)\\n'))\n",
        "    display(HTML(f'Attributions: {text_with_attributions}'))\n",
        "\n",
        "# Graph the attributions\n",
        "fig = go.Figure(\n",
        "    go.Bar(\n",
        "        x=atributions,\n",
        "        y=words,\n",
        "        orientation='h',\n",
        "        marker_color=colors\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.update_xaxes(\n",
        "    ticksuffix = \"\",\n",
        "    griddash='dash'\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    template='plotly_dark',\n",
        "    title_text=f'Atributions and Words',\n",
        "    paper_bgcolor='rgba(0, 0, 0, 0)',\n",
        "    plot_bgcolor='rgba(0, 0, 0, 0)'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3rVEKZPTy41"
      },
      "source": [
        "Above, you can see the tokens that had a greater influence on the prediction of our sentiment classifier. In this example, the word **\"bland\"** (token **2159**) was the main culprit for this negative classification.  🎭\n",
        "\n",
        "----\n",
        "\n",
        "Return to the [castle](https://github.com/Nkluge-correa/TeenyTinyCastle)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
