{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKoKPWEpfy1v"
      },
      "source": [
        "# Training and Exploring `Word2Vec` models\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1fBIX57Op-lcyjqO-hH5jb7RqJu0cZuCT\" target=\"_blank\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\">\n",
        "</a>\n",
        "\n",
        "Return to the [castle](https://github.com/Nkluge-correa/TeenyTinyCastle).\n",
        "\n",
        "Nearly all contemporary language models, rooted in neural network architectures, rely on dense word embeddings as a cornerstone for language representation. These embeddings, often derived from pre-trained models like `Word2Vec` and `GloVe`, or contextualized representations like `ELMo` or `BERT`, encode semantic and syntactic information into fixed-size vectors. This process transforms words into continuous numerical vectors that capture their contextual meanings and relationships within a given corpus.\n",
        "\n",
        "These embedding matrices serve as a go-to target for individuals seeking to delve into and interpret the inner workings of language models, particularly in understanding how they represent language. By examining the geometric relationships between word embeddings in the high-dimensional space, researchers can gain insights into how language models organize and process linguistic information. This exploration provides valuable insights into the semantic and syntactic structures encoded within the embeddings, shedding light on how language models capture nuances in meaning and context.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*T8WWibd7u8b7gfgeG0LgAA.gif\" width=400 />\n",
        "\n",
        "[Source](https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794).\n",
        "\n",
        "In this tutorial, we will analyze (while also training) a `Word2Vec` model. `Word2Vec` is a popular natural language processing technique representing words in a high-dimensional vector space. It is a neural network-based approach used to create distributed representations of words based on their co-occurrence patterns in a given text corpus.\n",
        "\n",
        "The basic idea behind `Word2Vec` is that words used in similar contexts tend to have similar meanings. So, if two words appear in similar contexts, they should be close to each other in the vector space.\n",
        "\n",
        "> **Note:** To learn more, \"_[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)_\" is the orginal study in which `Word2Vec` was proposed.\n",
        "\n",
        "Two main techniques are used to create a `Word2Vec` model: Continuous Bag of Words (**CBOW**) and **Skip-gram**.\n",
        "\n",
        "- **CBOW** is an algorithm to predict a target word based on its surrounding context words. The algorithm takes a window of context words as input and generates a probability distribution over the vocabulary of words for the target word.\n",
        "\n",
        "- **Skip-gram**, on the other hand, is an algorithm to predict context words given a target word. The algorithm takes a target word as input and generates a probability distribution over the vocabulary of words for the context words.\n",
        "\n",
        "In this tutorial, we will explore a `skip-gram` approach. First, we will explore what skip-grams are, and finally, we will train a word2vec us with the [News Category Dataset](xxx), available on Hub. ðŸ¤—\n",
        "\n",
        "While in CBOW, we predict a word based on the words that come before and after it, a skip-gram model seeks to predict the words that come before and after a given word (which is the inverse of CBOW). The model is trained using special groups of words called skip-grams. _But what is a skip-gram?_\n",
        "\n",
        "Let us consider the following sentence:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_uR8gnaafy1y"
      },
      "outputs": [],
      "source": [
        "sentence = \"\"\"There is a missing word in this sentence.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y_3V6V_fy1z"
      },
      "source": [
        "When counting skip-grams, we need to define a window size. The window size represents the context window for this sentence. In other words, the window size determines the span of words on either side of a target word that can be considered a context word. For example, a window of 2 means we only look up to two words to the left and right, and so forth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKbBnGeJfy1z",
        "outputId": "c354c360-9e4c-4221-cd11-d978701a7346"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 10 skip_grams of window_size 2 in 'There is a missing word in this sentence.'.\n",
            "Sentence size:  8 \n",
            "\n",
            "('There', 'is')\n",
            "('There', 'a')\n",
            "('is', 'a')\n",
            "('is', 'missing')\n",
            "('is', 'There')\n",
            "('a', 'missing')\n",
            "('a', 'word')\n",
            "('a', 'There')\n",
            "('a', 'is')\n",
            "('missing', 'word')\n"
          ]
        }
      ],
      "source": [
        "# Initialize an empty list to store skip-grams\n",
        "skip_grams = []\n",
        "\n",
        "# Iterate through each word in the sentence\n",
        "for i, word in enumerate(sentence.split()):\n",
        "\n",
        "    # Create skip-grams within a window of size 2\n",
        "    # Forward direction: iterate over words within the next 2 positions\n",
        "    for j in range(i+1, min(i+3, len(sentence.split()))):\n",
        "        skip_grams.append((word, sentence.split()[j]))\n",
        "\n",
        "    # Backward direction: iterate over words within the previous 2 positions\n",
        "    for j in range(max(i-2, 0), i):\n",
        "        skip_grams.append((word, sentence.split()[j]))\n",
        "\n",
        "print(f\"\"\"First 10 skip_grams of window_size 2 in '{sentence}'.\"\"\")\n",
        "print(\"Sentence size: \", len(sentence.split()), \"\\n\")\n",
        "\n",
        "for skip in skip_grams[:10]:\n",
        "    print(skip)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loShLHNwfy10"
      },
      "source": [
        "In simple terms, the skip-gram model tries to guess the words that will likely appear around a given word. The goal is to make the model good at predicting these surrounding words. This objective can be written as the average log probability:\n",
        "\n",
        "$$\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, j\\neq0} \\log p(w_{t+j} | w_{t})$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $T$ is the total number of words in the training corpus.\n",
        "- $c$ is the size of the context window.\n",
        "- $w_t$ is the target word at position $t$ in the corpus.\n",
        "- $w_{t+j}$ is the context word at position $t+j$ in the same context window.\n",
        "- $p(w_{t+j} | w_{t})$ is the conditional probability of the context word given the target word, which the skip-gram model estimates.\n",
        "\n",
        "\n",
        "The softmax formulation for the skip-gram model can then be written as:\n",
        "\n",
        "$$p(w_O | w_I) = \\frac{\\exp(v'{w_O} \\cdot v{w_I})}{\\sum_{w=1}^{W} \\exp(v'{w} \\cdot v{w_I})}$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- $w_I$ is the input (target) word.\n",
        "- $w_O$ is the output (context) word.\n",
        "- $v_{w_I}$ and $v'_{w_O}$ are the input and output vector representations of words $w_I$ and $w_O$ respectively.\n",
        "- $W$ is the size of the vocabulary of words.\n",
        "- The dot (Â·) represents the dot product of two vectors.\n",
        "\n",
        "While the numerator computes the similarity between the input and output word vectors using the dot product, the denominator is a normalization term that sums up the input word's similarities with all the vocabulary words. The resulting probability distribution is over all the words in the vocabulary and is used to estimate the conditional probability of observing an output word given an input word.\n",
        "\n",
        "Instead of using a softmax, which, when involving a lot of words, might be slow to calculate, we can use noise contrastive estimation (NCE) to make the computation more efficient, which simplifies the process by using negative sampling. The idea behind negative sampling is to randomly select a few words unrelated to the target word and use them to train the model. The model learns to distinguish between the context word and randomly chosen words, which helps it better understand the target word.\n",
        "\n",
        "In this simplified approach, we select a few random words (called negative samples) and try to train the model to distinguish them from the context word. A negative sample is a pair of words where the context word is not near the target word. For example, if the target word is \"_missing_\" and the context window is two, then a negative sample could be \"_algebra_\" because \"_algebra_\" is not in the window size neighborhood of \"_missing_\" in our sentence example.\n",
        "\n",
        "In practice, our model will not work with words but with tokens. Thus, let us create a tokenization dictionary for our custom sentence.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBywXNeXfy10",
        "outputId": "f598c339-57b6-4489-9b93-1b70860b4ea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'': 0, 'There': 1, 'is': 2, 'a': 3, 'missing': 4, 'word': 5, 'in': 6, 'this': 7, 'sentence.': 8}\n",
            "{0: '', 1: 'There', 2: 'is', 3: 'a', 4: 'missing', 5: 'word', 6: 'in', 7: 'this', 8: 'sentence.'}\n",
            "Our tokenized sequence:  [1, 2, 3, 4, 5, 6, 7, 8]\n",
            "Decoded sequence:  ['There', 'is', 'a', 'missing', 'word', 'in', 'this', 'sentence.']\n"
          ]
        }
      ],
      "source": [
        "# Initialize an empty dictionary to store the vocabulary and an index counter\n",
        "vocab, index = {}, 1\n",
        "\n",
        "# Assign index 0 to an empty string, which serves as a padding token\n",
        "vocab[''] = 0\n",
        "\n",
        "# Iterate through each token (word) in the sentence\n",
        "for token in sentence.split():\n",
        "    # If the token is not already in the vocabulary\n",
        "    if token not in vocab:\n",
        "        # Add the token to the vocabulary with its corresponding index\n",
        "        vocab[token] = index\n",
        "        # Increment the index counter for the next token\n",
        "        index += 1\n",
        "\n",
        "# Calculate the size of the vocabulary\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Create an inverse vocabulary mapping index to token\n",
        "inverse_vocab = {index: token for token, index in vocab.items()}\n",
        "\n",
        "print(vocab)\n",
        "print(inverse_vocab)\n",
        "\n",
        "print(\"Our tokenized sequence: \", [vocab[word] for word in sentence.split()])\n",
        "\n",
        "print(\"Decoded sequence: \", [inverse_vocab[index] for index in [vocab[word] for word in sentence.split()]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh6-pr7Kfy10"
      },
      "source": [
        "We could use the for loop implemented in our second code cell to create `skip-grams`. However, there is no need to reinvent the wheel. The [`tf.keras.preprocessing.sequence`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence) module provides the [`tf.keras.preprocessing.sequence.skipgrams`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/skipgrams) that can do this heavy lifting for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBbG4V6Sfy10",
        "outputId": "2cd45e49-d3ce-43a8-fb51-bc78af24d8f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(word, missing)\n",
            "(a, There)\n",
            "(There, is)\n",
            "(is, a)\n",
            "(missing, word)\n",
            "(There, a)\n",
            "(in, missing)\n",
            "(word, this)\n",
            "(this, sentence.)\n",
            "(a, missing)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Tokenize the input sentence using the previously created vocabulary\n",
        "tokenized_sentence = [vocab[word] for word in sentence.split()]\n",
        "\n",
        "# Generate positive skip-grams using TensorFlow's skipgrams function\n",
        "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "      tokenized_sentence,\n",
        "      vocabulary_size=vocab_size,\n",
        "      window_size=2,\n",
        "      negative_samples=0)\n",
        "\n",
        "for target, context in positive_skip_grams[:10]:\n",
        "    print(f\"({inverse_vocab[target]}, {inverse_vocab[context]})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FORQTFmsfy10"
      },
      "source": [
        "The skip-grams function looks for pairs of words that appear together within a certain window span. These pairs are called **positive skip-grams**.\n",
        "\n",
        "However, we also need **negative samples**. As mentioned, these are pairs of words that don't appear together. To create negative samples, we randomly choose words from the vocabulary that are not in the same window as the **positive skip-grams**.\n",
        "\n",
        "We use a function called [`tf.random.log_uniform_candidate_sampler`](https://www.tensorflow.org/api_docs/python/tf/random/log_uniform_candidate_sampler) to do this. This function randomly selects words from the vocabulary to create negative samples. We tell the function how many negative samples we want (`num_ns`) and give it the positive skip-gram's target word and context word. The context word is marked as `True` so that it won't be chosen as a negative sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3dGm0Ktfy10",
        "outputId": "fcc6a930-9a4d-4359-c688-04fcf54d208c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Sentence:  There is a missing word in this sentence.\n",
            "Positive skip-grams: (word, missing)\n",
            "Negative samples:  ['a', 'There', 'word', 'is']\n"
          ]
        }
      ],
      "source": [
        "# Extract the target word and context word from the first positive skip-gram\n",
        "target_word, context_word = positive_skip_grams[0]\n",
        "\n",
        "# Define the number of negative samples\n",
        "num_ns = 4\n",
        "\n",
        "# Reshape the context word to a tensor shape required by the negative sampling function\n",
        "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
        "\n",
        "# Generate negative samples using TensorFlow's random log uniform candidate sampler\n",
        "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "    true_classes=context_class,\n",
        "    num_true=1,\n",
        "    num_sampled=num_ns,\n",
        "    unique=True,\n",
        "    range_max=vocab_size,\n",
        "    seed=42,\n",
        "    name=\"negative_sampling\"\n",
        ")\n",
        "\n",
        "# Print original sentence, positive skip-gram, and negative samples\n",
        "print(\"Original Sentence: \", sentence)\n",
        "print(f\"Positive skip-grams: ({inverse_vocab[target_word]}, {inverse_vocab[context_word]})\")\n",
        "print(\"Negative samples: \", [inverse_vocab[index.numpy()] for index in negative_sampling_candidates])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8hUP1-Efy11"
      },
      "source": [
        "Now that we have positive and negative samples, we can combine them to create a set of training examples. For each positive skip-gram pair (`target_word`, `context_word`), we also have `num_ns` negative samples (words that don't appear in the same window). We group these positive and negative samples into a single set. Each positive sample is labeled 1, and each negative sample is labeled 0. So, for every target word, we end up with a set of positive skip-grams and negative samples that can be used to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLLyDYDEfy11",
        "outputId": "4c547b60-bd1a-4494-d135-a37d23648729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "One training sample: {\n",
            "target token    : 5\n",
            "target word     : word\n",
            "context tokens : [4 3 1 5 2]\n",
            "context words   : ['missing', 'a', 'There', 'word', 'is']\n",
            "labels           : [1 0 0 0 0]\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "context = tf.concat([tf.squeeze(context_class, 1), negative_sampling_candidates], 0)\n",
        "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "target = target_word\n",
        "\n",
        "print(f\"\"\"\n",
        "One training sample: {{\n",
        "target token    : {target}\n",
        "target word     : {inverse_vocab[target_word]}\n",
        "context tokens : {context}\n",
        "context words   : {[inverse_vocab[c.numpy()] for c in context]}\n",
        "labels           : {label}\n",
        "}}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYQI6Kczfy11"
      },
      "source": [
        "When we have a large dataset, we also have a lot of words to work with. Some words, like \"_the_\", \"_is_\", and \"_on_\", appear very frequently and don't provide much useful information to the model. However, we can remove some of these very frequent words from the training data to deal with this.\n",
        "\n",
        "The [`tf.keras.preprocessing.sequence.skipgrams`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/skipgrams) function can be used to subsample these frequent words by giving it a list of probabilities that tell it how likely each word is to be sampled. To get these values, we can use the [`tf.keras.preprocessing.sequence.make_sampling_table`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/make_sampling_table) function. This function generates a list of probabilities based on the frequency of each word in the dataset.\n",
        "\n",
        "Finally, now that we have described all the necessary steps to preprocess text data for training word embeddings using the skip-gram model, we can compile them into a function. Once this function is defined, we can use it in the later sections to preprocess our text data and prepare it for training our `Word2Vec` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Cm88WB2pfy11"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  \"\"\"\n",
        "    Generate training data for a skip-gram model using negative sampling.\n",
        "\n",
        "    Args:\n",
        "        sequences: A list of sequences, where each sequence is a list of integers\n",
        "        representing words.\n",
        "        window_size: An integer, the size of the window for generating skip-grams.\n",
        "        num_ns: An integer, the number of negative samples to use for each positive sample.\n",
        "        vocab_size: An integer, the size of the vocabulary.\n",
        "        seed: An integer, the random seed to use for sampling.\n",
        "\n",
        "    Returns:\n",
        "        Three lists: targets, contexts, and labels.\n",
        "        Targets is a list of integers representing target words, contexts is a list of lists\n",
        "        of integers representing context words and negative samples, and labels is a list of\n",
        "        lists of integers representing the labels for each context. Specifically, each label\n",
        "        list has a 1 in the first position (representing the positive sample) and 0s in the\n",
        "        remaining positions (representing the negative samples).\n",
        "  \"\"\"\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence,\n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "\n",
        "      context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1,\n",
        "          num_sampled=num_ns,\n",
        "          unique=True,\n",
        "          range_max=vocab_size,\n",
        "          seed=seed,\n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmYdXu6Rfy11"
      },
      "source": [
        "Now, we need some text. For this, we will use the [News Category Dataset](https://huggingface.co/datasets/AiresPucrs/News-Category-Dataset), created by [Rishabh Misra](https://arxiv.org/abs/2209.11429)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519,
          "referenced_widgets": [
            "72eb871926d042b1aca74daf7c26b659",
            "de496f97cfc34af7818fb31cf29c4c11",
            "9d39d7df9245458ca8b9e0593711c9dd",
            "0192567abaa64276abe62a9e5c6fdbe1",
            "3ab1c12bdc8d4365a581852a4e9349aa",
            "469703d8ff0942ee9e94c9afb042e803",
            "e6ac808ed93d468892be9267cdf49d4a",
            "dbe6bf4487b2474fb60e81b02d0837bc",
            "91f3628b081c455d9dbeda681e778cfb",
            "d460256057374466bda0cb102b66f93d",
            "b712427650f64d44a874e8167e707f59",
            "2877e8aa20e14560bb8dfbee23df5ce4",
            "5a7b25f5e2bc417db2c2eca5ef369f31",
            "b1d3969f3d29449ba5872f67df9f810e",
            "ec6ee5a76a4d444cb203ae7053c1e0a0",
            "caf657fd31344f78894e7369a3d8e9ce",
            "913f5c5bc19d486f97e43bfac4ffbf98",
            "889f7a0235ad40ddbe9051e902661c87",
            "8a9f9acad9ed43d2ace7b20939764663",
            "ef3eddd1c6ce41a7bcef8dbe9debc398",
            "e60f12cd73a0401cb39146182a7275e6",
            "cd84172960144a4e94977cb88cd3712b",
            "af376ae684f9460b9e20911eec42af6a",
            "49d089078a0049ea89487f08adf26638",
            "1544948976314140935ed61c6725f68e",
            "11ec2dad76814ec7b2adc7afa62adfd0",
            "b4f5b48411124098bbab61de1ed854e2",
            "7ec3c8cae11841d4bdf4b084a654d3e9",
            "88692981623d40baa21f8d2c037b3047",
            "988ff6c66d354b44a35ed49d9d91cfe3",
            "8c08d8fc094f433297adb6f5341b504f",
            "ee0cc7ff1bfe49e3a72ab941de593b7c",
            "5f9817ef0ca949e694f6a49a3044b4f3"
          ]
        },
        "id": "bTQ--PSt9vWO",
        "outputId": "2768c13f-ad15-4f2e-bcfa-f0efeaf8a293"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72eb871926d042b1aca74daf7c26b659",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/314 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2877e8aa20e14560bb8dfbee23df5ce4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/27.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af376ae684f9460b9e20911eec42af6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/209527 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c2ae708e-94d7-46d9-8981-f44ab07bf769\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
              "      <td>COMEDY</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
              "      <td>PARENTING</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
              "      <td>U.S. NEWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209522</th>\n",
              "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
              "      <td>TECH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209523</th>\n",
              "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
              "      <td>SPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209524</th>\n",
              "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
              "      <td>SPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209525</th>\n",
              "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
              "      <td>SPORTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209526</th>\n",
              "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
              "      <td>SPORTS</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>209527 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2ae708e-94d7-46d9-8981-f44ab07bf769')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c2ae708e-94d7-46d9-8981-f44ab07bf769 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c2ae708e-94d7-46d9-8981-f44ab07bf769');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-712cfe72-0fce-4b19-9d0c-6e54cd88e0bf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-712cfe72-0fce-4b19-9d0c-6e54cd88e0bf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-712cfe72-0fce-4b19-9d0c-6e54cd88e0bf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                     text     labels\n",
              "0       Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS\n",
              "1       American Airlines Flyer Charged, Banned For Li...  U.S. NEWS\n",
              "2       23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY\n",
              "3       The Funniest Tweets From Parents This Week (Se...  PARENTING\n",
              "4       Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS\n",
              "...                                                   ...        ...\n",
              "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...       TECH\n",
              "209523  Maria Sharapova Stunned By Victoria Azarenka I...     SPORTS\n",
              "209524  Giants Over Patriots, Jets Over Colts Among  M...     SPORTS\n",
              "209525  Aldon Smith Arrested: 49ers Linebacker Busted ...     SPORTS\n",
              "209526  Dwight Howard Rips Teammates After Magic Loss ...     SPORTS\n",
              "\n",
              "[209527 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install datasets -q\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"AiresPucrs/News-Category-Dataset\", split=\"train\")\n",
        "\n",
        "display(dataset.to_pandas())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hZwBlTzopSK"
      },
      "source": [
        "Now, we will create a folder to store the examples we will use in our dataset, given that we will be using the [`TextLineDataset`](https://www.tensorflow.org/api_docs/python/tf/data/TextLineDataset) from TensorFlow, which creates a tf.dataset comprising lines from one or more text files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8EciSPFfy11",
        "outputId": "544f105a-0f92-4c18-e698-58c569943f59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:23<00:00,  1.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Folder Created!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tqdm\n",
        "\n",
        "# Convert the dataset to a pandas DataFrame\n",
        "dataset = dataset.to_pandas()\n",
        "\n",
        "# Create a new directory named \"dataset\"\n",
        "os.makedirs(\"dataset/\", exist_ok=True)\n",
        "\n",
        "# Iterate over unique labels (categories) in the dataset\n",
        "for category in tqdm.tqdm(dataset.labels.unique()):\n",
        "    # Create a subdirectory for each category within the \"dataset\" directory\n",
        "    os.mkdir(f\"dataset/{category}\")\n",
        "\n",
        "    # Filter the dataset to include only samples belonging to the current category\n",
        "    dff = dataset[dataset['labels'] == category]\n",
        "\n",
        "    # Iterate over each sample in the filtered dataset\n",
        "    for i, sample in enumerate(list(dff.text)):\n",
        "        # Write the text sample to a text file named \"{i}.txt\" within the corresponding category subdirectory\n",
        "        with open(f'dataset/{category}/{i}.txt', 'w', encoding='utf-8') as fp:\n",
        "            fp.write(sample)\n",
        "            fp.close()\n",
        "\n",
        "print('Dataset Folder Created!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz4yepZmfy11"
      },
      "source": [
        "This dataset contains 42 directories with 209.527 files. However, this tutorial will use only a portion (a little more than 35K samples from the **\"POLITICS\"** folder).\n",
        "\n",
        "> **Note:** You can use more folders or a completely different text dataset. Remember that the more text you give, the longer it will be to train the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAB16yj9fy12",
        "outputId": "2b55f44a-4201-40b9-d34f-00da431da5b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 35602 files.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "filenames = []\n",
        "\n",
        "for folder in os.listdir(\"dataset/POLITICS\"):\n",
        "    filenames.append(os.path.join(\"dataset/POLITICS\", folder))\n",
        "\n",
        "print(f\"Found {len(filenames)} files.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fydc3y54fy12"
      },
      "source": [
        "With all of our text files listed in `filenames`, we can create a dataset using the `tf.data.TextLineDataset`, which loads text from text files and creates a dataset where each line of the files becomes an element of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Vfqc6ByAfy12"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "# Shuffle the list of filenames randomly\n",
        "random.shuffle(filenames)\n",
        "\n",
        "# Create a TextLineDataset from the shuffled filenames\n",
        "text_ds = tf.data.TextLineDataset(filenames)\n",
        "\n",
        "# Batch the dataset into batches of size 1024\n",
        "text_ds = text_ds.batch(1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJGKDvDqfy12"
      },
      "source": [
        "To create a tokenizer (i.e., vectorization layer), we will use the [`tf.keras.layers.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization), which maps text features to integer sequences. Meanwhile, we will pass a custom standardization function to lower strings and parse punctuations from our text samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QYEcoG3Jfy12"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "# Lower all strings and parse punctuation and symbols\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "# Maximum vocabulary size and will cut sequences with more than 100 tokens\n",
        "vocab_size = 10000\n",
        "sequence_length = 100\n",
        "\n",
        "# Create a vectorization layer and adapt it to the text\n",
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length,\n",
        "    encoding='utf-8')\n",
        "\n",
        "# Fit the TextVectorization layer to the dataset\n",
        "vectorize_layer.adapt(text_ds)\n",
        "\n",
        "# Get words back from token indices\n",
        "word2vec_vocabulary = vectorize_layer.get_vocabulary()\n",
        "\n",
        "# Save the vocabulary as a text file\n",
        "with open(f'word2vec_vocabulary.txt', 'w', encoding='utf-8') as fp:\n",
        "    for word in word2vec_vocabulary:\n",
        "        fp.write(\"%s\\n\" % word)\n",
        "    fp.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xoZN4Uvfy12"
      },
      "source": [
        "To get a dataset ready for training a `Word2Vec` model, you need to convert the dataset into a list of tokenized and batched sequences. Since we are implementing skip-gram, we must go through each sentence in the dataset and use it to create positive and negative examples to feed our model during training. For this, we will re-use our `generate_training_data` function.\n",
        "\n",
        "> **Note:** This function looks at each word in each sentence and uses them to create examples to teach the model how to predict related words. The function creates three lists - target words, context words, and labels - each list has the same number of items, representing the total number of examples the model will be trained on.\n",
        "\n",
        "When training a `Word2Vec` model, there are two critical things to consider: how big the `window_size` of words you're looking at and how many negative samples (`num_ns`) you're including.\n",
        "\n",
        "Different window sizes can be more beneficial depending on what you're trying to accomplish. Generally, smaller window sizes (2-15) will give you embeddings where words with similar meanings are treated as interchangeable, even if they're opposite. Larger window sizes (15-50 or more) will give you embeddings where related words, which are not necessarily interchangeable, will have higher similarity scores.\n",
        "\n",
        "For a more complete explanation of the effect `window size` has, [watch this video](https://www.youtube.com/watch?v=tAxrlAVw-Tk&t=648s).\n",
        "\n",
        "Regarding the number of `num_ns`, the [original paper](https://arxiv.org/abs/1301.3781) prescribes 5-20 as being a good number of negative samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOie0UZqfy12",
        "outputId": "bddb53a6-f29b-4676-9069-08ef9cee59d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have  35626  sequences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35626/35626 [03:42<00:00, 160.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Targets: (637410,)\n",
            "Contexts: (637410, 5)\n",
            "Labels: (637410, 5)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define AUTOTUNE for TensorFlow data pipeline optimization\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Prefetch the text dataset for performance optimization and apply vectorization to each element\n",
        "text_vector_ds = text_ds.prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
        "\n",
        "# Convert the text dataset into a list of sequences\n",
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(\"We have \", len(sequences), \" sequences.\")\n",
        "\n",
        "# Define window size and number of negative samples\n",
        "window_size = 2\n",
        "num_ns = 4\n",
        "\n",
        "# Generate training data (targets, contexts, labels) based on the sequences\n",
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences,\n",
        "    window_size=window_size,\n",
        "    num_ns=num_ns,\n",
        "    vocab_size=vocab_size,\n",
        "    seed=42)\n",
        "\n",
        "targets = np.array(targets)\n",
        "contexts = np.array(contexts)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(f\"Targets: {targets.shape}\")\n",
        "print(f\"Contexts: {contexts.shape}\")\n",
        "print(f\"Labels: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pf1Ylchbfy12"
      },
      "source": [
        "Depending on the chosen `window_size` and `num_ns`, the generation of our dataset can take a while. We created a 10.000 word Word2Vec vocabulary using the below sections from our dataset:\n",
        "\n",
        "- \"dataset/POLITICS\".\n",
        "- \"dataset/WORLD NEWS\".\n",
        "- \"dataset/ENTERTAINMENT\".\n",
        "- \"dataset/ENVIRONMENT\".\n",
        "- \"dataset/EDUCATION\".\n",
        "- \"dataset/SCIENCE\".\n",
        "- \"dataset/WELLNESS\".\n",
        "\n",
        "We also created two sets of [`targets`, `contexts`, and `labels`]. One has a `window_size` of 2, and the other has a `window_size` of 15. You can compare all of them to see how the increase in text data and `window_size` affect your `Word2Vec`model.\n",
        "\n",
        "All of these are available in the repository tied to the training of our Word2Vece model. To download them, run:\n",
        "\n",
        "```bash\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/AiresPucrs/Word2Vec\n",
        "```\n",
        "\n",
        "We will be using one of these already prepared datasets, but feel free to use or create the dataset as you wish!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20oAIyoRfy13",
        "outputId": "83a920dc-8005-46c8-dca4-3f0cad0b9783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Targets: (1569949,)\n",
            "Contexts: (1569949, 5)\n",
            "Labels: (1569949, 5)\n",
            "Vocabulary Size: 10000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/AiresPucrs/Word2Vec\n",
        "\n",
        "# Define window size and number of negative samples\n",
        "window_size = 2\n",
        "num_ns = 4\n",
        "\n",
        "# Load preprocessed training data from files\n",
        "with open(f'./word2vec/w2v_dataset_w{window_size}_nn{num_ns}.npy', 'rb') as fp:\n",
        "    targets = np.load(fp)  # Load targets\n",
        "    contexts = np.load(fp)  # Load contexts\n",
        "    labels = np.load(fp)    # Load labels\n",
        "    fp.close()\n",
        "\n",
        "# Load word2vec vocabulary from file\n",
        "with open('./word2vec/word2vec_vocabulary.txt', encoding='utf-8') as fp:\n",
        "    word2vec_vocabulary = [line.strip() for line in fp]  # Read vocabulary lines\n",
        "    fp.close()\n",
        "\n",
        "print(f\"Targets: {targets.shape}\")\n",
        "print(f\"Contexts: {contexts.shape}\")\n",
        "print(f\"Labels: {labels.shape}\")\n",
        "print(f\"Vocabulary Size: {len(word2vec_vocabulary)}\")\n",
        "\n",
        "# Create a TensorFlow dataset from loaded data\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "\n",
        "# Shuffle and batch the dataset\n",
        "dataset = dataset.shuffle(10000).batch(1024, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZipS3bsfy13"
      },
      "source": [
        "As said before, the `Word2Vec` model is a tool that can help us tell which words go together by looking at how often they appear near each other in sentences. It does this by comparing the meanings of different words and figuring out which ones are similar.\n",
        "\n",
        "To train the model, we can give it pairs of words and ask it to predict whether they belong together. We can check if the model is correct by comparing its predictions to the actual pairs of words that we already know go together. The model improves over time as it learns from more and more examples of word pairs.\n",
        "\n",
        "To create your `Word2Vec` model, you can use the Keras Subclassing API. Let us have a breakdown of our implementation:\n",
        "\n",
        "- The first layer will be the `target_embedding` layer, responsible for finding the meaning of a word when it is a target. The size of this layer depends on the size of our vocabulary and the dimension of your `embeddings`.\n",
        "- The second layer will be the `context_embedding` layer, responsible for finding the meaning of a word when it is in the context of another word. It has the same number of parameters as the `target_embedding` layer.\n",
        "- The `dots` layer combines the `target` and `context` embeddings and calculates a dot product.\n",
        "- The `flatten` layer takes the output of the dots layer and makes it flat.\n",
        "\n",
        "You can then define a `call()` function that takes a pair of words (`target` and `context`) and passes them through the target and context `embedding layers`, performs a dot product with their output, and returns the flattened result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "PIcDGZKAfy13"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Word2Vec(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Word2Vec model class for training word embeddings using skip-gram.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        \"\"\"\n",
        "        Initialize the Word2Vec model.\n",
        "\n",
        "        Parameters:\n",
        "            - vocab_size (int): Size of the vocabulary.\n",
        "            - embedding_dim (int): Dimensionality of word embeddings.\n",
        "        \"\"\"\n",
        "        super(Word2Vec, self).__init__()  # Initialize the parent class (tf.keras.Model)\n",
        "\n",
        "        # Define target embedding layer\n",
        "        self.target_embedding = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                          embedding_dim,\n",
        "                                                          input_length=1,\n",
        "                                                          name=\"w2v_target_embedding\")\n",
        "\n",
        "        # Define context embedding layer\n",
        "        self.context_embedding = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                           embedding_dim,\n",
        "                                                           input_length=num_ns + 1,\n",
        "                                                           name=\"w2v_context_embedding\")\n",
        "\n",
        "    def call(self, pair):\n",
        "        \"\"\"\n",
        "        Perform forward pass of the Word2Vec model.\n",
        "\n",
        "        Parameters:\n",
        "            - pair (tuple): Tuple containing target and context words.\n",
        "\n",
        "        Returns:\n",
        "            - dots (tensor): Dot product between target and context embeddings.\n",
        "        \"\"\"\n",
        "        target, context = pair  # Unpack target and context words from the input pair\n",
        "\n",
        "        # Squeeze the target tensor if its shape has two dimensions\n",
        "        if len(target.shape) == 2:\n",
        "            target = tf.squeeze(target, axis=1)\n",
        "\n",
        "        # Retrieve embeddings for target and context words using the embedding layers\n",
        "        word_emb = self.target_embedding(target)  # Embedding for the target word\n",
        "        context_emb = self.context_embedding(context)  # Embedding for the context words\n",
        "\n",
        "        # Compute dot product between target and context embeddings using einsum\n",
        "        dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
        "\n",
        "        return dots  # Return the dot product tensor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUtGKnYffy13"
      },
      "source": [
        "Since our labels are already one-hot-encoded, we will use `CategoricalCrossEntropy` as an alternative to the negative sampling loss and `Adam` as the optimizer. Finally, we instantiate our `Word2Vec` class with an embedding dimension of 512 and a vocabulary size of 10.000 words.\n",
        "\n",
        "> **Note: You can skip this part of the tutorial if you wish to avoid training this model from scratch!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r38E5b4rfy13"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "embedding_dimension = 512\n",
        "\n",
        "word2vec = Word2Vec(vocab_size, embedding_dimension)\n",
        "\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
        "\n",
        "word2vec.fit(dataset, verbose=1, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Fk-Y8wfy13"
      },
      "source": [
        "Now, we can recover the embeddings from both the `target` and `context` embedding layers. These embeddings will now hold some information about the relationship of words in our text corpus.\n",
        "\n",
        "> **Note: For easy access, we also made this available in our [`Word2Vec`](https://huggingface.co/AiresPucrs/Word2Vec) repository.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kO6ywIBUixP1"
      },
      "outputs": [],
      "source": [
        "# Extract the embedding layer from the model\n",
        "embeddings_target = word2vec.get_layer('w2v_target_embedding').get_weights()[0]\n",
        "embeddings_context = word2vec.get_layer('w2v_context_embedding').get_weights()[0]\n",
        "\n",
        "# Save the embeddings as a numpy array\n",
        "with open('./word2vec/w2v_embeddings_w2.npy', 'wb') as fp:\n",
        "    np.save(fp, embeddings_target)\n",
        "    np.save(fp, embeddings_context)\n",
        "    fp. close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RluCNXM1fy13"
      },
      "source": [
        "Let us now load our pre-trained embeddings (trained with a `window_size` of 15) to explore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b4-qWRhfy14",
        "outputId": "1d32eea3-bad8-4741-c4b4-64d8e9c540be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target Embeddings shape: (10000, 512)\n",
            "Context Embeddings shape: (10000, 512)\n",
            "Vocabulary Size: 10000\n"
          ]
        }
      ],
      "source": [
        "# !git lfs install\n",
        "# !git clone https://huggingface.co/AiresPucrs/Word2Vec\n",
        "\n",
        "with open('./word2vec/w2v_embeddings_w15.npy', 'rb') as fp:\n",
        "    embeddings_target = np.load(fp)\n",
        "    embeddings_context = np.load(fp)\n",
        "    fp.close()\n",
        "\n",
        "with open('./word2vec/word2vec_vocabulary.txt', encoding='utf-8') as fp:\n",
        "    word2vec_vocabulary = [line.strip() for line in fp]\n",
        "    fp.close()\n",
        "\n",
        "print(f\"Target Embeddings shape: {embeddings_target.shape}\")\n",
        "print(f\"Context Embeddings shape: {embeddings_context.shape}\")\n",
        "print(f\"Vocabulary Size: {len(word2vec_vocabulary)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH1fFPg4fy14"
      },
      "source": [
        "To associate each embedding with a human-readable string, we must pair our embeddings with our vocabulary, as done bellow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "0v7RhAU2fy18"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary of \"word: embedding\"\n",
        "word2vec_target_embeddings = {}\n",
        "word2vec_context_embeddings = {}\n",
        "\n",
        "# Iterating through the elements of the vocabulary\n",
        "for i, word in enumerate(word2vec_vocabulary):\n",
        "    # here we skip the embedding/token 0 (\"\"), because is just the PAD token.\n",
        "    if i == 0:\n",
        "        continue\n",
        "    word2vec_target_embeddings[word] = embeddings_target[i]\n",
        "    word2vec_context_embeddings[word] = embeddings_context[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNjadwLkfy18"
      },
      "source": [
        "Finally, we can perform some basic operations (`cosine similarity`) to understand and interpret what our model has learned, both for the `target` and `cosine` embeddings. While `target embeddings` hold information on **\"relatedness among words\"**, `context embeddings` hold information on **\"what words usually accompany the target word\"**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "5_9MGiTzfy18",
        "outputId": "3b570005-ac6c-45db-fbfa-b6f65e825908"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity for Target Embeddings\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "| Closest Match   |   Similarity Score |\n",
              "|:----------------|-------------------:|\n",
              "| donald          |           0.194642 |\n",
              "| counsel         |           0.194392 |\n",
              "| mexicos         |           0.192862 |\n",
              "| insider         |           0.184099 |\n",
              "| arpaio          |           0.183056 |\n",
              "| pence           |           0.181958 |\n",
              "| white           |           0.178896 |\n",
              "| trevor          |           0.17812  |\n",
              "| trumps          |           0.177152 |\n",
              "| scarborough     |           0.176633 |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity for Context Embeddings\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "| Closest Match   |   Similarity Score |\n",
              "|:----------------|-------------------:|\n",
              "| donald          |           0.67039  |\n",
              "| on              |           0.4248   |\n",
              "| with            |           0.400963 |\n",
              "| his             |           0.398482 |\n",
              "| tweeters        |           0.388154 |\n",
              "| trumps          |           0.386425 |\n",
              "| Â¯ãƒ„Â¯            |           0.385741 |\n",
              "| president       |           0.381984 |\n",
              "| he              |           0.376554 |\n",
              "| taunts          |           0.367066 |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from numpy.linalg import norm\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def compute_cosine_table(string, dictionary,\n",
        "                         vocabulary, top_n):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity between a given word and all other words in a dictionary.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    string : str\n",
        "        The word to compare against.\n",
        "    dictionary : dict\n",
        "        A dictionary with words as keys and their corresponding embeddings as values.\n",
        "    vocabulary : list\n",
        "        A list of words in the dictionary.\n",
        "    top_n : int\n",
        "        The number of closest matches to return.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    A pandas DataFrame with the closest matches to the input word and their\n",
        "    corresponding similarity scores. The DataFrame is sorted in descending\n",
        "    order of similarity score and limited to the top_n matches.\n",
        "    The index of the DataFrame is set to the closest matches.\n",
        "    \"\"\"\n",
        "\n",
        "    l = vocabulary.copy()\n",
        "    l.remove(string)\n",
        "\n",
        "    cos = []\n",
        "    for word in l[1::]:\n",
        "\n",
        "        cosine = np.dot(dictionary[string],\n",
        "                dictionary[word])/(norm(dictionary[string])*norm(dictionary[word]))\n",
        "        cos.append(cosine)\n",
        "\n",
        "    return pd.DataFrame({\"Closest Match\": l[1::],f\"Similarity Score\": cos})\\\n",
        "        .sort_values(f\"Similarity Score\", ascending=False)\\\n",
        "        .set_index('Closest Match').head(top_n)\n",
        "\n",
        "word = \"trump\"\n",
        "\n",
        "df = compute_cosine_table(word,\n",
        "        word2vec_target_embeddings,\n",
        "        word2vec_vocabulary, 10)\n",
        "\n",
        "print(\"Cosine Similarity for Target Embeddings\")\n",
        "display(Markdown(df.to_markdown()))\n",
        "\n",
        "df = compute_cosine_table(word,\n",
        "        word2vec_context_embeddings,\n",
        "        word2vec_vocabulary, 10)\n",
        "\n",
        "print(\"Cosine Similarity for Context Embeddings\")\n",
        "display(Markdown(df.to_markdown()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ECra979fy19"
      },
      "source": [
        "We can also perform basic arithmetic operations with these vector embeddings, which is another way to try to understand the knowledge they hold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_FJ1t-yify19",
        "outputId": "ef45520d-436b-4b41-efc1-48b0c48ec779"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "| Closest Match   |   Similarity Score |\n",
              "|:----------------|-------------------:|\n",
              "| jazz            |           0.214929 |\n",
              "| christina       |           0.188508 |\n",
              "| duff            |           0.185525 |\n",
              "| peek            |           0.184137 |\n",
              "| centers         |           0.181551 |"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def find_closest_match(array, dictionary, vocabulary,\n",
        "                           word1, word2, top_n):\n",
        "    \"\"\"\n",
        "    Computes the cosine similarity between a given array and all other word\n",
        "    embeddings in a dictionary except for two specified words.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    array : numpy.ndarray\n",
        "        An array representing the embedding of a word or phrase.\n",
        "    dictionary : dict\n",
        "        A dictionary with words as keys and their corresponding embeddings as values.\n",
        "    vocabulary : list\n",
        "        A list of words in the dictionary.\n",
        "    word1 : str\n",
        "        The first word to exclude from the matches.\n",
        "    word2 : str\n",
        "        The second word to exclude from the matches.\n",
        "    top_n : int\n",
        "        The number of closest matches to return.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "        A pandas DataFrame with the closest matches to the input array and\n",
        "        their corresponding similarity scores. The DataFrame is sorted in\n",
        "        descending order of similarity score and limited to the top_n matches.\n",
        "        The index of the DataFrame is set to the closest matches.\n",
        "    \"\"\"\n",
        "\n",
        "    l = vocabulary.copy()\n",
        "    l.remove(word1)\n",
        "    l.remove(word2)\n",
        "\n",
        "    cos = []\n",
        "\n",
        "    for word in l[1::]:\n",
        "        cosine = np.dot(array,\n",
        "                dictionary[word])/(norm(array)*norm(dictionary[word]))\n",
        "        cos.append(cosine)\n",
        "\n",
        "    return pd.DataFrame({\"Closest Match\": l[1::],f\"Similarity Score\": cos})\\\n",
        "        .sort_values(f\"Similarity Score\", ascending=False)\\\n",
        "        .set_index('Closest Match').head(top_n)\n",
        "\n",
        "word1 = 'man'\n",
        "word2 = 'music'\n",
        "\n",
        "difference_vec = word2vec_target_embeddings[word1] + word2vec_target_embeddings[word2]\n",
        "\n",
        "df = find_closest_match(difference_vec, word2vec_target_embeddings,\n",
        "                           word2vec_vocabulary, word1, word2, 5)\n",
        "\n",
        "display(Markdown(df.to_markdown()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQmyq-ohfy19"
      },
      "source": [
        "Apperently, **\"man\"** + **\"music\"** get us close to **\"jazz\"** ðŸŽ¶ðŸŽ·.\n",
        "\n",
        "Researchers can uncover interesting semantic relationships and analogies encoded within the embedding space by manipulating word vectors. This approach allows the uncovering of the implicit knowledge and associations embedded in the learned representations, shedding light on how language models understand and interpret linguistic concepts. This exploration can inform the development of more interpretable and transparent NLP models, aiding in tasks such as semantic similarity assessment, concept categorization, and analogy reasoning. Moreover, it provides a means to validate and verify the quality and coherence of learned representations, contributing to the overall trustworthiness and reliability of NLP systems.\n",
        "\n",
        "---\n",
        "\n",
        "Return to the [castle](https://github.com/Nkluge-correa/TeenyTinyCastle)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0192567abaa64276abe62a9e5c6fdbe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d460256057374466bda0cb102b66f93d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b712427650f64d44a874e8167e707f59",
            "value": "â€‡314/314â€‡[00:00&lt;00:00,â€‡19.2kB/s]"
          }
        },
        "11ec2dad76814ec7b2adc7afa62adfd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee0cc7ff1bfe49e3a72ab941de593b7c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5f9817ef0ca949e694f6a49a3044b4f3",
            "value": "â€‡209527/209527â€‡[00:00&lt;00:00,â€‡737865.09â€‡examples/s]"
          }
        },
        "1544948976314140935ed61c6725f68e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_988ff6c66d354b44a35ed49d9d91cfe3",
            "max": 209527,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c08d8fc094f433297adb6f5341b504f",
            "value": 209527
          }
        },
        "2877e8aa20e14560bb8dfbee23df5ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a7b25f5e2bc417db2c2eca5ef369f31",
              "IPY_MODEL_b1d3969f3d29449ba5872f67df9f810e",
              "IPY_MODEL_ec6ee5a76a4d444cb203ae7053c1e0a0"
            ],
            "layout": "IPY_MODEL_caf657fd31344f78894e7369a3d8e9ce"
          }
        },
        "3ab1c12bdc8d4365a581852a4e9349aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "469703d8ff0942ee9e94c9afb042e803": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49d089078a0049ea89487f08adf26638": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ec3c8cae11841d4bdf4b084a654d3e9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_88692981623d40baa21f8d2c037b3047",
            "value": "Generatingâ€‡trainâ€‡split:â€‡100%"
          }
        },
        "5a7b25f5e2bc417db2c2eca5ef369f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_913f5c5bc19d486f97e43bfac4ffbf98",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_889f7a0235ad40ddbe9051e902661c87",
            "value": "Downloadingâ€‡data:â€‡100%"
          }
        },
        "5f9817ef0ca949e694f6a49a3044b4f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72eb871926d042b1aca74daf7c26b659": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de496f97cfc34af7818fb31cf29c4c11",
              "IPY_MODEL_9d39d7df9245458ca8b9e0593711c9dd",
              "IPY_MODEL_0192567abaa64276abe62a9e5c6fdbe1"
            ],
            "layout": "IPY_MODEL_3ab1c12bdc8d4365a581852a4e9349aa"
          }
        },
        "7ec3c8cae11841d4bdf4b084a654d3e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88692981623d40baa21f8d2c037b3047": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "889f7a0235ad40ddbe9051e902661c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a9f9acad9ed43d2ace7b20939764663": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c08d8fc094f433297adb6f5341b504f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "913f5c5bc19d486f97e43bfac4ffbf98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91f3628b081c455d9dbeda681e778cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "988ff6c66d354b44a35ed49d9d91cfe3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d39d7df9245458ca8b9e0593711c9dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbe6bf4487b2474fb60e81b02d0837bc",
            "max": 314,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91f3628b081c455d9dbeda681e778cfb",
            "value": 314
          }
        },
        "af376ae684f9460b9e20911eec42af6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49d089078a0049ea89487f08adf26638",
              "IPY_MODEL_1544948976314140935ed61c6725f68e",
              "IPY_MODEL_11ec2dad76814ec7b2adc7afa62adfd0"
            ],
            "layout": "IPY_MODEL_b4f5b48411124098bbab61de1ed854e2"
          }
        },
        "b1d3969f3d29449ba5872f67df9f810e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a9f9acad9ed43d2ace7b20939764663",
            "max": 27470311,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef3eddd1c6ce41a7bcef8dbe9debc398",
            "value": 27470311
          }
        },
        "b4f5b48411124098bbab61de1ed854e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b712427650f64d44a874e8167e707f59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caf657fd31344f78894e7369a3d8e9ce": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd84172960144a4e94977cb88cd3712b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d460256057374466bda0cb102b66f93d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbe6bf4487b2474fb60e81b02d0837bc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de496f97cfc34af7818fb31cf29c4c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_469703d8ff0942ee9e94c9afb042e803",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e6ac808ed93d468892be9267cdf49d4a",
            "value": "Downloadingâ€‡readme:â€‡100%"
          }
        },
        "e60f12cd73a0401cb39146182a7275e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6ac808ed93d468892be9267cdf49d4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec6ee5a76a4d444cb203ae7053c1e0a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e60f12cd73a0401cb39146182a7275e6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cd84172960144a4e94977cb88cd3712b",
            "value": "â€‡27.5M/27.5Mâ€‡[00:01&lt;00:00,â€‡28.3MB/s]"
          }
        },
        "ee0cc7ff1bfe49e3a72ab941de593b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef3eddd1c6ce41a7bcef8dbe9debc398": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
