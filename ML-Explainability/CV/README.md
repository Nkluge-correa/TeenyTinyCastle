# Machine Learning Explainability (Computer Vision)

You can find ML Explainability/Interpretability lessons in this folder, mainly focusing on applications involving [computer vision](../../ML-Explainability/CV).

Understanding and interpreting the decisions made by machine learning models is essential for building trust and making informed decisions. In this course, we explore various techniques for interpretability in computer vision. From introducing convolutional neural networks with CIFAR-10 to exploring feature visualization, maximum activation manipulation, saliency mapping, and using LIME for interpretation, each tutorial provides insights into the inner workings of CV models.

| Tutorial                                                 | GitHub                                                                                                                                                    | Colab                                                                                                                                                                                            |
|----------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Creating computer vision models for image classification | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/CV/CNN_model_maker.ipynb" target="_blank">LINK</a>                | <a href="https://colab.research.google.com/drive/1-0TkvCFAtmxnt8DOZVKDsTLw3fQa5Mb3" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a> |
| Introduction to feature visualization                    | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/CV/CNN_feature_visualization.ipynb" target="_blank">LINK</a>      | ‚ùå                                                                                                                                                                                                |
| Activation Maximization in CNNs                          | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/CV/CNN_activation_maximization.ipynb" target="_blank">LINK</a>    | <a href="https://colab.research.google.com/drive/1rypOHeIM8l1_oEzZVUhpaueqblYIS7Sw" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a> |
| Introduction to saliency mapping with CNNs               | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/CV/CNN_attribution_maps.ipynb" target="_blank">LINK</a>           | <a href="hhttps://colab.research.google.com/drive/1ti9W_W2LjZxbc-0JU6GZSGOZH3Mt5kHn" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a> |
| Applying LIME to CNNs                                    | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/CV/CNN_attribution_maps_with_LIME.ipynb" target="_blank">LINK</a> | <a href="https://colab.research.google.com/drive/1DW7wY5HOQ-J7GpRQXhvLuXqKT0MWpnuS" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a> |

---

Return to the [castle](https://github.com/Nkluge-correa/TeenyTinyCastle).
