# Machine Learning Explainability (Computer Vision)

You can find ML Explainability/Interpretability lessons in this folder, mainly focusing on applications involving [computer vision](../../ML-Explainability/CV).

Understanding and interpreting the decisions made by machine learning models is essential for building trust and making informed decisions. In this course, we explore various techniques for interpretability in computer vision. From introducing convolutional neural networks with CIFAR-10 to exploring feature visualization, maximum activation manipulation, saliency mapping, and using LIME for interpretation, each tutorial provides insights into the inner workings of CV models.

| Tutorial                                                 | GitHub                                                                                                                                                                         | Colab |
|----------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------|
| Creating computer vision models for image classification                     | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/CV/CNN_model_maker.ipynb" target="_blank">LINK</a>                | <a href="https://colab.research.google.com/drive/1-0TkvCFAtmxnt8DOZVKDsTLw3fQa5Mb3" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a>     |
| Introduction to feature visualization                    | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/CV/CNN_feature_visualization.ipynb" target="_blank">LINK</a>      | ❌     |
| Introduction to maximum activation manipulation          | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/CV/CNN_activation_maximization.ipynb" target="_blank">LINK</a>    | ❌     |
| Introduction to  saliency mapping                        | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/CV/CNN_attribution_maps.ipynb" target="_blank">LINK</a>           | ❌     |
| Introduction to `LIME`                                   | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/CV/CNN_attribution_maps_with_LIME.ipynb" target="_blank">LINK</a> | ❌     |
| Interpreting diffusion models with `diffusers-interpret` | <a href="https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/CV/diffusion_interpreter.ipynb" target="_blank">LINK</a>          | ❌     |

---

Return to the [castle](https://github.com/Nkluge-correa/TeenyTinyCastle).
