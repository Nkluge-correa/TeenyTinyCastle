{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeR8GOMoNaaU"
      },
      "source": [
        "# Creating ML apps with Gradio\n",
        "\n",
        "<a href=\"https://colab.research.google.com/drive/1rghHhR4TShqkYk4qbZvg0N5oAku2USon\" target=\"_blank\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\">\n",
        "</a>\n",
        "\n",
        "Return to the [castle](https://github.com/Nkluge-correa/TeenyTinyCastle).\n",
        "\n",
        "In this repository, you will find examples of how to create simple interfaces for things like, for example, [sentiment analysis](https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/NLP/model_maker.ipynb), and even a [playground for HuggingFace language models](https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/NLP%20Playgroung/playground.py). These interfaces can be used to create ML applications since all of them use things like `Flask` for the backend, and bootstrap and CSS for the front end part.\n",
        "\n",
        "However, there are simpler ways to create demo ML apps. And one of the simpler ways is by using `Gradio`.\n",
        "\n",
        "[Gradio](https://gradio.app/) is a free and open-source Python library that allows you to develop an easy-to-use customizable component demo for your machine learning model that anyone can use anywhere. Gradio integrates with the most popular Python libraries used for ML and Data Science, including [Scikit-learn](https://scikit-learn.org/stable/), [PyTorch](https://pytorch.org/), [NumPy](https://numpy.org/), [seaborn](https://seaborn.pydata.org/), [pandas](https://pandas.pydata.org/), [TensorFlow](https://www.tensorflow.org/), and many others.\n",
        "\n",
        "<img src=\"https://pypi-camo.global.ssl.fastly.net/a95ef5913dc4cc84d2155ff690a0fa0d4c33d7e2/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f67726164696f2d6170702f67726164696f2f6d61696e2f726561646d655f66696c65732f67726164696f2e737667\" alt=\"gradio-image\" width=\"400px\">\n",
        "\n",
        "[Source](https://pypi.org/project/gradio/).\n",
        "\n",
        "Let's first create an application to [recognize digits](https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Intro-Course/7_MNIST_numpy.ipynb), one of the first tasks in ML that we explore in our mini course.\n",
        "\n",
        "For this application, instead of training a dense feed-forward model, we will train a convolutional neural network (`CNN`) since this is the standard in most computer vision applications. You can find other examples of how to build CNNs on [this](https://github.com/Nkluge-correa/TeenyTinyCastle/blob/master/ML-Explainability/CV/CNN_model_maker.ipynb) notebook.\n",
        "\n",
        "> **Note:** If you do not want to train this model from scratch, you can download it directly from the Hub 🤗 ([AiresPucrs/digit-classifier](https://huggingface.co/AiresPucrs/digit-classifier))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPCsS7JiNaaZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "train_images = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "test_images = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "val_images = x_val.reshape(x_val.shape[0], 28, 28, 1)\n",
        "\n",
        "train_labels = tf.keras.utils.to_categorical(y_train)\n",
        "test_labels = tf.keras.utils.to_categorical(y_test)\n",
        "val_labels = tf.keras.utils.to_categorical(y_val)\n",
        "\n",
        "print('Training Set Size: '), print(x_train.shape)\n",
        "print('Validation Set Size: '), print(x_val.shape)\n",
        "print('Test Set Size: '), print(x_test.shape)\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(20, (5,5), padding='same', activation='relu', input_shape=(28,28,1)),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
        "    tf.keras.layers.Conv2D(50, (5,5), padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(500, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callbacks = [tf.keras.callbacks.ModelCheckpoint(\"digit-classifier.h5\",\n",
        "                                                save_best_only=True),\n",
        "            tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                                            patience=3,\n",
        "                                            verbose=1,\n",
        "                                            mode=\"auto\",\n",
        "                                            baseline=None,\n",
        "                                            restore_best_weights=True)]\n",
        "\n",
        "history = model.fit(train_images, train_labels,\n",
        "                    validation_data=(val_images, val_labels),\n",
        "                    epochs=10,\n",
        "                    batch_size=256,\n",
        "                    verbose=2,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "\n",
        "test_loss_score, test_acc_score = model.evaluate(test_images, test_labels)\n",
        "\n",
        "print(f'Final Loss: {round(test_loss_score, 2)}.')\n",
        "print(f'Final Performance: {round(test_acc_score * 100, 2)} %.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue0jgNE7Naac"
      },
      "source": [
        "With our trained and saved model, creating an application with gradio takes not much more than 20 lines of code. You can style certain components with HTML, Markdown, and CSS. Applications created with gradio can even be hosted on the `gradio.app` for 72 hours, free of charge. Just use launch the application with the `share` argument equal to `True` (`demo.launch(share=True)`).\n",
        "\n",
        "Permanent hosting can be easily done through the [HuggingFace Spaces](https://www.huggingface.co/spaces), or any PaaS (Platforms as a service) you wish to use, like [Heroku](https://heroku.com/) or [Render](https://render.com/).\n",
        "\n",
        "You can check our Hugging Face space 🤗🏰  with the two applications worked on in this notebook:\n",
        "[Digit Classifier](https://huggingface.co/spaces/AiresPucrs/play-gradio)\n",
        "and [Basic Chatbot](https://huggingface.co/spaces/AiresPucrs/Basic-Chatbot).\n",
        "\n",
        "> Note: First, you need to install `gradio` through the command\n",
        "pip install. We will also be installing the `huggingface_hub` library to download our trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un1jcopvJQPh",
        "outputId": "50a9480d-d48e-4a0a-f7fd-3c4f1ac3b461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.8/304.8 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m665.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install gradio==3.35.2 huggingface_hub[\"tensorflow\"] -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6qbiRCTJnLm"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from huggingface_hub import from_pretrained_keras\n",
        "\n",
        "model = from_pretrained_keras(\"AiresPucrs/digit-classifier\")\n",
        "\n",
        "classes = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
        "\n",
        "def predict(img):\n",
        "  img = img.reshape(1, 28, 28, 1)\n",
        "  prediction = model.predict(img, verbose=0).tolist()[0]\n",
        "  return {classes[i]: prediction[i] for i in range(10)}\n",
        "\n",
        "title = \"Digit Classifier - By Teeny-Tiny Castle 🏰\"\n",
        "\n",
        "head = (\n",
        "  \"<center>\"\n",
        "  \"<img src='https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png' width=400>\"\n",
        "  \"This model was trained to classify numbers (from 0 to 9). To test it, write your number in the space provided.\"\n",
        "  \"</center>\"\n",
        ")\n",
        "\n",
        "\n",
        "ref = (\n",
        "\"<center>\"\n",
        "\"Return to the <a href='https://github.com/Nkluge-correa/TeenyTinyCastle)'>castle</a>.\"\n",
        "\"</center>\")\n",
        "\n",
        "# create interface\n",
        "demo = gr.Interface(fn=predict,\n",
        "             inputs=\"sketchpad\",\n",
        "             outputs=gr.Label(num_top_classes=3),\n",
        "             allow_flagging=\"never\",\n",
        "             title=title,\n",
        "             description=head,\n",
        "             article=ref)\n",
        "\n",
        "# launch interface\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG_khmAyNaad"
      },
      "source": [
        "You can even use Gradio to create applications that are not ML-based. For example, below, we show you how to create a closed-domain chatbot with a small number of answers and questions (_a basic rules-based system that performs n-gram search_). To see the full implementation of this bot, go to [this link](https://github.com/Nkluge-correa/Aira/tree/master/Aira-1).\n",
        "\n",
        "> Note: You need to install `unidecode` first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU0p4HtHBr_k",
        "outputId": "bdf1609c-ac85-4cc9-b468-e11f84594d23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/235.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/235.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install unidecode -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1y7Xeg82q6Y_",
        "outputId": "dacf6c7b-61ce-4328-e4ec-01845e1136e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-12-15 15:42:48--  https://github.com/Nkluge-correa/Aira/raw/master/Aira-1/data/generated_data/keys_en.json\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Nkluge-correa/Aira/master/Aira-1/data/generated_data/keys_en.json [following]\n",
            "--2023-12-15 15:42:48--  https://raw.githubusercontent.com/Nkluge-correa/Aira/master/Aira-1/data/generated_data/keys_en.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 568709 (555K) [text/plain]\n",
            "Saving to: ‘keys_en.json.1’\n",
            "\n",
            "\rkeys_en.json.1        0%[                    ]       0  --.-KB/s               \rkeys_en.json.1      100%[===================>] 555.38K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-12-15 15:42:48 (13.3 MB/s) - ‘keys_en.json.1’ saved [568709/568709]\n",
            "\n",
            "--2023-12-15 15:42:48--  https://github.com/Nkluge-correa/Aira/raw/master/Aira-1/data/original_data/answers_en.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Nkluge-correa/Aira/master/Aira-1/data/original_data/answers_en.txt [following]\n",
            "--2023-12-15 15:42:48--  https://raw.githubusercontent.com/Nkluge-correa/Aira/master/Aira-1/data/original_data/answers_en.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46169 (45K) [text/plain]\n",
            "Saving to: ‘answers_en.txt.1’\n",
            "\n",
            "answers_en.txt.1    100%[===================>]  45.09K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-12-15 15:42:48 (4.22 MB/s) - ‘answers_en.txt.1’ saved [46169/46169]\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "(async (port, path, width, height, cache, element) => {\n                        if (!google.colab.kernel.accessAllowed && !cache) {\n                            return;\n                        }\n                        element.appendChild(document.createTextNode(''));\n                        const url = await google.colab.kernel.proxyPort(port, {cache});\n\n                        const external_link = document.createElement('div');\n                        external_link.innerHTML = `\n                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n                                    https://localhost:${port}${path}\n                                </a>\n                            </div>\n                        `;\n                        element.appendChild(external_link);\n\n                        const iframe = document.createElement('iframe');\n                        iframe.src = new URL(path, url).toString();\n                        iframe.height = height;\n                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n                        iframe.width = width;\n                        iframe.style.border = 0;\n                        element.appendChild(iframe);\n                    })(7861, \"/\", \"100%\", 500, false, window.element)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from statistics import mode\n",
        "import urllib.request\n",
        "import gradio as gr\n",
        "import unidecode\n",
        "import requests\n",
        "import string\n",
        "import json\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "# Download keys_en.json\n",
        "urllib.request.urlretrieve('https://github.com/Nkluge-correa/Aira/raw/master/Aira-1/data/generated_data/keys_en.json', 'keys_en.json')\n",
        "\n",
        "# Download answers_en.txt\n",
        "urllib.request.urlretrieve('https://github.com/Nkluge-correa/Aira/raw/master/Aira-1/data/original_data/answers_en.txt', 'answers_en.txt')\n",
        "\n",
        "# Load data from files\n",
        "with open('answers_en.txt', encoding='utf-8') as fp:\n",
        "    answers = [line.strip() for line in fp]\n",
        "\n",
        "with open('keys_en.json') as json_file:\n",
        "    dictionary = json.load(json_file)\n",
        "\n",
        "def generate_ngrams(text, WordsToCombine):\n",
        "    \"\"\"\n",
        "    Generates n-grams of length WordsToCombine from the input text.\n",
        "\n",
        "    Args:\n",
        "        text: A string representing the input text\n",
        "        WordsToCombine: An integer representing the\n",
        "            size of the n-grams to be generated\n",
        "\n",
        "    Returns:\n",
        "        A list of n-grams generated from the input text, where each\n",
        "        n-gram is a list of WordsToCombine words\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    output = []\n",
        "    for i in range(len(words) - WordsToCombine+1):\n",
        "        output.append(words[i:i+WordsToCombine])\n",
        "    return output\n",
        "\n",
        "\n",
        "def make_keys(text, WordsToCombine):\n",
        "    \"\"\"\n",
        "    Given a text and a number of words to combine, returns\n",
        "    a list of keys that correspond to all possible combinations\n",
        "    of n-grams (sequences of n consecutive words) in the text.\n",
        "\n",
        "    Args:\n",
        "        - text (str): The input text.\n",
        "        - WordsToCombine (int): The number of words to combine.\n",
        "\n",
        "    Returns:\n",
        "        - sentences (list of str): A list of all the keys, which are\n",
        "        the n-grams in the text.\n",
        "    \"\"\"\n",
        "    gram = generate_ngrams(text, WordsToCombine)\n",
        "    sentences = []\n",
        "    for i in range(0, len(gram)):\n",
        "        sentence = ' '.join(gram[i])\n",
        "        sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def chat(message, history):\n",
        "    \"\"\"\n",
        "    A function that generates a response to a user input message\n",
        "    based on a pre-built dictionary of responses.\n",
        "\n",
        "    Args:\n",
        "        message (str): A string representing the user's input message.\n",
        "        history (list): A list of tuples containing previous\n",
        "        messages and responses.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two lists of tuples. The first list is\n",
        "        the original history with the user's input message and the bot's\n",
        "        response appended as a tuple. The second list is an updated history\n",
        "        with the same tuples.\n",
        "    \"\"\"\n",
        "    history = history or []\n",
        "    text = message.lower()\n",
        "    sentences = []\n",
        "    values = []\n",
        "    new_text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    new_text = unidecode.unidecode(new_text)\n",
        "\n",
        "    if len(new_text.split()) == 1:\n",
        "        if new_text in dictionary.keys():\n",
        "            l = [dictionary[new_text]] * 100\n",
        "            values.append(l)\n",
        "        new_text = new_text + ' ' + new_text\n",
        "\n",
        "    else:\n",
        "        if new_text in dictionary.keys():\n",
        "            l = [dictionary[new_text]] * 100\n",
        "            values.append(l)\n",
        "\n",
        "    for i in range(1, len(new_text.split()) + 1):\n",
        "        sentence = make_keys(new_text, i)\n",
        "        sentences.append(sentence)\n",
        "\n",
        "    for i in range(len(sentences)):\n",
        "        attention = sentences[i]\n",
        "        for i in range(len(attention)):\n",
        "            if attention[i] in dictionary.keys():\n",
        "                l = [dictionary[attention[i]]] * i\n",
        "                values.append(l)\n",
        "\n",
        "    if len([item for sublist in values for item in sublist]) == 0:\n",
        "        bot_input_ids = \"I'm sorry, either I didn't understand the question, or it is not part of my domain of expertise... :( Try asking it in another way or using other words. Maybe then I can help you!\"\n",
        "        history.append((message, bot_input_ids))\n",
        "        return history, history\n",
        "\n",
        "    else:\n",
        "        values = [item for sublist in values for item in sublist]\n",
        "        prediction = mode(values)\n",
        "        bot_input_ids = answers[int(prediction)-1]\n",
        "        history.append((message, bot_input_ids))\n",
        "        return history, history\n",
        "\n",
        "title = \"Basic Chatbot - By Teeny-Tiny Castle 🏰\"\n",
        "\n",
        "head = (\n",
        "  \"<center>\"\n",
        "  \"<img src='https://d2vrvpw63099lz.cloudfront.net/do-i-need-a-chatbot/header-chat-box.png' width=400>\"\n",
        "  \"This is an example of a rules-based closed domain chatbot. It knows a couple of answers to questions related to AI.\"\n",
        "  \"<br>\"\n",
        "  \"</center>\"\n",
        ")\n",
        "\n",
        "ref = (\n",
        "\"<center>\"\n",
        "\"To see its full version (ML style) of this bot, go to <a href='https://playground.airespucrs.org/aira'>this link</a>.\"\n",
        "\"</center>\")\n",
        "\n",
        "# create a chat interface\n",
        "chatbot = gr.Chatbot()\n",
        "\n",
        "demo = gr.Interface(\n",
        "    chat,\n",
        "    [\"text\", \"state\"],\n",
        "    [chatbot, \"state\"],\n",
        "    allow_flagging=\"never\",\n",
        "    title=title,\n",
        "    description=head,\n",
        "    article=ref\n",
        ")\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M6xa53sNaaf"
      },
      "source": [
        "Now you know how to create simple AI applications to show and share with your friends and colleagues!\n",
        "\n",
        "\n",
        "To see its full version (ML style) of this bot, go to <a href='https://nkluge-correa.github.io/Aira/'>this link</a>. The implementation of this gradio app can be found [on the Hub](https://huggingface.co/spaces/nicholasKluge/Aira-Demo)!🤗\n",
        "\n",
        "---\n",
        "\n",
        "Return to the [castle](https://github.com/Nkluge-correa/TeenyTinyCastle)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aca09746cf57686f00a55ae76e987247ecfb5dd0b3b2e2474d8dbbf0c5e3377e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
